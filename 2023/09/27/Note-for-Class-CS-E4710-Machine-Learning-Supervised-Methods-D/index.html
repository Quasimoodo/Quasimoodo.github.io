<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.2.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/mine-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/mine-16x16.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css" integrity="sha256-DfWjNxDkM94fVBWx1H5BMMp0Zq7luBlV8QRcSES7s+0=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"quasimoodo.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.12.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":3,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="这是关于课程 机器学习-监督方法的笔记。这门课作业不算多，但是内容普遍比较理论化 希望可以学的好一点。">
<meta property="og:type" content="article">
<meta property="og:title" content="Note for Class CS-E4710 - Machine Learning: Supervised Methods D">
<meta property="og:url" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/index.html">
<meta property="og:site_name" content="MetaExistential">
<meta property="og:description" content="这是关于课程 机器学习-监督方法的笔记。这门课作业不算多，但是内容普遍比较理论化 希望可以学的好一点。">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230927192311142-16958321007251.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230927191941385.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230927192755056-16958321038722.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231208033807955.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230927194640083.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230927195059252.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230927195124896.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230927200137127.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230927200148870.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230927200435685.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230927200655041.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230927201411977.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230927201544950.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230927201913451.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230927202255998.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230927202511785.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230927192755056-16958321038722.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230929164001463.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230929165522922.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230929170658370.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230929173533871.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230929190656757.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230929192028402.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230929192254785.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230929194054908.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230929194534646.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230929195059752.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230929205824865.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230929205745657.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230929205419126.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230929210542203.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230929210728950.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230929211935506.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231208204056083.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231208204814056.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231208204450075.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231208205315166.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231208205523218.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231208205709230.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231208210055643.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231208220455948.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231208220643143.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231208220715969.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231208220759112.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231208220932936.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209034508916.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209034657651.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209035700649.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209035755025.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209035919977.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209040225218.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209040015503.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209040329651.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209040341303.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209040430231.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209040541458.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209040800184.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209040814241.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209040904307.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209041204053.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209043016830.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209043207610.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209043302587.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209043827299.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209043904187.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209044136286.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209044129577.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209044613396.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209044808627.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209045002927.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209045159599.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209045222049.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209045550298.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209050651227.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209050739626.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209050923754.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209051425440.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209190340424.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209190504366.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209191349201.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209204644767.png">
<meta property="article:published_time" content="2023-09-27T15:46:34.000Z">
<meta property="article:modified_time" content="2023-12-10T01:49:21.085Z">
<meta property="article:author" content="Quasimoodo">
<meta property="article:tag" content="Note">
<meta property="article:tag" content="Course">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230927192311142-16958321007251.png">


<link rel="canonical" href="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/","path":"2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/","title":"Note for Class CS-E4710 - Machine Learning: Supervised Methods D"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Note for Class CS-E4710 - Machine Learning: Supervised Methods D | MetaExistential</title>
  





  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">MetaExistential</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#introduction"><span class="nav-number">1.</span> <span class="nav-text">Introduction</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#empirical-risk"><span class="nav-number">1.0.0.1.</span> <span class="nav-text">empirical risk ：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#hypothesis-classes-or-model-families"><span class="nav-number">1.0.0.2.</span> <span class="nav-text">hypothesis classes or
model families</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="nav-number">1.1.</span> <span class="nav-text">线性回归</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%8C%E5%85%83%E5%88%86%E7%B1%BB"><span class="nav-number">1.2.</span> <span class="nav-text">二元分类</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#consistent-hypothesis"><span class="nav-number">1.2.0.1.</span> <span class="nav-text">consistent hypothesis</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AF%84%E4%BC%B0%E6%A8%A1%E5%9E%8B"><span class="nav-number">1.3.</span> <span class="nav-text">评估模型</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95"><span class="nav-number">2.</span> <span class="nav-text">统计学习方法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A6%82%E7%8E%87%E8%BF%91%E4%BC%BC%E6%AD%A3%E7%A1%AE%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6-pac-framework"><span class="nav-number">2.1.</span> <span class="nav-text">概率近似正确学习框架 PAC
framework</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#pac%E5%8F%AF%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA%E5%AF%B9%E6%9C%89%E9%99%90%E5%81%87%E8%AE%BE%E9%9B%86"><span class="nav-number">2.2.</span> <span class="nav-text">PAC可学习理论对有限假设集</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#boolean-conjunctions"><span class="nav-number">2.2.1.</span> <span class="nav-text">Boolean conjunctions</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%BB%E6%84%8F%E5%B8%83%E5%B0%94%E5%85%AC%E5%BC%8F"><span class="nav-number">2.2.2.</span> <span class="nav-text">任意布尔公式</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AF%81%E6%98%8E%E5%A4%A7%E7%BA%B2"><span class="nav-number">2.3.</span> <span class="nav-text">证明大纲</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E8%80%83"><span class="nav-number">2.4.</span> <span class="nav-text">参考：</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#l3-pac-learning%E7%9A%84%E6%8B%93%E5%B1%95"><span class="nav-number">3.</span> <span class="nav-text">L3 PAC learning的拓展</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#vc-dimension"><span class="nav-number">3.1.</span> <span class="nav-text">VC dimension</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#lecture-4-model-selection"><span class="nav-number">4.</span> <span class="nav-text">Lecture 4 Model selection</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#bayes-error"><span class="nav-number">4.0.0.1.</span> <span class="nav-text">Bayes error</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#bayes-classifier"><span class="nav-number">4.0.0.1.1.</span> <span class="nav-text">Bayes classifier</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#noise"><span class="nav-number">4.0.0.1.2.</span> <span class="nav-text">noise</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#excess-error"><span class="nav-number">4.0.0.2.</span> <span class="nav-text">excess error</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#estimation-error"><span class="nav-number">4.0.0.2.1.</span> <span class="nav-text">estimation error</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#approximation-error"><span class="nav-number">4.0.0.2.2.</span> <span class="nav-text">approximation error</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#regularization-based-algorithms"><span class="nav-number">4.1.</span> <span class="nav-text">Regularization-based
algorithms</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%8C%83%E6%95%B0"><span class="nav-number">4.1.0.1.</span> <span class="nav-text">范数</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#model-selection-by-using-a-validation-set"><span class="nav-number">4.2.</span> <span class="nav-text">Model selection by
using a validation set</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81-cross-validation"><span class="nav-number">4.2.1.</span> <span class="nav-text">交叉验证 Cross-validation</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%90%9E%E9%AA%8C%E8%AF%81%E9%9B%86"><span class="nav-number">4.2.1.1.</span> <span class="nav-text">搞验证集</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#n-fold"><span class="nav-number">4.2.1.1.1.</span> <span class="nav-text">n-Fold</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#leave-one-out-cross-validation-loo"><span class="nav-number">4.2.1.2.</span> <span class="nav-text">Leave-one-out
cross-validation (LOO)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%B5%8B%E8%AF%95%E9%9B%86%E4%B8%80%E8%B5%B7%E6%90%9E"><span class="nav-number">4.2.1.3.</span> <span class="nav-text">测试集一起搞</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#nested-cross-validation"><span class="nav-number">4.2.1.3.1.</span> <span class="nav-text">Nested cross-validation</span></a></li></ol></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8Blinear-models"><span class="nav-number">5.</span> <span class="nav-text">线性模型，Linear models</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB%E5%99%A8"><span class="nav-number">5.1.</span> <span class="nav-text">线性分类器：</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BC%98%E7%82%B9"><span class="nav-number">5.1.0.1.</span> <span class="nav-text">优点：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%A9%BA%E9%97%B4%E8%A7%A3%E9%87%8A"><span class="nav-number">5.1.0.2.</span> <span class="nav-text">空间解释：</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB%E5%99%A8%E7%9A%84%E5%AD%A6%E4%B9%A0%E8%BF%87%E7%A8%8B"><span class="nav-number">5.2.</span> <span class="nav-text">线性分类器的学习过程</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#margin"><span class="nav-number">5.2.0.1.</span> <span class="nav-text">Margin</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%84%9F%E7%9F%A5%E6%9C%BA-perceptron"><span class="nav-number">5.3.</span> <span class="nav-text">感知机 Perceptron</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%85%B7%E4%BD%93%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B"><span class="nav-number">5.3.0.1.</span> <span class="nav-text">具体训练过程</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%8B%A5%E5%B9%B2%E5%A5%BD%E6%80%A7%E8%B4%A8"><span class="nav-number">5.3.0.2.</span> <span class="nav-text">若干好性质</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#theorem-novikoff"><span class="nav-number">5.3.0.2.1.</span> <span class="nav-text">Theorem （Novikoff)</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9A%84%E5%BD%A2%E5%BC%8F%E5%8C%96%E8%A1%A8%E8%BE%BE"><span class="nav-number">5.3.0.3.</span> <span class="nav-text">损失函数的形式化表达</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#logistic-regression"><span class="nav-number">5.4.</span> <span class="nav-text">Logistic regression</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#loss"><span class="nav-number">5.4.0.1.</span> <span class="nav-text">loss</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83"><span class="nav-number">5.4.0.2.</span> <span class="nav-text">训练</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="nav-number">5.4.0.2.1.</span> <span class="nav-text">随机梯度下降</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%BF%87%E7%A8%8B"><span class="nav-number">5.4.0.2.2.</span> <span class="nav-text">过程:</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#stepsize"><span class="nav-number">5.4.0.2.3.</span> <span class="nav-text">stepsize</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%81%9C%E6%AD%A2%E6%9D%A1%E4%BB%B6"><span class="nav-number">5.4.0.2.4.</span> <span class="nav-text">停止条件</span></a></li></ol></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#svm"><span class="nav-number">6.</span> <span class="nav-text">SVM</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#section"><span class="nav-number">7.</span> <span class="nav-text"></span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E9%9D%A2%E5%90%91%E8%80%83%E8%AF%95%E5%AD%A6%E4%B9%A0"><span class="nav-number">8.</span> <span class="nav-text">面向考试学习</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#a1"><span class="nav-number">8.1.</span> <span class="nav-text">A1</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8El1"><span class="nav-number">8.1.1.</span> <span class="nav-text">基于L1</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#a2"><span class="nav-number">8.2.</span> <span class="nav-text">A2</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8El2"><span class="nav-number">8.2.1.</span> <span class="nav-text">基于L2</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8El3"><span class="nav-number">8.2.2.</span> <span class="nav-text">基于L3</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#a3"><span class="nav-number">8.3.</span> <span class="nav-text">A3</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#l45"><span class="nav-number">8.3.1.</span> <span class="nav-text">L4，5</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#a4"><span class="nav-number">8.4.</span> <span class="nav-text">A4</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#l467"><span class="nav-number">8.4.1.</span> <span class="nav-text">L4,6,7</span></a></li></ol></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Quasimoodo"
      src="/images/taffy.png">
  <p class="site-author-name" itemprop="name">Quasimoodo</p>
  <div class="site-description" itemprop="description">Plodding in Truth</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">18</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">17</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/taffy.png">
      <meta itemprop="name" content="Quasimoodo">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="MetaExistential">
      <meta itemprop="description" content="Plodding in Truth">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="Note for Class CS-E4710 - Machine Learning: Supervised Methods D | MetaExistential">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Note for Class CS-E4710 - Machine Learning: Supervised Methods D
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-09-27 18:46:34" itemprop="dateCreated datePublished" datetime="2023-09-27T18:46:34+03:00">2023-09-27</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-12-10 03:49:21" itemprop="dateModified" datetime="2023-12-10T03:49:21+02:00">2023-12-10</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Substance/" itemprop="url" rel="index"><span itemprop="name">Substance</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <p>这是关于课程
机器学习-监督方法的笔记。这门课作业不算多，但是内容普遍比较理论化
希望可以学的好一点。 <span id="more"></span></p>
<h1 id="introduction">Introduction</h1>
<p>典型任务：分类，回归，排序</p>
<ul>
<li>分类：
<ul>
<li>将数据通过 决定面或边界 来分到预定义的类中</li>
<li>Multi-label Classification： 某个样本可以同时属于多个类</li>
<li>Extreme classification：类非常非常多</li>
</ul></li>
<li>回归
<ul>
<li>期望的输出是数字变量</li>
</ul></li>
<li>排序
<ul>
<li>不需要具体的值但是希望有一个排序列表</li>
<li>输入通常是一个偏序对列表：x&gt;y</li>
</ul></li>
</ul>
<p>定义了输入空间，输出空间，损失函数等内容。</p>
<p>将模型记作<span class="math inline">\(h\)</span></p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230927192311142-16958321007251.png" alt="image-20230927192311142" style="zoom:50%;"></p>
<p>值得注意的是有一个没见过的东西：</p>
<h4 id="empirical-risk">empirical risk ：</h4>
<p>​ 通过计算训练集的平均loss 来衡量模型的错误近似水平</p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230927191941385.png" alt="image-20230927191941385" style="zoom: 67%;"></p>
<p>在训练集上使用，评估模型对于训练集的拟合能力</p>
<p><span class="math inline">\(R\)</span> 有一个帽子，是训练集上的</p>
<p>####generalization error</p>
<p>另一个东西是 泛化误差 （generalization error) 或者称之为（真实）风险
(<strong>risk</strong>)</p>
<figure>
<img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230927192755056-16958321038722.png" alt="image-20230927192755056">
<figcaption aria-hidden="true">image-20230927192755056</figcaption>
</figure>
<p>大概意思是，这个数值是 损失函数在真实数据集的期望，其中<span class="math inline">\(D [ L(h(x), y)
]\)</span>真实数据下损失函数的期望。</p>
<p>在我们不知道整体数据分布<span class="math inline">\(D\)</span>的前提下，我们怎么通过训练集和模型类<span class="math inline">\(H\)</span>来说<span class="math inline">\(R(h)\)</span>呢？</p>
<p>有两种方法：</p>
<ul>
<li>通过测试集的经验风险评估</li>
<li>统计学习理论（下面要学的）</li>
</ul>
<p>这给俩名词解释：</p>
<h4 id="hypothesis-classes-or-model-families">hypothesis classes or
model families</h4>
<p>指的就是大类模型：</p>
<ul>
<li>线性模型
<ul>
<li>逻辑回归</li>
</ul></li>
<li>神经网络</li>
<li>核方法
<ul>
<li>SVM</li>
</ul></li>
<li>组装方法
<ul>
<li>Random Forests</li>
</ul></li>
</ul>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231208033807955.png" alt="image-20231208033807955" style="zoom: 50%;">可以认为是一个超参数未定的模型，而<span class="math inline">\(h\)</span>就是一个我们给出了的确定参数的模型</p>
<h2 id="线性回归">线性回归</h2>
<figure>
<img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230927194640083.png" alt="image-20230927194640083">
<figcaption aria-hidden="true">image-20230927194640083</figcaption>
</figure>
<p>优化问题：</p>
<figure>
<img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230927195059252.png" alt="image-20230927195059252">
<figcaption aria-hidden="true">image-20230927195059252</figcaption>
</figure>
<p>最小值有定值，只要关于x的一个矩阵可逆</p>
<figure>
<img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230927195124896.png" alt="image-20230927195124896">
<figcaption aria-hidden="true">image-20230927195124896</figcaption>
</figure>
<h2 id="二元分类">二元分类</h2>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230927200137127.png" alt="image-20230927200137127" style="zoom:67%;"></p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230927200148870.png" alt="image-20230927200148870" style="zoom:67%;"></p>
<h4 id="consistent-hypothesis">consistent hypothesis</h4>
<p>下面探讨关于学到的<span class="math inline">\(h\)</span></p>
<p><strong>当然需要训练集上（数据）没问题</strong></p>
<p>如果一个<span class="math inline">\(h\)</span>能够在训练集上全部分对，那么我们称其为一致假设(<strong>consistent
hypothesis</strong>) 并在此基础上有两个极端例子：</p>
<ul>
<li>最兼容假设(Most general hypothesis)<span class="math inline">\(G\)</span></li>
<li>最特定假设(Most specific hypothesis)<span class="math inline">\(S\)</span></li>
</ul>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230927200435685.png" alt="image-20230927200435685" style="zoom:67%;"></p>
<p>直观上来说，选一个中间的会最安全：</p>
<ul>
<li>Margin：
<ul>
<li>the minimum distance between the decision boundary and a training
point</li>
<li>在决定边界核训练点之间的最小距离</li>
</ul></li>
</ul>
<p><strong>最小化到两个极端例子的距离</strong></p>
<p>这种方式在SVM支持向量机中使用</p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230927200655041.png" alt="image-20230927200655041" style="zoom:67%;"></p>
<p>这里还有点问题，关于边距Margin的</p>
<h2 id="评估模型">评估模型</h2>
<p>对于分类问题最常用的LOSS是01损失，但是他有俩个问题：</p>
<ul>
<li>难以应对数据不平衡</li>
<li>不同的预测错误可能代价不一致：致命疾病的诊断</li>
</ul>
<p>引入假阴，假阳的概念：</p>
<ul>
<li>这里的假 说的是预测错误</li>
<li>这里的阴和阳说的是预测结果</li>
</ul>
<p>一般来说，更加specific的模型会倾向于假阴更多，假阳更少（更趋向于预测阴？）</p>
<p><strong>长方形面积更小</strong></p>
<p>如果更general就相反</p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230927201411977.png" alt="image-20230927201411977" style="zoom:67%;"></p>
<p><strong>Confusion matrix</strong>（混淆矩阵/误差矩阵）</p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230927201544950.png" alt="image-20230927201544950" style="zoom:67%;"></p>
<p>在此基础上，引入了四个评价模型的指标</p>
<ul>
<li>Empirical risk
<ul>
<li>错误预测的比例</li>
</ul></li>
<li>Precision/Positive Predictive Value
<ul>
<li>预测为阳中，正确的比例</li>
</ul></li>
<li>Recall/Sensitivity
<ul>
<li>真阳中对了多少</li>
</ul></li>
<li>F1
<ul>
<li>Recall 核 Precision搞一起的一个东西</li>
<li></li>
</ul></li>
</ul>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230927201913451.png" alt="image-20230927201913451" style="zoom:67%;"></p>
<p>在这个基础上，引入了一个称为<strong>ROC</strong>的概念。引入某种分类时候模型给出的可信度阈值<span class="math inline">\(θ\)</span>，大于他算成阳反之则阴，通过调节这个东西可以调节模型的假阴假阳占比</p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230927202255998.png" alt="image-20230927202255998" style="zoom:67%;"></p>
<p>ROC曲线可以衡量一个模型的指标，包括在假阴假阳上的权衡/是否有全局最优的<span class="math inline">\(h\)</span></p>
<ul>
<li>底下的面积称为AUC/AUROC</li>
<li>一般来说，一个分类器越倾向于预测为阳，其中真/假的比例就越高</li>
</ul>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230927202511785.png" alt="image-20230927202511785" style="zoom:67%;"></p>
<p>怎么估计模型在更广阔数据中的表现呢？</p>
<ul>
<li>可以使用独立测试集的 empirical risk来估计，期望就是
<ul>
<li>测试集上的Rh 肯定更低</li>
<li>模型越复杂，empirical risk越低，可能把模型太复杂了</li>
<li>可能搞的是fitting而非learning</li>
</ul></li>
</ul>
<h1 id="统计学习方法">统计学习方法</h1>
<p>我们希望可以最小化这个东西 true risk/generalization error：</p>
<figure>
<img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230927192755056-16958321038722.png" alt="image-20230927192755056">
<figcaption aria-hidden="true">image-20230927192755056</figcaption>
</figure>
<p>但是不知道真实的数据分布： 这就是本节课要解决的问题</p>
<p>假设真实数据和训练集独立同分布（i.i.d.)</p>
<h2 id="概率近似正确学习框架-pac-framework">概率近似正确学习框架 PAC
framework</h2>
<p>Probably Approximate Correct (PAC) Learning
framework，形式化了机器学习的泛化概念</p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230929164001463.png" alt="image-20230929164001463" style="zoom:50%;"></p>
<p>值得一提的是其中的概念类<span class="math inline">\(C\)</span>,这是我们希望找到$h $ ∈ <span class="math inline">\(H\)</span>,来尽可能地接近他。</p>
<p>这样的话，最底下的公式也就好理解了</p>
<p>下面定义了对于这种概念类（concept class) <span class="math inline">\(C\)</span> 是否是PAC-learnable的。</p>
<p>如果他是可学习的，那么：</p>
<ul>
<li>存在一个算法<span class="math inline">\(A\)</span></li>
<li>给定数据集<span class="math inline">\(S\)</span></li>
<li>能够学到一个目标假设类<span class="math inline">\(h_s\)</span>∈<span class="math inline">\(H\)</span></li>
<li>能使得泛化误差小于<span class="math inline">\(\epsilon\)</span>的概率大于1-δ</li>
</ul>
<p><strong>对于任何分布和随机<span class="math inline">\(\epsilon\)</span>，sample size <span class="math inline">\(m\)</span>对1/<span class="math inline">\(\epsilon\)</span>,1/δ多项式增长，只要两个</strong></p>
<p>在此基础上，增加了一个 efficiently
PAC-learnable的概念，<strong>更强，对数据集大小<span class="math inline">\(m\)</span>也有要求</strong>，如果训练的算法<span class="math inline">\(A\)</span>能够以某种多项式复杂度时间对这三个变量增长的话</p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230929165522922.png" alt="image-20230929165522922" style="zoom:50%;"></p>
<p>具体解释一下这个式子：</p>
<ul>
<li>$<span class="math inline">\(, Generalization error
bound，泛化误差，我们有信心这个学到的概念类\)</span>h<span class="math inline">\(在全局数据上的错误率 通常采用\)</span>$</li>
<li><span class="math inline">\(1-\delta\)</span>, Confidence
level，信心程度，我们认为前面那个错误率小于多少多少的失败率，通常采用<span class="math inline">\(\delta =0.05\)</span> <strong><span class="math inline">\(\delta\)</span>可以说是错误率</strong></li>
<li>所需的样本大小和运行时间，不应该随着误差降低和信心增强，而爆炸增长——需要多项式复杂度</li>
<li><span class="math inline">\(\{R(h_S ) ≤ \epsilon\}\)</span>
被看成了一个随机变量，因为我们确实不知道哪个<span class="math inline">\(H中的h\)</span>会被挑出来，以及他在真实数据上的表现（这一句和最后一句不一样，有点问题）</li>
</ul>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230929170658370.png" alt="image-20230929170658370" style="zoom:50%;"></p>
<p>上面概念的可视化：<strong>非常好图片</strong></p>
<ul>
<li>最高的就是泛化误差的期望</li>
<li>信心越强，<span class="math inline">\(\delta\)</span>越小,<span class="math inline">\(\epsilon\)</span>就越大（能接受的泛化误差）
gap就越大</li>
</ul>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230929173533871.png" alt="image-20230929173533871" style="zoom:50%;"></p>
<p>下面讨论怎么样得出样本量，错误率和置信程度的一个公式（这破玩意看了快一下午）</p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230929190656757.png" alt="image-20230929190656757" style="zoom:50%;"></p>
<p>首先我们看这样一个东西，<span class="math inline">\(R\)</span>是想学到的概念类<span class="math inline">\(C\)</span>, <span class="math inline">\(R`\)</span>是我们在目前样本上，能学到的最紧的<span class="math inline">\(h\)</span>，可以把目前的样本都预测对，也是一个最紧的预测，没有留下什么余地。那么</p>
<ul>
<li>错误只会来自于假阴，即，<span class="math inline">\(R`\)</span>把真实的应该是蓝色的错误给排除出去了</li>
<li>泛化误差即是两个预测的差，即<span class="math inline">\(R(h)&lt;=P(R-R`)\)</span></li>
<li>我们能得到的理想<span class="math inline">\(h\)</span>肯定要大于<span class="math inline">\(R`\)</span></li>
<li>计算出的泛化误差<span class="math inline">\(\epsilon
&#39;=Pr_D(R-h)\)</span></li>
</ul>
<p>下面我们引入一个用于辅助证明的基础预设， <span class="math display">\[
Pr_D(R)&gt;\epsilon
\]</span> 这是几乎显然的，不然你用最紧的<span class="math inline">\(R&#39;\)</span> 都能得到 <span class="math inline">\(R(R&#39;)&lt;\epsilon\)</span>
,也就没有继续研究的意义了(因为<span class="math inline">\(R&#39;\)</span>是<span class="math inline">\(R\)</span>的子集，<span class="math inline">\(Pr_D(R-h)&lt;=Pr_D(R-R&#39;)&lt;Pr_D(R)\)</span>)</p>
<p>下面在误差区域中，构造四个小长方形辅助证明</p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230929192028402.png" alt="image-20230929192028402" style="zoom:50%;"></p>
<p><span class="math inline">\(r_i\)</span>之间彼此有重叠，且 <span class="math display">\[
Pr_D(r_i)={\epsilon \over 4}
\]</span> 可以看出来，他们的并集的概率密度几乎是显然小等于<span class="math inline">\(\epsilon\)</span>
的（从图上来看，等于去不到，但不妨严谨一点）</p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230929192254785.png" alt="image-20230929192254785" style="zoom:50%;"></p>
<p>错误的预测仅会出现在浅蓝色外框和橘色内框中。</p>
<p>如果我们得到的预测类<span class="math inline">\(h\)</span>能够交到四个小矩形<span class="math inline">\(r_i\)</span>,那么剩下的面积显然小于<span class="math inline">\(\epsilon\)</span>,即 <span class="math display">\[
R(h)&lt;\epsilon
\]</span>
换句话说，即上一句的逆否命题：如果算出来的泛化误差大于阈值<span class="math inline">\(\epsilon\)</span>
,那么至少有一个小矩形没被交到。</p>
<p>对于目前的训练数据，蓝色点点来说，不在某个小矩形<span class="math inline">\(r_i\)</span>的概率是<span class="math inline">\(P=1-{\epsilon \over 4}\)</span></p>
<p><strong>这里没有对数据的分布进行假定，因为取这个矩形的时候就假定了某个小矩形所占的概率密度，不需要数据是均匀分布的</strong></p>
<p>那么，现在的数据集容量<span class="math inline">\(m=card(D_{train})\)</span>,这些点都不在某个小矩形的概率就是
<span class="math display">\[
(1-{\epsilon \over 4})^m
\]</span>
考虑每个点不在每个小矩形之间彼此独立（为什么？），那么’至少有一个小矩形没被分布到‘
就是四个上述事件的和事件， <span class="math display">\[
P(R(h)&gt;\epsilon)&lt;=\Sigma_1^4(1-{\epsilon \over 4})^m=4(1-{\epsilon
\over 4})^m
\]</span> 我们希望这种事情（误差大于阈值）的概率小于一个信心程度<span class="math inline">\(\delta\)</span> ,那么就有了 <span class="math display">\[
P(R(h)&gt;\epsilon)&lt;\delta
\]</span> 同时也是： <span class="math display">\[
P(R(h)&lt;=\epsilon)&gt;=1-\delta
\]</span></p>
<p>=&gt; <span class="math display">\[
4(1-{\epsilon \over 4})^m&lt;\delta
\]</span> 继续放缩</p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230929194054908.png" alt="image-20230929194054908" style="zoom:50%;"></p>
<p><span class="math display">\[
4(1−{\epsilon \over 4})^m ≤ 4 exp({−m \epsilon \over 4})&lt;\delta
\]</span> 解得： <span class="math display">\[
m&gt;= {4 \over \epsilon}*ln({4\over\delta})
\]</span> 揭示了训练样本量和泛化误差<span class="math inline">\(\epsilon\)</span>,信心指数<span class="math inline">\(\delta\)</span>的关系</p>
<p>同时有了要达到希望的错误率，使用$m,$的关系</p>
<p>（为什么放缩这么一家伙？）</p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230929194534646.png" alt="image-20230929194534646" style="zoom:50%;"></p>
<p>这里展示了一些三者之间的函数关系图，可以看到：</p>
<ul>
<li>数据量越大，泛化误差越低</li>
<li>数据量的增加有边际递减效应（law of diminishing returns)</li>
<li>信心指数<span class="math inline">\(\delta\)</span>越高，就需要越多的数据来降低同样的误差</li>
</ul>
<p>这样的一个泛化误差边界有这样的好处：</p>
<ul>
<li>对于任意的目标概念类<span class="math inline">\(C\)</span>(包括难学的)，任意的数据分布<span class="math inline">\(D\)</span>(包括对抗性生成的让学习更难的)都适用，因为没有依赖他们进行假设推导</li>
<li>我们此时研究的是
误差分布的最大，也就是图像的尾巴，而没有研究这个量收敛到尾部的情况）</li>
</ul>
<p>所以：</p>
<ul>
<li>经验预估检测错误率（empirically estimated test
errors）应该比这个低很多
（训练集上算的？，<strong>也是从真分布上估计出来的</strong>）</li>
<li>这个也应该比实际的上界松很多，是一个非常general的上界</li>
</ul>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230929195059752.png" alt="image-20230929195059752" style="zoom:50%;"></p>
<h2 id="pac可学习理论对有限假设集">PAC可学习理论对有限假设集</h2>
<p>对于有限假设集<span class="math inline">\(H\)</span>，它满足以下条件：</p>
<ul>
<li>输入元素满足有限域（例如离散化的数值或布尔变量，即，可列举的）</li>
<li>假设的表示也是有有限空间的（变量重复有限次数）</li>
<li>用布尔代数运算符对付布尔公式的自己</li>
</ul>
<p>这个东西已经被统计学习理论研究清楚了</p>
<p>可以知道：</p>
<ul>
<li>样本复杂边界（Sample complexity bound），就是样本量。
即给定泛化误差阈值<span class="math inline">\(\epsilon\)</span>
和置信度<span class="math inline">\(\delta\)</span>，需要多大的样本量才行</li>
<li>泛化误差（ generalization error bound）。
给定样本量和置信度，可以达到多大的误差。</li>
</ul>
<p>当然这俩都得要你算出来有限假设集的大小才行</p>
<p>成立的前提是，<strong>存在</strong>一个一致假设…<strong>consistent
hypothesis, one with zero empirical risk</strong>，</p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230929205824865.png" alt="image-20230929205824865" style="zoom:67%;"></p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230929205745657.png" alt="image-20230929205745657" style="zoom:67%;"></p>
<p><strong>后面对于这个公式（<span class="math inline">\(m&gt;=\)</span>)给了证明，不过没太看懂</strong></p>
<p>？为什么这边还单独对有限假设类给了另一个公式</p>
<p>下面给出例子进行详细说明</p>
<p>对于一种相对简单的情况：</p>
<h3 id="boolean-conjunctions">Boolean conjunctions</h3>
<p>（包括了 and 和 not的布尔公式，不能用or）</p>
<p>每一个变量是布尔变量</p>
<p>比如这样的输入：</p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230929205419126.png" alt="image-20230929205419126" style="zoom:50%;"></p>
<p>假设集大小为 <span class="math display">\[
|H|=3^d
\]</span> 每一个变量要么正，要么否，要么不出现——承认无关变量的可能</p>
<h3 id="任意布尔公式">任意布尔公式</h3>
<p>？怎么区分到底是3为底还是2</p>
<p>若有<span class="math inline">\(d\)</span>个变量，每一个都可以为正或负，那么输入空间<span class="math inline">\(X\)</span> <span class="math display">\[
|X| = 2^d
\]</span> 考虑到并的存在，一个假设<span class="math inline">\(h\)</span>就是一个子集<span class="math inline">\(S\)</span>的每一项并集</p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230929210542203.png" alt="image-20230929210542203" style="zoom:50%;"></p>
<p>对于<span class="math inline">\(X\)</span>中的每一个向量，都有挑选与否的可能：
<span class="math display">\[
|H|=2^{2^d}
\]</span> 计算其他东西用上面的公式就行了</p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230929210728950.png" alt="image-20230929210728950" style="zoom:50%;"></p>
<p>这个可以看出来，对<span class="math inline">\(d\)</span>是指数增长的，所以不认为是PAC-learnable</p>
<h2 id="证明大纲">证明大纲</h2>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230929211935506.png" alt="image-20230929211935506" style="zoom:50%;"></p>
<p>这页没看懂</p>
<p>后面基本都能看懂 先搁着把</p>
<p>下面还讨论了一点有限假设集不一致的情况，从公式来看是多了一点项</p>
<h2 id="参考">参考：</h2>
<p>https://theigrams.github.io/zjblog/2021/07/22/pac.html</p>
<h1 id="l3-pac-learning的拓展">L3 PAC learning的拓展</h1>
<p>上面学的，一个是假设 假设类是矩形，一个是假设 假设类是有限的</p>
<p>但是在现实中，有更多不依赖这种前提的问题（SVM的hyperplane，神经网络的连续输入）</p>
<p>所以我们希望有更好的工具来分析这些案例</p>
<h2 id="vc-dimension">VC dimension</h2>
<p>可以说是一个用来衡量假说类容量的概念，用来适用于不同的理论</p>
<h1 id="lecture-4-model-selection">Lecture 4 Model selection</h1>
<ul>
<li>给定数据集 怎么算<span class="math inline">\(R(h),R(h*),R*\)</span>?</li>
</ul>
<p>开始一个大型的名词解释来整清楚上面这个过程：</p>
<h4 id="bayes-error">Bayes error</h4>
<p>给定数据（<span class="math inline">\(X,Y\)</span>）的分布，能够实现的最小误差</p>
<p><span class="math inline">\(R^*\)</span></p>
<ul>
<li><p>算不出来，但可以用下面介绍的方式分解</p></li>
<li><p>generalization error<span class="math inline">\(R(h)\)</span>也算不出来</p></li>
<li><p>是一个用来衡量最好表现的理论工具</p></li>
</ul>
<h5 id="bayes-classifier">Bayes classifier</h5>
<p>一个假设<span class="math inline">\(h\)</span>，能实现<span class="math inline">\(R(h) = R^∗\)</span>泛化误差等于Bayes error的称之为
<strong>Bayes classfier</strong></p>
<h5 id="noise">noise</h5>
<p>Bayes classifier 实现的平均误差是<strong>noise</strong></p>
<p>noise的期望 就是Bayes error</p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231208204056083.png" alt="image-20231208204056083" style="zoom:50%;"></p>
<p>不过这张图里，noise是对于每一个<span class="math inline">\(x\)</span>
预测概率较小的那一项的概率？</p>
<h4 id="excess-error">excess error</h4>
<p>衡量一个假设<span class="math inline">\(h\)</span>相对于最优误差的
Bayes classifier，会达到多少泛化误差</p>
<p>通常会这么分解：</p>
<figure>
<img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231208204814056.png" alt="image-20231208204814056">
<figcaption aria-hidden="true">image-20231208204814056</figcaption>
</figure>
<ul>
<li>泛化误差的差就是我们感兴趣的东西</li>
</ul>
<h5 id="estimation-error">estimation error</h5>
<p><span class="math inline">\(\epsilon_{est}= R(h) −
R(h∗)\)</span>这一部分称为估计误差，是目前<span class="math inline">\(h\)</span>距离<span class="math inline">\(h`∈H\)</span> ，假设类中最优的一个，的距离</p>
<ul>
<li>这一部分也被称为variance，好像能达到</li>
</ul>
<h5 id="approximation-error">approximation error</h5>
<ul>
<li><span class="math inline">\(\epsilon_{approxiamation}== R(h ∗ ) −
R∗\)</span> 这一部分成为估计误差，描述假设类和最佳假设类的距离
<ul>
<li>这部分称为bias，难以消除</li>
</ul></li>
</ul>
<hr>
<p>下面给出一个理想情况下计算的例子：</p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231208204450075.png" alt="image-20231208204450075" style="zoom:50%;"></p>
<ul>
<li>X均匀分布</li>
<li>分类器会将概率最大的作为预测输出</li>
<li><span class="math inline">\(R^*\)</span> 计算了对于每一个X的概率
乘以他的预测小标签的概率的和</li>
</ul>
<p>为了拟合这个分布，我们提出了一个假设类<span class="math inline">\(H\)</span>：</p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231208205315166.png" alt="image-20231208205315166" style="zoom:50%;"></p>
<p><span class="math inline">\(H\)</span>中最好的<span class="math inline">\(h^`\)</span>,将3以下的数字标为0，3标为1</p>
<p>他的泛化误差<span class="math inline">\(R(h^`)\)</span>就是这么算的：，通过没有达到设计的那一部分数据的影响</p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231208205523218.png" alt="image-20231208205523218" style="zoom:50%;"></p>
<p>然后目前，我们在一个13个数据的训练集上，训练出了1个h，能够将1标为正，其他为0</p>
<p>他能达到最好的empirical
error<img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231208205709230.png" alt="image-20231208205709230" style="zoom: 50%;">=0.384（虽然没什么用），</p>
<p>利用上面的真数据分布计算出的generalization
error（在这个例子中都算错的项
对应的概率）是0.433（有点奇怪，因为干的事情就不一样）</p>
<p>最后以R(h)的形状展示出来</p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231208210055643.png" alt="image-20231208210055643" style="zoom:67%;"></p>
<hr>
<p>总结：</p>
<ul>
<li><span class="math inline">\(\epsilon\)</span>estimation = R(h) − R(h
`) 这一部分可以通过PAC理论估计出来
<ul>
<li>？PAC估计的是什么来着</li>
</ul></li>
</ul>
<p>可以看出来，<span class="math inline">\(\epsilon_{app}=approximation
= R(h ∗ ) − R ∗\)</span>
这一部分搞不出来，因为i不知道概念类和目标的差异——无法评估概念类的好坏！</p>
<p>在我们使<span class="math inline">\(H\)</span>
更复杂的时候，我们就有更大机会包括离 Bayes classifer 更近的<span class="math inline">\(h\)</span>，降低 approximation error</p>
<p><strong>但是会让<span class="math inline">\(h\)</span>更难学！——增大Estimation
error，generalization bounds 更松，VC维更高</strong></p>
<p>下来要干的事情： 尽可能好的训练模型，使得它既有加好的 empirical
error，又有较好的复杂性（在当前数据上表现好，但代价不太高）</p>
<h2 id="regularization-based-algorithms">Regularization-based
algorithms</h2>
<p>我们将假设类按照复杂度分层划分，<img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231208220455948.png" alt="image-20231208220455948" style="zoom: 67%;"><span class="math inline">\(\gamma\)</span>就是他的复杂度，可以是布尔的变量数也可以是神经网络的大小</p>
<p>需要挑选合适的<span class="math inline">\(\gamma\)</span></p>
<p>希望对模型中的大权重进行惩罚，下面的讨论将给予线性函数：<span class="math inline">\(x → w^T x\)</span></p>
<p>这里的<span class="math inline">\(\gamma\)</span>就是<span class="math inline">\(w\)</span>的范数<img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231208220643143.png" alt="image-20231208220643143" style="zoom:67%;"></p>
<h4 id="范数">范数</h4>
<p>范数常用有两种算法：</p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231208220715969.png" alt="image-20231208220715969" style="zoom:50%;"></p>
<hr>
<p>对于L2来说，算他的Rademacher complexity是有捷径的</p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231208220759112.png" alt="image-20231208220759112" style="zoom: 67%;"></p>
<p>一般来说，可以用<span class="math inline">\(w\)</span>的范数来当作这个假设类的Rm上界</p>
<hr>
<p>回到最开始的想法，学习的目标应该是最小化一个函数，考虑到经验误差和模型的复杂性，这里给出形式化定义：</p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231208220932936.png" alt="image-20231208220932936" style="zoom:67%;"></p>
<h2 id="model-selection-by-using-a-validation-set">Model selection by
using a validation set</h2>
<p>这里给出了一般的训练集/验证集和测试集划分手段</p>
<ul>
<li>使用网格搜，来寻找在验证集上表现最好的超参数
<ul>
<li>代价大！</li>
</ul></li>
<li>有意思的是，找到了最好的超参数后，<strong>在训练集和验证集上共同</strong>重新训练最后的模型，
<ul>
<li>再去测试集验证</li>
</ul></li>
<li>验证集：
<ul>
<li>防止过拟合</li>
<li>如果测试集拿来找超参，可能过分乐观
<ul>
<li>测试集不能参与贡献！</li>
</ul></li>
</ul></li>
<li>怎么分？
<ul>
<li>一个分法是分层分，每一个类中都挑出等比例的，最后合并</li>
<li>防止有的小类没被见过</li>
</ul></li>
</ul>
<h3 id="交叉验证-cross-validation">交叉验证 Cross-validation</h3>
<p>这一部分的内容目的是考虑普通的验证集和测试集划分，仍然可能存在一定问题：</p>
<ul>
<li>训练集 测试集可能虽然小但还是有噪声/outliers</li>
<li>训练过程仍然有随机性（初始化）</li>
</ul>
<p>————我们希望通过多搞几次（averaging multiple
splits）来克服这些问题！</p>
<ul>
<li>最好的超参数由N个验证集上平均最好的给出</li>
</ul>
<h4 id="搞验证集">搞验证集</h4>
<p>这一步的前提是你已经留好了一个不变的测试集</p>
<ol type="1">
<li>当然，你可以重复随机划分</li>
<li>但是我们这里用更科学的方法——</li>
</ol>
<h5 id="n-fold">n-Fold</h5>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209034508916.png" alt="image-20231209034508916" style="zoom:50%;"></p>
<h4 id="leave-one-out-cross-validation-loo">Leave-one-out
cross-validation (LOO)</h4>
<p>每次只把一个当作验证集，其他全部当成训练集</p>
<p>——很强，但计算代价大</p>
<h4 id="测试集一起搞">测试集一起搞</h4>
<p>n-fold的问题是，单个选择的测试集仍然肯恩恶搞有偏差</p>
<p>使用</p>
<h5 id="nested-cross-validation">Nested cross-validation</h5>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209034657651.png" alt="image-20231209034657651" style="zoom:50%;"></p>
<p>两层循环，先留测试集，剩下的内容上再N-fold搞训练集/验证集划分</p>
<h1 id="线性模型linear-models">线性模型，Linear models</h1>
<p>这一章开始学习具体的模型，内容包括</p>
<ul>
<li>模型的任务</li>
<li>模型是如何构建的
<ul>
<li>包括几何意义</li>
</ul></li>
<li>如何优化（损失函数）
<ul>
<li>错误是什么形式</li>
<li>错误又应该如何更新</li>
<li>对损失函数的证明/优化</li>
</ul></li>
<li>收敛条件
<ul>
<li>能对付什么样子的数据？
<ul>
<li>（是否收敛） 收敛速度</li>
</ul></li>
<li>数据大小，要求 分布</li>
</ul></li>
</ul>
<hr>
<p>这一章全部都是在做分类任务，甚至集中在线性分类</p>
<ul>
<li>先介绍任务</li>
<li>在介绍线性分类器
<ul>
<li>在其基础上介绍感知机</li>
</ul></li>
<li>再介绍逻辑回归（确实是在做分类的）</li>
</ul>
<h2 id="线性分类器">线性分类器：</h2>
<p>输入是多维实数，分类为正负例<img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209035700649.png" alt="image-20231209035700649" style="zoom:50%;"></p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209035755025.png" alt="image-20231209035755025" style="zoom:50%;"></p>
<p>假设类长这个样子，向量乘一下，再加上偏置项 然后再激活</p>
<h4 id="优点">优点：</h4>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209035919977.png" alt="image-20231209035919977" style="zoom:50%;"></p>
<ul>
<li>好理解</li>
<li>空间小</li>
<li>简单（好学习）</li>
</ul>
<p><strong>应该是第一个尝试的算法！</strong></p>
<h4 id="空间解释">空间解释：</h4>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209040225218.png" alt="image-20231209040225218" style="zoom:50%;"></p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209040015503.png" alt="image-20231209040015503" style="zoom:50%;"></p>
<ul>
<li>线是<span class="math inline">\(wx\)</span></li>
<li>法向量是<span class="math inline">\(w\)</span></li>
<li>其他分别标注出了
<ul>
<li>它到原点的距离</li>
<li>偏置项会导致法向量方向和超平面的关系</li>
<li>点和平面距离</li>
</ul></li>
</ul>
<h2 id="线性分类器的学习过程">线性分类器的学习过程</h2>
<p>可以看出来
原来的式子<img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209040329651.png" alt="image-20231209040329651" style="zoom:50%;">可以写的更简单</p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209040341303.png" alt="image-20231209040341303" style="zoom:50%;"></p>
<p>分类后的点的函数值对应向量和法向量所成的角度与正负例有关</p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209040430231.png" alt="image-20231209040430231" style="zoom:50%;"></p>
<p>这里学习到了一个重要的概念：</p>
<h4 id="margin">Margin</h4>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209040541458.png" alt="image-20231209040541458" style="zoom:50%;"></p>
<p>大概就是标准化过的点到超平面的距离向量，再和判断是否正确乘一下,&lt;0就是错了</p>
<p><strong>我们会希望这个margin大于0</strong></p>
<h2 id="感知机-perceptron">感知机 Perceptron</h2>
<p>第一个机器学习算法，1956，用来做二分类问题</p>
<p>形式是这样子的：</p>
<figure>
<img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209040800184.png" alt="image-20231209040800184">
<figcaption aria-hidden="true">image-20231209040800184</figcaption>
</figure>
<p>训练过程是这样的：</p>
<figure>
<img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209040814241.png" alt="image-20231209040814241">
<figcaption aria-hidden="true">image-20231209040814241</figcaption>
</figure>
<p>直到全部都分类正确</p>
<h4 id="具体训练过程">具体训练过程</h4>
<p><strong>主要研究对象是训练集</strong></p>
<p>在干什么呢？</p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209040904307.png" alt="image-20231209040904307" style="zoom:50%;"></p>
<ul>
<li><span class="math inline">\(yixi\)</span>被当作错误代价用来更新权重向量</li>
<li>我们来看一看对于一个（错误分类的点），它的margin会如何迭代：</li>
<li>可以看到它的下次margin就是这次再加上<span class="math inline">\(x^2\)</span>,一直在增大
<ul>
<li>当然不能保证一次更新就行</li>
</ul></li>
</ul>
<p>大概就是这样的一个过程：平面的法向量在转，就分好了</p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209041204053.png" alt="image-20231209041204053" style="zoom:50%;"></p>
<h4 id="若干好性质">若干好性质</h4>
<ul>
<li>只要两个类是线性可分的，就一定能超平面收敛</li>
</ul>
<h5 id="theorem-novikoff">Theorem （Novikoff)</h5>
<p>** 应用条件 应该是两个线性可分的类和感知机**</p>
<p>证明对于一个知道了</p>
<ul>
<li>大小</li>
<li>最大模(衡量数据集有多分散)</li>
</ul>
<p>的测试集</p>
<p>并且满足某条件</p>
<ul>
<li><span class="math inline">\(\gamma\)</span> 是最大可实现的margin
<ul>
<li>衡量数据集中两个类的分离程度</li>
</ul></li>
</ul>
<p><strong>可以证明训练的收敛步数上界</strong></p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209043016830.png" alt="image-20231209043016830" style="zoom:50%;"></p>
<p>?为什么这边<span class="math inline">\(w\)</span>都是标准化过的</p>
<p>但是，对于非线性可分数据来说，<strong>算法不会停止！</strong></p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209043207610.png" alt="image-20231209043207610" style="zoom:50%;"></p>
<h4 id="损失函数的形式化表达">损失函数的形式化表达</h4>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209043302587.png" alt="image-20231209043302587" style="zoom:50%;"></p>
<p>可以看出这个损失函数是凸的，所以有很好的性质：</p>
<ul>
<li>局部最小也是全局最小</li>
<li>比较好通过逐步更新减少loss
<ul>
<li>不是NP-hard</li>
</ul></li>
</ul>
<hr>
<p>但是现在，我们希望还有一个比感知机更好的算法，<strong>离最优比较近就终止</strong>就行了</p>
<h2 id="logistic-regression">Logistic regression</h2>
<p>它确实是分类器</p>
<ul>
<li><p>logistic函数</p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209043827299.png" alt="image-20231209043827299" style="zoom:50%;"></p></li>
<li><p>logit函数</p>
<ul>
<li>是他的反函数</li>
<li><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209043904187.png" alt="image-20231209043904187" style="zoom:50%;"></li>
</ul></li>
</ul>
<hr>
<p>logistic regression
认为<img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209044136286.png" alt="image-20231209044136286" style="zoom:50%;">可以用底下的式子写出来</p>
<ul>
<li>随之而来的它的margin</li>
</ul>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209044129577.png" alt="image-20231209044129577" style="zoom:50%;"></p>
<h4 id="loss">loss</h4>
<p>我们希望能够最大化训练集上的正确的条件概率分布，将其表示为<span class="math inline">\(w^*\)</span></p>
<p>两边取对数再稍作变形，得到它的相反数，<strong>loss</strong></p>
<p>最大化可能性，就是最小化loss</p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209044613396.png" alt="image-20231209044613396" style="zoom:50%;"></p>
<p>这是一个单调递减的可微凸函数，他还有好性质：</p>
<ul>
<li>离谱的时候值大，</li>
<li>正确的时候很慢——不会对已经分的比较好的继续努力</li>
</ul>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209044808627.png" alt="image-20231209044808627" style="zoom:50%;"></p>
<h4 id="训练">训练</h4>
<p>我们希望能够求出这个loss的最小值</p>
<p>因为它不是线性的，所以没办法直接给——使用随机梯度下降<strong>stochastic
gradient descent</strong></p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209045002927.png" alt="image-20231209045002927" style="zoom:50%;"></p>
<p>梯度就是<span class="math inline">\(J\)</span>对所有变量的偏导，组成的一个向量</p>
<p>算还是正经算，算出来发现仍然能扯回去</p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209045159599.png" alt="image-20231209045159599" style="zoom:50%;"></p>
<p>作为对比，原始的条件概率是这样的：</p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209045222049.png" alt="image-20231209045222049" style="zoom: 67%;"></p>
<p>对每个训练输入<span class="math inline">\(x\)</span>,计算出来他的梯度向量,最后发现直接乘对应向量就可以了</p>
<p><strong>这个梯度向量给出了最快下降的方向</strong></p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209045550298.png" alt="image-20231209045550298" style="zoom:50%;"></p>
<p>但是算这个东西其实很烦,因为对每个<span class="math inline">\(x\)</span>都要算这么一家伙(如果算全体梯度的话</p>
<h5 id="随机梯度下降">随机梯度下降</h5>
<p>每次从全体样本中抽一条,用它的样本更新<span class="math inline">\(w\)</span></p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209050651227.png" alt="image-20231209050651227" style="zoom:50%;"></p>
<p>显然,它的期望是和全梯度一样的(随机抽取的)</p>
<p>?更新了y不是不一样了</p>
<h5 id="过程">过程:</h5>
<p><strong>初始化<span class="math inline">\(w=0\)</span></strong></p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209050739626.png" alt="image-20231209050739626" style="zoom:50%;"></p>
<h5 id="stepsize">stepsize</h5>
<p>SGD 代表随机梯度下降（Stochastic Gradient Descent）</p>
<p>学习率被称为η, stepsize</p>
<ul>
<li>如果使用固定学习率
<ul>
<li>太大太小都不好</li>
</ul></li>
</ul>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209050923754.png" alt="image-20231209050923754" style="zoom:50%;"></p>
<ul>
<li>也可以使用关于训练轮数的动态学习率
<ul>
<li>仍然需要多次调整衰减系数<span class="math inline">\(\alpha\)</span></li>
</ul></li>
</ul>
<h5 id="停止条件">停止条件</h5>
<p>可能的选择:</p>
<ul>
<li>最大训练次数
<ul>
<li>需要为每个数据集单独设立</li>
</ul></li>
<li>梯度条件
<ul>
<li>梯度的维度小于某阈值</li>
</ul></li>
<li>一般来说,常用的是都能分类对就行了
<ul>
<li>loss收敛前就能达到</li>
</ul></li>
</ul>
<hr>
<p>这个东西还不是太理解</p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209051425440.png" alt="image-20231209051425440" style="zoom:50%;"></p>
<p>?? 这两个具体是啥什么区别?</p>
<ul>
<li>functional margin
<ul>
<li><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209190340424.png" alt="image-20231209190340424" style="zoom:50%;"></li>
<li>加了符号(判断是否正确的) 分类映射后的x</li>
<li>衡量离超平面有多远,可以认为是信心程度</li>
</ul></li>
<li>geometric margin
<ul>
<li><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209190504366.png" alt="image-20231209190504366" style="zoom:50%;"></li>
<li>上面那个东西的标准化,</li>
<li>带方向的,到超平面的距离:</li>
</ul></li>
<li><span class="math inline">\(|g(x)|/|w|\)</span>
<ul>
<li>不带方向的距离</li>
</ul></li>
</ul>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209191349201.png" alt="image-20231209191349201" style="zoom:50%;"></p>
<h1 id="svm">SVM</h1>
<p>这一章延续上一章logistic模型和感知机,在最大化functional
margin的基础上 提出了怎么找这样一个模型,这就是SVM</p>
<p>但是,对于支持向量的阐述其实并没有很明白</p>
<p>这一章的主题大概按照以下思路</p>
<ul>
<li>怎么找最大margin
<ul>
<li>functional margin没啥意义 要最大化 geometry margin</li>
<li><strong>这里需要明确<span class="math inline">\(\gamma\)</span></strong>是什么
做了一个巧妙变形,后面的优化求解问题建立在这个假设上</li>
</ul></li>
<li>首先讨论的是硬间隔
<ul>
<li>它有若干好性质</li>
<li>在其上讨论了它的VCD和 Rademacher</li>
</ul></li>
<li>其次考虑到线性不可分数据,我们希望模型能够对他仍然有处理能力
<ul>
<li>提出了软间隔模型,对每一个误分类加以代价的情况允许存在</li>
<li>在其上讨论了它的loss,称为<span class="math inline">\(L_{Hinge}\)</span>
<ul>
<li>通过不可导处定义使得梯度下降可行</li>
</ul></li>
<li>讨论了它的损失优化和梯度</li>
</ul></li>
<li>最后讨论了对偶软间隔SVM
<ul>
<li>认为超平面法向量可以写成支持向量的线性组合</li>
<li>然后将目标函数(也就是总错误)写成了含有<span class="math inline">\(a_i\)</span>的拉格朗日形式 求最小的优化问题
<ul>
<li>在此基础上讨论了一点核技巧</li>
</ul></li>
<li>下来把<span class="math inline">\(\alpha\)</span>像<span class="math inline">\(w\)</span>一样讨论了一下他的优化过程</li>
</ul></li>
</ul>
<hr>
<p>?是0-1损失的上界有什么好处?</p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209204644767.png" alt="image-20231209204644767" style="zoom:50%;"></p>
<p><br>
</p>
<h1 id="section"></h1>
<p>为什么|w| 就是评估过拟合的指标?</p>
<h1 id="面向考试学习">面向考试学习</h1>
<h2 id="a1">A1</h2>
<h3 id="基于l1">基于L1</h3>
<ul>
<li>给任务 ，选择对应的模型
<ul>
<li>回归/排序/多标签分类/二元分类</li>
</ul></li>
<li>对于给定结果，不同损失函数哪个代价更高？
<ul>
<li>MSE</li>
<li>MAE</li>
</ul></li>
<li>基础概念，
<ul>
<li>假设，假设类，假设集</li>
<li>损失函数的定义，用途等</li>
</ul></li>
<li>不同指标的定义，计算
<ul>
<li>precision</li>
<li>recall</li>
<li>accuracy</li>
</ul></li>
</ul>
<h2 id="a2">A2</h2>
<h3 id="基于l2">基于L2</h3>
<ul>
<li>PAC learnable 理论
<ul>
<li>里面的符号定义</li>
<li>generalisation error 怎么算，含义</li>
</ul></li>
<li>boolean conjunctions classifier
<ul>
<li>是什么</li>
<li>要达到若干准确度需要多少组合</li>
</ul></li>
</ul>
<h3 id="基于l3">基于L3</h3>
<ul>
<li>VC维的概念</li>
<li>分类器VC维是x，意味什么？</li>
<li>估计需要多少训练样本，才能让test error 到达多少
<ul>
<li>才能让基于VC dimension complexity 降到多少一下</li>
<li>和<span class="math inline">\(\delta\)</span>的关系</li>
</ul></li>
<li>generalisation bound 是什么？</li>
</ul>
<h2 id="a3">A3</h2>
<h3 id="l45">L4，5</h3>
<ul>
<li>计算Bayes classifier的 Byes error
<ul>
<li>x和y的条件概率</li>
<li>h，x的分布</li>
</ul></li>
<li>Percetron Algorithm
<ul>
<li>不同类型标准化的影响</li>
<li>类的线性可分</li>
<li>Novikoff`s theorem
<ul>
<li>标准化对于权重矩阵的影响</li>
<li>迭代次数和标准化的影响</li>
</ul></li>
</ul></li>
<li>通过 cross-validation 选择最好的超参数
<ul>
<li>nested cross validation
<ul>
<li>5-fold</li>
</ul></li>
<li>Roc-Auc 分数</li>
<li>Stochastic gradient algorithm</li>
</ul></li>
<li>row-wise normalization</li>
</ul>
<h2 id="a4">A4</h2>
<h3 id="l467">L4,6,7</h3>
<ul>
<li>polynomial kernel
<ul>
<li>explicit feature space
<ul>
<li>degree</li>
</ul></li>
<li>定义polynomial 的向量空间维度</li>
</ul></li>
</ul>
<p>（L6)</p>
<ul>
<li>SVM,随机梯度下降，原始方法</li>
</ul>
<p>（L7)</p>
<ul>
<li><p>SVC，sklearn中的</p></li>
<li><p>在 compliexity model和 emperical error 取得平衡</p></li>
<li><p>Kernel method</p>
<ul>
<li>从已知的kernel中搞到一个新的</li>
<li>给出参数 看看是不是给了一个半正定的核</li>
</ul></li>
<li><p>Gaussian kernel （RBF）</p>
<ul>
<li>对应的表达式</li>
<li>gradient</li>
<li>结合 Gaussian kernel 和 概率密度函数的gradient</li>
<li>看看新的核是不是半正定</li>
</ul></li>
</ul>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Note/" rel="tag"># Note</a>
              <a href="/tags/Course/" rel="tag"># Course</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/" rel="prev" title="Note-CSE4650 - Methods of Data Mining">
                  <i class="fa fa-chevron-left"></i> Note-CSE4650 - Methods of Data Mining
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2024/01/02/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Constitutional-AI-Harmlessness-from-AI-Feedback/" rel="next" title="论文笔记-Constitutional AI: Harmlessness From AI Feedback">
                  论文笔记-Constitutional AI: Harmlessness From AI Feedback <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Quasimoodo</span>
</div>
<!--
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>
-->

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  
<script src="https://cdn.jsdelivr.net/npm/hexo-generator-searchdb@1.4.0/dist/search.js" integrity="sha256-vXZMYLEqsROAXkEw93GGIvaB2ab+QW6w3+1ahD9nXXA=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>





  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdn.jsdelivr.net/npm/mathjax@3.2.1/es5/tex-mml-chtml.js","integrity":"sha256-hlC2uSQYTmPsrzGZTEQEg9PZ1a/+SV6VBCTclohf2og="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
