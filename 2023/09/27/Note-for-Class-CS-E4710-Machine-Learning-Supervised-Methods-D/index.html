<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.2.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/mine-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/mine-16x16.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css" integrity="sha256-DfWjNxDkM94fVBWx1H5BMMp0Zq7luBlV8QRcSES7s+0=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"quasimoodo.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.12.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":3,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="这是关于课程 机器学习-监督方法的笔记。这门课作业不算多，但是内容普遍比较理论化 希望可以学的好一点。">
<meta property="og:type" content="article">
<meta property="og:title" content="Note for Class CS-E4710 - Machine Learning: Supervised Methods D">
<meta property="og:url" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/index.html">
<meta property="og:site_name" content="MetaExistential">
<meta property="og:description" content="这是关于课程 机器学习-监督方法的笔记。这门课作业不算多，但是内容普遍比较理论化 希望可以学的好一点。">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230927192311142-16958321007251.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230927191941385.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230927192755056-16958321038722.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230927194640083.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230927195059252.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230927195124896.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230927200137127.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230927200148870.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230927200435685.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230927200655041.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230927201411977.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230927201544950.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230927201913451.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230927202255998.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230927202511785.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230927192755056-16958321038722.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230929164001463.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230929165522922.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230929170658370.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230929173533871.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230929190656757.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230929192028402.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230929192254785.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230929194054908.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230929194534646.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230929195059752.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230929205824865.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230929205745657.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230929205419126.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230929210542203.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230929210728950.png">
<meta property="og:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230929211935506.png">
<meta property="article:published_time" content="2023-09-27T15:46:34.000Z">
<meta property="article:modified_time" content="2023-09-29T18:34:40.548Z">
<meta property="article:author" content="Quasimoodo">
<meta property="article:tag" content="Note">
<meta property="article:tag" content="Course">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230927192311142-16958321007251.png">


<link rel="canonical" href="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/","path":"2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/","title":"Note for Class CS-E4710 - Machine Learning: Supervised Methods D"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Note for Class CS-E4710 - Machine Learning: Supervised Methods D | MetaExistential</title>
  





  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">MetaExistential</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#introduction"><span class="nav-number">1.</span> <span class="nav-text">Introduction</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="nav-number">1.1.</span> <span class="nav-text">线性回归</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%8C%E5%85%83%E5%88%86%E7%B1%BB"><span class="nav-number">1.2.</span> <span class="nav-text">二元分类</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AF%84%E4%BC%B0%E6%A8%A1%E5%9E%8B"><span class="nav-number">1.3.</span> <span class="nav-text">评估模型</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95"><span class="nav-number">2.</span> <span class="nav-text">统计学习方法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A6%82%E7%8E%87%E8%BF%91%E4%BC%BC%E6%AD%A3%E7%A1%AE%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6-pac-framework"><span class="nav-number">2.1.</span> <span class="nav-text">概率近似正确学习框架 PAC
framework</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#pac%E5%8F%AF%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA%E5%AF%B9%E6%9C%89%E9%99%90%E5%81%87%E8%AE%BE%E9%9B%86"><span class="nav-number">2.2.</span> <span class="nav-text">PAC可学习理论对有限假设集</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#boolean-conjunctions"><span class="nav-number">2.2.1.</span> <span class="nav-text">Boolean conjunctions</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%BB%E6%84%8F%E5%B8%83%E5%B0%94%E5%85%AC%E5%BC%8F"><span class="nav-number">2.2.2.</span> <span class="nav-text">任意布尔公式</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AF%81%E6%98%8E%E5%A4%A7%E7%BA%B2"><span class="nav-number">2.3.</span> <span class="nav-text">证明大纲</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E8%80%83"><span class="nav-number">2.4.</span> <span class="nav-text">参考：</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Quasimoodo"
      src="/images/taffy.png">
  <p class="site-author-name" itemprop="name">Quasimoodo</p>
  <div class="site-description" itemprop="description">Plodding in Truth</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">17</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">17</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="https://quasimoodo.github.io/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/taffy.png">
      <meta itemprop="name" content="Quasimoodo">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="MetaExistential">
      <meta itemprop="description" content="Plodding in Truth">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="Note for Class CS-E4710 - Machine Learning: Supervised Methods D | MetaExistential">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Note for Class CS-E4710 - Machine Learning: Supervised Methods D
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-09-27 18:46:34" itemprop="dateCreated datePublished" datetime="2023-09-27T18:46:34+03:00">2023-09-27</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-09-29 21:34:40" itemprop="dateModified" datetime="2023-09-29T21:34:40+03:00">2023-09-29</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Substance/" itemprop="url" rel="index"><span itemprop="name">Substance</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <p>这是关于课程
机器学习-监督方法的笔记。这门课作业不算多，但是内容普遍比较理论化
希望可以学的好一点。 <span id="more"></span></p>
<h1 id="introduction">Introduction</h1>
<p>典型任务：分类，回归，排序</p>
<ul>
<li>回归
<ul>
<li>期望的输出是数字变量</li>
</ul></li>
<li>排序
<ul>
<li>不需要具体的值但是希望有一个排序列表</li>
<li>输入通常是一个偏序对列表：x&gt;y</li>
</ul></li>
</ul>
<p>定义了输入空间，输出空间，损失函数等内容。</p>
<p>将模型记作<span class="math inline">\(h\)</span></p>
<figure>
<img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230927192311142-16958321007251.png" alt="image-20230927192311142">
<figcaption aria-hidden="true">image-20230927192311142</figcaption>
</figure>
<p>值得注意的是有一个没见过的东西：</p>
<p>empirical risk ：</p>
<p>​ 通过计算单个例子的平均loss 来衡量模型的错误近似水平</p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230927191941385.png" alt="image-20230927191941385" style="zoom: 67%;"></p>
<p>在训练集上使用，评估模型对于训练集的拟合能力</p>
<p><span class="math inline">\(R\)</span> 有一个帽子，是训练集上的</p>
<p>另一个东西是 泛化误差 （generalization error) 或者称之为（真实）风险
(<strong>risk</strong>)</p>
<figure>
<img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230927192755056-16958321038722.png" alt="image-20230927192755056">
<figcaption aria-hidden="true">image-20230927192755056</figcaption>
</figure>
<p>大概意思是，这个数值是 损失函数在真实数据集的期望，其中<span class="math inline">\(D [ L(h(x), y)
]\)</span>是给定数据下损失函数的期望。</p>
<p>（还是没太搞懂）</p>
<p>在我们不知道整体数据分布<span class="math inline">\(D\)</span>的前提下，我们怎么通过<span class="math inline">\(R(h)\)</span>和模型类<span class="math inline">\(H\)</span>来说<span class="math inline">\(R(h)\)</span>呢？</p>
<p>有两种方法：</p>
<ul>
<li>通过测试集的经验风险评估</li>
<li>统计学习理论（下面要学的）</li>
</ul>
<h2 id="线性回归">线性回归</h2>
<figure>
<img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230927194640083.png" alt="image-20230927194640083">
<figcaption aria-hidden="true">image-20230927194640083</figcaption>
</figure>
<p>优化问题：</p>
<figure>
<img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230927195059252.png" alt="image-20230927195059252">
<figcaption aria-hidden="true">image-20230927195059252</figcaption>
</figure>
<p>最小值有定值，只要关于x的一个矩阵可逆</p>
<figure>
<img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230927195124896.png" alt="image-20230927195124896">
<figcaption aria-hidden="true">image-20230927195124896</figcaption>
</figure>
<h2 id="二元分类">二元分类</h2>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230927200137127.png" alt="image-20230927200137127" style="zoom:67%;"></p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230927200148870.png" alt="image-20230927200148870" style="zoom:67%;"></p>
<p>下面探讨关于学到的<span class="math inline">\(h\)</span></p>
<p>如果一个<span class="math inline">\(h\)</span>能够在训练集上全部分对，那么我们称其为一致假设(<strong>consistent
hypothesis</strong>*) 并在此基础上有两个极端例子：</p>
<ul>
<li>最兼容假设(Most general hypothesis)<span class="math inline">\(G\)</span></li>
<li>最特定假设(Most specific hypothesis)<span class="math inline">\(S\)</span></li>
</ul>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230927200435685.png" alt="image-20230927200435685" style="zoom:67%;"></p>
<p>直观上来说，选一个中间的会最安全：</p>
<p><strong>最小化到两个极端例子的距离</strong></p>
<p>这种方式在SVM支持向量机中使用</p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230927200655041.png" alt="image-20230927200655041" style="zoom:67%;"></p>
<p>这里还有点问题，关于边距Margin的</p>
<h2 id="评估模型">评估模型</h2>
<p>对于分类问题最常用的LOSS是01损失，但是他有俩个问题：</p>
<ul>
<li>难以应对数据不平衡</li>
<li>不同的预测错误可能代价不一致：致命疾病的诊断</li>
</ul>
<p>引入假阴，假阳的概念：</p>
<ul>
<li>这里的假 说的是预测错误</li>
<li>这里的阴和阳说的是预测结果</li>
</ul>
<p>一般来说，更加specific的模型会倾向于假阴更多，假阳更少（更趋向于预测阴？）</p>
<p>如果更general就相反</p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230927201411977.png" alt="image-20230927201411977" style="zoom:67%;"></p>
<p><strong>Confusion matrix</strong>（混淆矩阵/误差矩阵）</p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230927201544950.png" alt="image-20230927201544950" style="zoom:67%;"></p>
<p>在此基础上，引入了四个评价模型的指标</p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230927201913451.png" alt="image-20230927201913451" style="zoom:67%;"></p>
<p>在这个基础上，引入了一个称为<strong>ROC</strong>的概念。引入某种分类时候模型给出的可信度阈值<span class="math inline">\(θ\)</span>，大于他算成阳反之则阴，通过调节这个东西可以调节模型的假阴假阳占比</p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230927202255998.png" alt="image-20230927202255998" style="zoom:67%;"></p>
<p>ROC曲线可以衡量一个模型的指标，包括在假阴假阳上的权衡/是否有全局最优的<span class="math inline">\(h\)</span></p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230927202511785.png" alt="image-20230927202511785" style="zoom:67%;"></p>
<h1 id="统计学习方法">统计学习方法</h1>
<p>我们希望可以最小化这个东西 true risk：</p>
<figure>
<img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230927192755056-16958321038722.png" alt="image-20230927192755056">
<figcaption aria-hidden="true">image-20230927192755056</figcaption>
</figure>
<p>但是不知道真实的数据分布： 这就是本节课要解决的问题</p>
<p>假设真实数据和训练集独立同分布（i.i.d.)</p>
<h2 id="概率近似正确学习框架-pac-framework">概率近似正确学习框架 PAC
framework</h2>
<p>Probably Approximate Correct (PAC) Learning
framework，形式化了机器学习的泛化概念</p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230929164001463.png" alt="image-20230929164001463" style="zoom:50%;"></p>
<p>值得一提的是其中的概念类<span class="math inline">\(C\)</span>,这是我们希望找到$h $ ∈ <span class="math inline">\(H\)</span>,来尽可能地接近他。</p>
<p>这样的话，最底下的公式也就好理解了</p>
<p>下面定义了对于这种概念类（concept class) <span class="math inline">\(C\)</span> 是否是PAC-learnable的。</p>
<p>如果他是可学习的，那么：</p>
<ul>
<li>存在一个算法<span class="math inline">\(A\)</span></li>
<li>给定数据集<span class="math inline">\(S\)</span></li>
<li>能够学到一个目标假设类<span class="math inline">\(h_s\)</span>∈<span class="math inline">\(H\)</span></li>
<li>能使得泛化误差小于<span class="math inline">\(\epsilon\)</span>的概率大于1-δ</li>
</ul>
<p>在此基础上，增加了一个 efficiently
PAC-learnable的概念，如果训练的算法<span class="math inline">\(A\)</span>能够以某种多项式复杂度时间对这三个变量增长的话</p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230929165522922.png" alt="image-20230929165522922" style="zoom:50%;"></p>
<p>具体解释一下这个式子：</p>
<ul>
<li>$<span class="math inline">\(, Generalization error
bound，泛化误差，我们有信心这个学到的概念类\)</span>h<span class="math inline">\(在全局数据上的错误率 通常采用\)</span>$</li>
<li><span class="math inline">\(\delta\)</span>, Confidence
level，信心程度，我们认为前面那个错误率小于多少多少的失败率，通常采用<span class="math inline">\(\delta =0.05\)</span></li>
<li>所需的样本大小和运行时间，不应该随着误差降低和信心增强，而爆炸增长——需要多项式复杂度</li>
<li><span class="math inline">\(\{R(h_S ) ≤ \epsilon\}\)</span>
被看成了一个随机变量，因为我们确实不知道他在真实数据上的表现（这一句和最后一句不一样，有点问题）</li>
</ul>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230929170658370.png" alt="image-20230929170658370" style="zoom:50%;"></p>
<p>上面概念的可视化：</p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230929173533871.png" alt="image-20230929173533871" style="zoom:50%;"></p>
<p>下面讨论怎么样得出样本量，错误率和置信程度的一个公式（这破玩意看了快一下午）</p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230929190656757.png" alt="image-20230929190656757" style="zoom:50%;"></p>
<p>首先我们看这样一个东西，<span class="math inline">\(R\)</span>是想学到的概念类<span class="math inline">\(C\)</span>, <span class="math inline">\(R`\)</span>是我们在目前样本上，能学到的最紧的<span class="math inline">\(h\)</span>，可以把目前的样本都预测对，也是一个最紧的预测，没有留下什么余地。那么</p>
<ul>
<li>错误只会来自于假阴，即，<span class="math inline">\(R`\)</span>把真实的应该是蓝色的错误给排除出去了</li>
<li>泛化误差即是两个预测的差，即<span class="math inline">\(R(h)&lt;=P(R-R`)\)</span></li>
<li>我们能得到的理想<span class="math inline">\(h\)</span>肯定要大于<span class="math inline">\(R`\)</span></li>
<li>计算出的泛化误差<span class="math inline">\(\epsilon
&#39;=Pr_D(R-h)\)</span></li>
</ul>
<p>下面我们引入一个用于辅助证明的基础预设， <span class="math display">\[
Pr_D(R)&gt;\epsilon
\]</span> 这是几乎显然的，不然你用最紧的<span class="math inline">\(R&#39;\)</span> 都能得到 <span class="math inline">\(R(R&#39;)&lt;\epsilon\)</span>
,也就没有继续研究的意义了(因为<span class="math inline">\(R&#39;\)</span>是<span class="math inline">\(R\)</span>的子集，<span class="math inline">\(Pr_D(R-h)&lt;=Pr_D(R-R&#39;)&lt;Pr_D(R)\)</span>)</p>
<p>下面在误差区域中，构造四个小长方形辅助证明</p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230929192028402.png" alt="image-20230929192028402" style="zoom:50%;"></p>
<p><span class="math inline">\(r_i\)</span>之间彼此有重叠，且 <span class="math display">\[
Pr_D(r_i)={\epsilon \over 4}
\]</span> 可以看出来，他们的并集的概率密度几乎是显然小等于<span class="math inline">\(\epsilon\)</span>
的（从图上来看，等于去不到，但不妨严谨一点）</p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230929192254785.png" alt="image-20230929192254785" style="zoom:50%;"></p>
<p>错误的预测仅会出现在浅蓝色外框和橘色内框中。</p>
<p>如果我们得到的预测类<span class="math inline">\(h\)</span>能够交到四个小矩形<span class="math inline">\(r_i\)</span>,那么剩下的面积显然小于<span class="math inline">\(\epsilon\)</span>,即 <span class="math display">\[
R(h)&lt;\epsilon
\]</span>
换句话说，即上一句的逆否命题：如果算出来的泛化误差大于阈值<span class="math inline">\(\epsilon\)</span>
,那么至少有一个小矩形没被交到。</p>
<p>对于目前的训练数据，蓝色点点来说，不在某个小矩形<span class="math inline">\(r_i\)</span>的概率是<span class="math inline">\(P=1-{\epsilon \over 4}\)</span></p>
<p><strong>这里没有对数据的分布进行假定，因为取这个矩形的时候就假定了某个小矩形所占的概率密度，不需要数据是均匀分布的</strong></p>
<p>那么，现在的数据集容量<span class="math inline">\(m=card(D_{train})\)</span>,这些点都不在某个小矩形的概率就是
<span class="math display">\[
(1-{\epsilon \over 4})^m
\]</span>
考虑每个点不在每个小矩形之间彼此独立（为什么？），那么’至少有一个小矩形没被分布到‘
就是四个上述事件的和事件， <span class="math display">\[
P(R(h)&gt;\epsilon)&lt;=\Sigma_1^4(1-{\epsilon \over 4})^m=4(1-{\epsilon
\over 4})^m
\]</span> 我们希望这种事情（误差大于阈值）的概率小于一个信心程度<span class="math inline">\(\delta\)</span> ,那么就有了 <span class="math display">\[
P(R(h)&gt;\epsilon)&lt;\delta
\]</span> 同时也是： <span class="math display">\[
P(R(h)&lt;=\epsilon)&gt;=1-\delta
\]</span></p>
<p>=&gt; <span class="math display">\[
4(1-{\epsilon \over 4})^m&lt;\delta
\]</span> 继续放缩</p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230929194054908.png" alt="image-20230929194054908" style="zoom:50%;"></p>
<p><span class="math display">\[
4(1−{\epsilon \over 4})^m ≤ 4 exp({−m \epsilon \over 4})&lt;\delta
\]</span> 解得： <span class="math display">\[
m&gt;= {4 \over \epsilon}*ln({4\over\delta})
\]</span> 揭示了训练样本量和泛化误差<span class="math inline">\(\epsilon\)</span>,信心指数<span class="math inline">\(\delta\)</span>的关系</p>
<p>（为什么放缩这么一家伙？）</p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230929194534646.png" alt="image-20230929194534646" style="zoom:50%;"></p>
<p>这里展示了一些三者之间的函数关系图，可以看到：</p>
<ul>
<li>数据量越大，泛化误差越低</li>
<li>数据量的增加有边际递减效应（law of diminishing returns)</li>
<li>信心指数<span class="math inline">\(\delta\)</span>越高，就需要越多的数据来降低同样的误差</li>
</ul>
<p>这样的一个泛化误差边界有这样的好处：</p>
<ul>
<li>对于任意的目标概念类<span class="math inline">\(C\)</span>(包括难学的)，任意的数据分布<span class="math inline">\(D\)</span>(包括对抗性生成的让学习更难的)都适用，因为没有依赖他们进行假设推导</li>
<li>我们此时研究的是
误差分布的最大，也就是图像的尾巴，而没有研究这个量收敛到尾部的情况）</li>
</ul>
<p>所以：</p>
<ul>
<li>经验预估检测错误率（empirically estimated test
errors）应该比这个低很多 （训练集上算的？）</li>
<li>这个也应该比实际的上界松很多，是一个非常general的上界</li>
</ul>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230929195059752.png" alt="image-20230929195059752" style="zoom:50%;"></p>
<h2 id="pac可学习理论对有限假设集">PAC可学习理论对有限假设集</h2>
<p>对于有限假设集<span class="math inline">\(H\)</span>，它满足以下条件：</p>
<ul>
<li>输入元素满足有限域（例如离散化的数值或布尔变量，即，可列举的）</li>
<li>假设的表示也是有有限空间的（变量重复有限次数）</li>
<li>用布尔代数运算符对付布尔公式的自己</li>
</ul>
<p>这个东西已经被统计学习理论研究清楚了</p>
<p>可以知道：</p>
<ul>
<li>样本复杂边界（Sample complexity bound），就是样本量。
即给定泛化误差阈值<span class="math inline">\(\epsilon\)</span>
和置信度<span class="math inline">\(\delta\)</span>，需要多大的样本量才行</li>
<li>泛化误差（ generalization error bound）。
给定样本量和置信度，可以达到多大的误差。</li>
</ul>
<p>当然这俩都得要你算出来有限假设集的大小才行</p>
<p>成立的前提是，这是一个一致假设，</p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230929205824865.png" alt="image-20230929205824865" style="zoom:67%;"></p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230929205745657.png" alt="image-20230929205745657" style="zoom:67%;"></p>
<p><strong>后面对于这个公式（<span class="math inline">\(m&gt;=\)</span>)给了证明，不过没太看懂</strong></p>
<p>下面给出例子进行详细说明</p>
<p>对于一种相对简单的情况：</p>
<h3 id="boolean-conjunctions">Boolean conjunctions</h3>
<p>（包括了 and 和 not的布尔公式，不能用or）</p>
<p>每一个变量是布尔变量</p>
<p>比如这样的输入：</p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230929205419126.png" alt="image-20230929205419126" style="zoom:50%;"></p>
<p>假设集大小为 <span class="math display">\[
|H|=3^d
\]</span> 每一个变量要么正，要么否，要么不出现——承认无关变量的可能</p>
<h3 id="任意布尔公式">任意布尔公式</h3>
<p>若有<span class="math inline">\(d\)</span>个变量，每一个都可以为正或负，那么输入空间<span class="math inline">\(X\)</span> <span class="math display">\[
|X| = 2^d
\]</span> 考虑到并的存在，一个假设<span class="math inline">\(h\)</span>就是一个子集<span class="math inline">\(S\)</span>的每一项并集</p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230929210542203.png" alt="image-20230929210542203" style="zoom:50%;"></p>
<p>对于<span class="math inline">\(X\)</span>中的每一个向量，都有挑选与否的可能：
<span class="math display">\[
|H|=2^{2^d}
\]</span> 计算其他东西用上面的公式就行了</p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230929210728950.png" alt="image-20230929210728950" style="zoom:50%;"></p>
<p>这个可以看出来，对<span class="math inline">\(d\)</span>是指数增长的，所以不认为是PAC-learnable</p>
<h2 id="证明大纲">证明大纲</h2>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230929211935506.png" alt="image-20230929211935506" style="zoom:50%;"></p>
<p>这页没看懂</p>
<p>后面基本都能看懂 先搁着把</p>
<p>下面还讨论了一点有限假设集不一致的情况，从公式来看是多了一点项</p>
<h2 id="参考">参考：</h2>
<p>https://theigrams.github.io/zjblog/2021/07/22/pac.html</p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Note/" rel="tag"># Note</a>
              <a href="/tags/Course/" rel="tag"># Course</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/" rel="prev" title="Note-CSE4650 - Methods of Data Mining">
                  <i class="fa fa-chevron-left"></i> Note-CSE4650 - Methods of Data Mining
                </a>
            </div>
            <div class="post-nav-item">
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Quasimoodo</span>
</div>
<!--
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>
-->

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  
<script src="https://cdn.jsdelivr.net/npm/hexo-generator-searchdb@1.4.0/dist/search.js" integrity="sha256-vXZMYLEqsROAXkEw93GGIvaB2ab+QW6w3+1ahD9nXXA=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>





  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdn.jsdelivr.net/npm/mathjax@3.2.1/es5/tex-mml-chtml.js","integrity":"sha256-hlC2uSQYTmPsrzGZTEQEg9PZ1a/+SV6VBCTclohf2og="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
