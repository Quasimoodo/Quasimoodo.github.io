<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>About Moby Dick</title>
    <url>/2022/05/24/About-Moby-Dick/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><blockquote>
<p>现在，除了一艘轻轻摇晃的船赋予你的摇摆不定的生命，你没有生命；船的生命借自于大海；大海的生命借自于上帝神秘难测的潮汐。可是，当这睡眠，这幻梦将你笼罩，你的手或脚要是稍微挪动一下——你的双手彻底松开——你就会在惊恐中能够恢复自己的本性。你就盘旋在笛卡尔的涡流之上了。而也许，恰当正午，又是响晴的天气，你便随着一声半带窒息的尖叫，穿过透明的空气，坠入夏天的海洋，再也没有浮上来。好好留神吧，你们这些泛神论者！——《白鲸》桅顶瞭望</p>
</blockquote>
<blockquote>
<p>There is no life in thee, now, expect that rocking life imparted by a gently rolling ship; by her, borrowed from the sea; by the sea, from the inscrutable tides of God. But while this sleep, this dream is on ye, move your foot or hand an inch, slip your hold at all; and your identity comes back in horror. Over Descartian vortices you hover. And perhaps, at mid-day, in the fairest weather, with one half-throttled shriek you drop through that transparent air into the summer sea, no more to rise for ever. Heed it well, ye Pantheists! ——&lt;Moby dick&gt; CHAPTER 35 The Mast-Head</p>
</blockquote>
]]></content>
      <categories>
        <category>Metaphysics</category>
      </categories>
      <tags>
        <tag>Moby Dick</tag>
        <tag>Reading</tag>
      </tags>
  </entry>
  <entry>
    <title>SPT-Code</title>
    <url>/2022/05/27/SPT-Code/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>这是《Sequence-to-Sequence Pre-Training for Learning Source Code Representations》的读书笔记</p>
<span id="more"></span>



<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>Pre-trained models 用于代码相关下游任务的应用时的 问题？</p>
<ol>
<li>仅用了Pre-trained encoder 但生成任务需要两个部件都预训练</li>
<li>现在许多Pre-trained model 包括T5，只是简单复用了NL的预训练任务，这要求NL-CODE的corpus 这使得数据受限</li>
</ol>
<p>为了应对这两个问题 提出了SPT-Code ，在微调后可以在5个代码相关任务上SOTA</p>
<p>这是一个seq2seq 预训练模型，通过三个预训练任务使得其能够学习到下面三点，并在下游任务中使用</p>
<ul>
<li>代码知识</li>
<li>对应代码结构</li>
<li>自然语言描述</li>
</ul>
<p>而不需要双语corpus</p>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>作者在第一部分提了自监督训练，然后说预训练模型的成功和这个有关系，下面谈预训练模型到软工SE任务的时候认为的问题是：</p>
<ul>
<li><p>主流预训练模型仅对encoder搞 ，不够理想</p>
<p>最有名的encoder也就是BERT吧 也确实就是用了MLM（Masked Language Modeling） 也确实有好多Pre-trained Bert 下面套个任务头或者另一个decoder就开干的……不过这应该算是蛮荒时代了 T5虽然好 但是对一般研究人员来说想改进模型architecture 不是那么容易？</p>
<ul>
<li><p>别人的解决：</p>
<p>T5-learning , TreeBERT 两个工作使得Encoder-Decoder jointly trainded</p>
</li>
</ul>
</li>
<li><p>这些预训练模型预设输入的是NL-CODE 忽视了代码结构</p>
<p>为什么呢？因为就是简单偷了NLP的拿来用</p>
<ul>
<li><p>别人的解决：</p>
<p>专门的预训练任务 包括预测数据流图中的边与对齐节点和代码 </p>
<p>——dataflow 有语义信息而无语法信息（AST）</p>
</li>
</ul>
</li>
<li><p>而且都假设有严格对齐的双语语料</p>
<ul>
<li><p>T5-learning :</p>
<p>分别处理两种输入，不要求语料库中展示二者的关系</p>
</li>
</ul>
</li>
</ul>
<p>—— 没有一个模型能够统一处理这三个问题</p>
<p>SPT-Code就可以！</p>
<ul>
<li>这是一个encoder-decoder共同预训练的模型</li>
<li>数据实例由CODE,AST,NL三部分构成</li>
<li>使用方法名和调用此方法的方式作为自然语言描述（以避免对bilingual corpus的依赖）</li>
</ul>
<p>方法：</p>
<p>设计了三种预训练任务，每一种获取一种数据信息</p>
<ul>
<li>改进的MASS-用于CODE：遮蔽Seq2Seq恢复</li>
<li>Code-AST Predict CAP：预测code-AST是否匹配</li>
<li>Method Name Generation MNG：生成 方法名的 子token</li>
</ul>
<p>数据集：</p>
<p>CodeSearchNet</p>
<p>贡献：</p>
<ol>
<li>提出了SPT-Code预训练模型，可用于分类和生成任务</li>
<li>使用了线性和简化的AST 第一个使用了NL&amp;AST作为输入对于预训练</li>
<li>通过输入表示和三个与训练任务使得预训练模型不依赖双语语料库（labeled data)</li>
<li>用未标注数据库在五个下游任务实现了SOTA</li>
</ol>
<h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><p>架构，输入和预训练任务，微调</p>
<h2 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h2><p>类似于BART和T5的典型Transformer</p>
<p>分类任务和生成任务，模型采用相同的输入：</p>
<ul>
<li>分类对encoder和decoder输入相同</li>
<li>生成采用传统方法</li>
</ul>
<img src="/2022/05/27/SPT-Code/image-20220527230530594.png" alt="image-20220527230530594" style="zoom:50%;">

<h2 id="输入"><a href="#输入" class="headerlink" title="输入"></a>输入</h2><img src="/2022/05/27/SPT-Code/image-20220527231806850.png" alt="image-20220527231806850" style="zoom:50%;">

<p>由三部分组成，每一部分[SEP]连接</p>
<h3 id="Code："><a href="#Code：" class="headerlink" title="Code："></a>Code：</h3><p>没有使用笨蛋tokenizer，而是用了stl for Python 或者 antlr for Java,Php.etc 其他的用了NLTK</p>
<h3 id="AST"><a href="#AST" class="headerlink" title="AST"></a>AST</h3><p>用的Tree-sitter 搞的 AST </p>
<p> 如何序列化AST?</p>
<ul>
<li><p>传统方法：SBT （Structure-Based Traversal)</p>
<p>比先序遍历之类的更有效，但可能产生过长的序列（可能超过代码三倍长）</p>
<img src="/2022/05/27/SPT-Code/image-20220527232629535.png" alt="image-20220527232629535" style="zoom:50%;">

<center>
    一种类似中序遍历的说法 来自那篇论文忘了 反正绝对看过
</center>


</li>
<li><p>本文的方法：X-SBT：XML-like SBT</p>
<p>可以减少超过一半的长度</p>
<img src="/2022/05/27/SPT-Code/image-20220527233143119.png" alt="image-20220527233143119" style="zoom:67%;">

<p>论文自带的图好看一点 这个创新点……只能说是情理之中，毕竟原来那个也太呆了（作者甚至还装模做样证了一下必然更短）</p>
<p>为了更短: AST——XSBT时，仅取表达式级别以上节点，放弃终结符</p>
<img src="/2022/05/27/SPT-Code/image-20220527233752649.png" alt="image-20220527233752649" style="zoom:50%;">

<p><strong>这种优化为可接受的，为什么呢？下面这个说得很漂亮：</strong></p>
<p>AST中包含了语法信息和词法信息，舍弃掉终结符丢失了词法信息，但之前的token都是词法单元，所以这个信息是没有丢掉的，因此改进可接受</p>
</li>
</ul>
<h3 id="NL"><a href="#NL" class="headerlink" title="NL"></a>NL</h3><p>难点：从仅有CODE中提取NL</p>
<p>方法：获取方法名与调用的API序列</p>
<p>对驼峰和下划线命名掰开</p>
<p>问题：怎么提取的API序列：从AST里偷出来的？</p>
<h2 id="预训练任务"><a href="#预训练任务" class="headerlink" title="预训练任务"></a>预训练任务</h2><p>𝐼𝑛𝑝𝑢𝑡 &#x3D; 𝐶,[SEP],𝐴,[SEP],𝑁 </p>
<h3 id="Code-AST-Prediction"><a href="#Code-AST-Prediction" class="headerlink" title="Code-AST Prediction."></a>Code-AST Prediction.</h3><p>这是第一个</p>
<p>在构建输入𝐼𝑛𝑝𝑢𝑡 时，一半是对应的AST,一半是随机的AST</p>
<h3 id="MASS"><a href="#MASS" class="headerlink" title="MASS"></a>MASS</h3><p>随机遮蔽C中的一部分，将所有遮蔽的token设置为[MASK]（改进前为对应数量个[MASK])</p>
<p>根据别人的论文，最大遮蔽长度是C长度l的一半</p>
<h3 id="Method-Name-Generation"><a href="#Method-Name-Generation" class="headerlink" title="Method Name Generation"></a>Method Name Generation</h3><p>希望可以通过这个任务学到代码的动机</p>
<p>代码名的词汇和对应代码总结的词汇由高度相关，因此希望通过改善 预测代码名 这一任务提升 代码总结 的能力</p>
<p>此任务的输入时，从𝐼𝑛𝑝𝑢𝑡中的C扣掉对应token，并在N中去掉前s个token（方法名总在最前），作为输入，试图让decoder输出扣掉的前s个token，即方法名</p>
<h2 id="微调"><a href="#微调" class="headerlink" title="微调"></a>微调</h2><p>端到端 根据不同任务分成两类，分类或生成，不同任务就缺掉一点输入</p>
<h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><h2 id="预训练"><a href="#预训练" class="headerlink" title="预训练"></a>预训练</h2><p>交代了数据集的数据使用，任务顺序，epoches，用的cross-entropy loss和Adam-W，batchsize和显卡（……）</p>
<p>Tokenizer Encoding 用的BPE对CODE和NL，在预训练data上干过 每个下游任务照用</p>
<p>问题：不是都有token了 还tokenize？</p>
<h2 id="下游任务微调"><a href="#下游任务微调" class="headerlink" title="下游任务微调"></a>下游任务微调</h2>]]></content>
      <categories>
        <category>Substance</category>
      </categories>
      <tags>
        <tag>ICSE 2022</tag>
        <tag>Code generation</tag>
        <tag>Note</tag>
      </tags>
  </entry>
  <entry>
    <title>火柴</title>
    <url>/2022/05/29/%E7%81%AB%E6%9F%B4/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>我在<sup id="fnref:6"><a href="#fn:6" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="火柴梗快要烧完的时候，会因为火焰过分接近而忍不住松手">[6]</span></a></sup>水中桥下<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="「尾生與女子期於梁下，女子不來，水至不去，抱樑柱而死。」《莊子·盜跖》
">[1]</span></a></sup> 饮酒<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label=" 「夢飲酒者，旦而哭泣；夢哭泣者，旦而田獵。方其夢也，不知其夢也。」《莊子·齊物論》
">[2]</span></a></sup>&#x2F;<del>忘相泉涸<sup id="fnref:4"><a href="#fn:4" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="「泉涸，魚相與處於陸，相呴以溼，相濡以沫，不如相忘於江湖。」《莊子·大宗師》">[4]</span></a></sup>前日的红烛<sup id="fnref:5"><a href="#fn:5" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="「蠟炬成灰淚始幹」 李商隐
">[5]</span></a></sup>泪眼<sup id="fnref:3"><a href="#fn:3" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="「文侯與虞人期獵。是日，飲酒樂，天雨。文侯將出，左右曰：「今日飲酒樂，天又雨，公將焉之？」文侯曰：「吾與虞人期獵，雖樂，豈可不一會期哉！」乃往，身自罷之。」《戰國策·魏策》
">[3]</span></a></sup></del>红烛淡忘镜中的泪眼</p>
<p>自己写完还解诗的人肯定是天下第一无聊 但这只不过是记录灵感媾和时的一些脉络和取舍。</p>
<span id="more"></span>


<hr>
<p>2022&#x2F;5&#x2F;30日凌晨睡不着床上对后半句做了修正</p>
<p>第二天起来看前天写得也太迫真了 简直就是笨蛋版本李商隐</p>
<p>当时是怎么想的呢？「忘相」是什么表达？生怕别人看不出来是你直接从庄子里偷来的？「泉涸」同理，太白太直了 反而失去了解读空间。「前日」也不明所以的。都要删掉。「红烛泪眼」是核心意象要留下来。</p>
<p>想要留下来的是什么呢？遗忘肯定是要有的，这是我给出的解答。互相也是要有的，起源就是由防风火柴想到的尾生抱柱抛出的一问。「红烛泪眼」没法共轭，还是拆开好了。</p>
<p>互相的话 就用镜子好了。破妄，吊诡，空间的拓展却又重复，自反中带有异质性。很好。</p>
<p>「红烛在镜中总是泪眼」？当时还开心的把手机翻出来赶紧记下，记完想想又觉得不好 啰啰嗦嗦的。</p>
<p>「红烛向镜中抛去泪眼」？我很喜欢这个动作带有的力量感，和伴随而来的主体性。用什么迎接你？以眼泪，以沉默。这是有力量的沉默。但是汉语还是半通不通的 忘记也没了。不好。</p>
<p>「忘记」这个字其实很好，自反又偏义，但是口语中用得太多了 读来感觉不到妙处 不好。</p>
<p>我想，烛火燃烧时上腾的青烟，蒸腾的雾气凝结成雨，落下化为沙尘，恰好就有一种复调式的演出效果。水汽也好，腾烟也好，怎么放在这里处理”忘“这个要素呢？想到了溶解，融化，但都用不好。这里卡了很久没想出来。</p>
<p>灵光一闪，就用「淡忘」。「淡」字自己就好像是拿来给水墨化开的，要是到token级别就是又有水又有火的自反，不管是前句的湖中还是镜子都能超距作用。「淡忘」本来不是什么僻词，但放在这里就妙得没话说。</p>
<p>「红烛淡忘镜中的泪眼」，真好。</p>
<p>「我」「饮酒」，「烛」「忘眼」。好像比兴一样的氛围，又构成了复调的演奏。「水」与「镜」，「泪」与「眼」，几乎每一个元素都能够进行笛卡尔式呼应。比兴之中，阅读顺序的先后带来的时序性还为文本增添了并列以外的递进因素，自问自答。很好。我很喜欢。</p>
<p>如果说昨天是向义山一样堆叠典故，这次就是处理意象了，也是很好玩呐。</p>
<p>下面又试着加点东西。一方面是平衡语感。这两句佶屈聱牙，像极了祭祀用的七言律诗，但是要是能像冯君一样，神神叨叨念完“长剑归来乎，”，令人不容小觑，立马接上一句“食无鱼”产生节目效果。那就可以说是非常成功了。</p>
<p>此外，也和我想要追求的吊诡氛围有点差异。水中饮酒还有镜子，好像是月光下的水晶湖一样，太明亮通透了些&#x2F;</p>
<p>加什么呢？四个字的好。「烟波浩渺」？我想到洞庭湖，蹭蹭湘君的隐喻正好在调上。「烟涛微茫」？直接偷过来好像也不坏。像是舞台布景的话，很明白的小舞台放在巨大的烟幕里，也是那个意思。</p>
<p>不过怎么放怎么感觉不妙，况且下来也不知怎么接手。先这么放着好了。<div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">「尾生與女子期於梁下，女子不來，水至不去，抱樑柱而死。」《莊子·盜跖》<a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">「夢飲酒者，旦而哭泣；夢哭泣者，旦而田獵。方其夢也，不知其夢也。」《莊子·齊物論》<a href="#fnref:2" rev="footnote"> ↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">「文侯與虞人期獵。是日，飲酒樂，天雨。文侯將出，左右曰：「今日飲酒樂，天又雨，公將焉之？」文侯曰：「吾與虞人期獵，雖樂，豈可不一會期哉！」乃往，身自罷之。」《戰國策·魏策》<a href="#fnref:3" rev="footnote"> ↩</a></span></li><li id="fn:4"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">4.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">「泉涸，魚相與處於陸，相呴以溼，相濡以沫，不如相忘於江湖。」《莊子·大宗師》<a href="#fnref:4" rev="footnote"> ↩</a></span></li><li id="fn:5"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">5.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">「蠟炬成灰淚始幹」 李商隐<a href="#fnref:5" rev="footnote"> ↩</a></span></li><li id="fn:6"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">6.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">火柴梗快要烧完的时候，会因为火焰过分接近而忍不住松手<a href="#fnref:6" rev="footnote"> ↩</a></span></li></ol></div></div></p>
]]></content>
      <categories>
        <category>Metaphysics</category>
      </categories>
      <tags>
        <tag>Poem</tag>
        <tag>Self</tag>
      </tags>
  </entry>
  <entry>
    <title>Follow Ace Taffy Meow！</title>
    <url>/2022/05/23/testpic/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Follow Ace Taffy thanks Meow!&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/05/23/testpic/image-20220524021029166.png" alt="image-20220524021029166"></p>
]]></content>
      <categories>
        <category>Meow</category>
      </categories>
      <tags>
        <tag>Ace</tag>
        <tag>Taffy</tag>
      </tags>
  </entry>
</search>
