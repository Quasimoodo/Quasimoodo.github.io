<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>About Moby Dick</title>
    <url>/2022/05/23/About-Moby-Dick/</url>
    <content><![CDATA[<blockquote>
<p>现在，除了一艘轻轻摇晃的船赋予你的摇摆不定的生命，你没有生命；船的生命借自于大海；大海的生命借自于上帝神秘难测的潮汐。可是，当这睡眠，这幻梦将你笼罩，你的手或脚要是稍微挪动一下——你的双手彻底松开——你就会在惊恐中能够恢复自己的本性。你就盘旋在笛卡尔的涡流之上了。而也许，恰当正午，又是响晴的天气，你便随着一声半带窒息的尖叫，穿过透明的空气，坠入夏天的海洋，再也没有浮上来。好好留神吧，你们这些泛神论者！——《白鲸》桅顶瞭望</p>
</blockquote>
<blockquote>
<p>There is no life in thee, now, expect that rocking life imparted by a gently rolling ship; by her, borrowed from the sea; by the sea, from the inscrutable tides of God. But while this sleep, this dream is on ye, move your foot or hand an inch, slip your hold at all; and your identity comes back in horror. Over Descartian vortices you hover. And perhaps, at mid-day, in the fairest weather, with one half-throttled shriek you drop through that transparent air into the summer sea, no more to rise for ever. Heed it well, ye Pantheists! ——&lt;Moby dick&gt; CHAPTER 35 The Mast-Head</p>
</blockquote>
]]></content>
      <categories>
        <category>Metaphysics</category>
      </categories>
      <tags>
        <tag>Moby Dick</tag>
        <tag>Reading</tag>
      </tags>
  </entry>
  <entry>
    <title>Bajwa OCL 相关笔记</title>
    <url>/2022/07/31/Bajwa-OCL-%E7%9B%B8%E5%85%B3%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<p>这是Bajwa 有关OCL 生成工作的笔记</p>
<span id="more"></span>

<p>endow 资助 赋予</p>
<p>expertise 专业知识</p>
<h1 id="2010"><a href="#2010" class="headerlink" title="2010"></a>2010</h1><p>OCL Constraints Generation from Natural Language Specification</p>
<p>通过英语生成约束和前后条件</p>
<p>主要问题：将自然语言形式化：SBVR</p>
<p>使用 LESSA [14]（用于语义分析的语言工程系统）方法对 NL 表示进行语义分析</p>
<p>解析自然语言，将自然语言的部分映射到对应的SBVR规则上</p>
<p>将名词，动词，形容词映射到类，实例，方法和属性，也有规则</p>
<p><img src="/2022/07/31/Bajwa-OCL-%E7%9B%B8%E5%85%B3%E7%AC%94%E8%AE%B0/image-20220806213704877.png" alt="image-20220806213704877"></p>
<p>读取XMI作为目标UML类模型的输入，将SBVR映射到类模型</p>
<h2 id="2012"><a href="#2012" class="headerlink" title="2012"></a>2012</h2><p>Translating natural language constraints to OCL</p>
<p>基本没什么区别 方法说的更详细一点 还补了几个经验性的判断</p>
<h1 id="2017"><a href="#2017" class="headerlink" title="2017"></a>2017</h1><p><strong>Automated Generation of OCL Constraints: NL based Approach vs Pattern Based Approach</strong></p>
<p>通过对自然语言的语法和语义分析 得到SBVR为基础的半正则表达，下来就很容易翻译成别的了。</p>
<p>研究趋势：</p>
<p>手动将英语约束映射到OCL</p>
<p>遇到的问题：</p>
<ol>
<li>用到的英文词汇必须是系统中有的</li>
<li>英语同样具有歧义性</li>
<li>如果英语没有问题 那么就可以自动生成了</li>
</ol>
<p>限制&#x2F;不足：</p>
<ul>
<li>NL不应包括UML类图以外的词汇，名字也应该一致</li>
<li>不完整，无效的就不行</li>
<li>不能包括UML聚合</li>
<li>不能有带参数的函数调用</li>
<li>异或关系不支持</li>
<li><img src="/2022/07/31/Bajwa-OCL-%E7%9B%B8%E5%85%B3%E7%AC%94%E8%AE%B0/image-20220803222739734.png" alt="image-20220803222739734"></li>
<li>query based 不可以</li>
<li>对于有歧义的句子不太行</li>
<li>需要用户手动纠正</li>
<li>只能处理一个句子</li>
</ul>
<p>OCL用的少：</p>
<p>introducing a template based language：</p>
<p>3 Wahler, M., “Patterns to Develop Consistent Design Constraints”, Ph.D. Thesis, ETH Zurich, Switzerland, 2007.</p>
<p>OCL困难的语法：</p>
<p>4 Gogolla, M., Büttner, F., and Richters, M., “USE: A UML-Based Specification Environment for Validating UML and OCL”, Science of Computer Programming, Volume 69, No. 1, pp. 27-34, 2007.</p>
<p>同义可以有多个实现</p>
<p>[5] Cabot, J., “Ambiguity Issues in OCL Postconditions”, Proceedings of 6th Conference OCL Workshop at the UML&#x2F;MoDELS, pp. 194-204, 2006. </p>
<p>[6] Kristofer, J., “Disambiguation Implicit Constructions in OCL”, Conference on OCL and Model Driven Engineering, Lisbon, Portugal, pp. 30-44, October 12, 2004.</p>
<p>大型软件建模中可能有OCL的理解性问题</p>
<p>[7] Correa A., Werner, C., and Barros, M., “An Empirical Study of the Impact of OCL Smells and Refactorings on the Understandability of OCL Specifications”, MODELS, LNCS 4735, pp. 76-90, 2007.</p>
<p>template based approach (the Copacabana [27] too</p>
<p>Raj, A., Prabharkar, T., and Hendryx, S., “Transformation of SBVR Business Design to UML Models”, ACM Conference on India Software Engineering, pp. 29-38, 2008.</p>
<h1 id="Constraint-Detection-in-Natural-Language-Problem-Descriptions"><a href="#Constraint-Detection-in-Natural-Language-Problem-Descriptions" class="headerlink" title="Constraint Detection in Natural Language Problem Descriptions"></a>Constraint Detection in Natural Language Problem Descriptions</h1><p>rigorous 严密的</p>
<p>alleviate 减轻 缓和</p>
<p>tailored 定做的 时尚的</p>
<p>substantial 重要的</p>
<p>insubstantial 毫无疑问的</p>
<p>目的：</p>
<p>Automated model reformulation aims at assisting a naive user in modeling constraint problems.</p>
<p>做的事：</p>
<p>detecting constraints in natural language problem descriptions using a structured-output classifier.</p>
<p>检测文本中描述约束的部分？？</p>
<p>主要的工作内容就是使用SVM-HMM进行了对于输入文本中，标注出描述的约束部分，算是一个文本提取&#x2F;标记问题</p>
<h1 id="Automating-Inference-of-OCL-Business-Rules-from-User-Scenarios"><a href="#Automating-Inference-of-OCL-Business-Rules-from-User-Scenarios" class="headerlink" title="Automating Inference of OCL Business Rules from User Scenarios"></a>Automating Inference of OCL Business Rules from User Scenarios</h1><p>OCL的作用：</p>
<p>OCL could be employed in several other cases such as (1) to describe pre- and post conditions of operations, (2) to give restrictions as guards in a state transition system and (3) to query over a given system state with OCL queries [4].</p>
<p>做的事：</p>
<p>generate the ocl business rules, invariants  from the conceptual model for the designer,use the predefined ocl invariant pattern and the snapshots of the conceputal model</p>
<h1 id="Bidirectional-Translation-between-OCL-and-JML-for-Round-trip-Engineering"><a href="#Bidirectional-Translation-between-OCL-and-JML-for-Round-trip-Engineering" class="headerlink" title="Bidirectional Translation between OCL and JML for Round-trip Engineering"></a>Bidirectional Translation between OCL and JML for Round-trip Engineering</h1><p>OCL 和 JML 的双向翻译 保持OCL的原有形态</p>
<h1 id="OCL-Constraints-Automatic-Generation-for-UML-Class-Diagram"><a href="#OCL-Constraints-Automatic-Generation-for-UML-Class-Diagram" class="headerlink" title="OCL Constraints Automatic Generation for UML Class Diagram"></a>OCL Constraints Automatic Generation for UML Class Diagram</h1><p>use predefined template and lexcial analyse to generate simple ocl constraints for input uml xmi</p>
<h1 id="Generating-OCL-Constraints-from-Test-Case-Schemas-for-Testing-Model-Behavior"><a href="#Generating-OCL-Constraints-from-Test-Case-Schemas-for-Testing-Model-Behavior" class="headerlink" title="Generating OCL Constraints from Test Case Schemas for Testing Model Behavior"></a>Generating OCL Constraints from Test Case Schemas for Testing Model Behavior</h1><p>形式化合约的重要性——引出OCL</p>
<p>项目验证，测试用例生成</p>
<p>OCL是轻量化的合约——OCL重要性</p>
<p>现状：难写难读，</p>
<p>需要一个辅助写对的东西</p>
<p>干这个事情的难点：</p>
<p>NL的二义性</p>
<p>本身不那么直接对应——OO-类</p>
<p>六点……</p>
<p><img src="/2022/07/31/Bajwa-OCL-%E7%9B%B8%E5%85%B3%E7%AC%94%E8%AE%B0/image-20220807203515349.png" alt="image-20220807203515349"></p>
<p>现有工作对难点的应对情况：那些解决好，哪些解决不好：限制分析清楚</p>
<p>我们的工作提出方法 应对他们的缺陷</p>
<p><img src="/2022/07/31/Bajwa-OCL-%E7%9B%B8%E5%85%B3%E7%AC%94%E8%AE%B0/image-20220807203635745.png" alt="image-20220807203635745"></p>
<p><img src="/2022/07/31/Bajwa-OCL-%E7%9B%B8%E5%85%B3%E7%AC%94%E8%AE%B0/image-20220809130755249.png" alt="image-20220809130755249"></p>
<p><img src="/2022/07/31/Bajwa-OCL-%E7%9B%B8%E5%85%B3%E7%AC%94%E8%AE%B0/image-20220814202922555.png" alt="image-20220814202922555"><img src="/2022/07/31/Bajwa-OCL-%E7%9B%B8%E5%85%B3%E7%AC%94%E8%AE%B0/image-20220814202948326.png" alt="image-20220814202948326"></p>
]]></content>
      <categories>
        <category>Substance</category>
      </categories>
      <tags>
        <tag>OCL</tag>
        <tag>Note</tag>
      </tags>
  </entry>
  <entry>
    <title>CS224N</title>
    <url>/2022/07/24/CS224N/</url>
    <content><![CDATA[<p>这是Stanford 课程CS224N的学习笔记 可能还有一些别的。<br>Winter, 2021 <a href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1214/">https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1214/</a></p>
<span id="more"></span>
<p>enamor 迷恋</p>
<p>zoom in&#x2F;out 缩放</p>
<p>fiddle 调整，伪造</p>
<p>likehood 似然——给定结果求某参数的可能性</p>
<p>open region 开区间</p>
<p>amplify 放大（扩增）</p>
<p>partial derivative 偏导数</p>
<p>denominator 分母</p>
<p>arithmetic 算术</p>
<p>analogy 类比 比喻</p>
<h1 id="L1"><a href="#L1" class="headerlink" title="L1"></a>L1</h1><p>最早：类似于WordNet：手工编写的词汇关系-hierathy and synomy ——依靠劳动，无法随时更新，无法衡量相似性</p>
<p>——离散的单独符号表示——one hot encode 高维向量——希望可以用稠密向量dense vector衡量相似性</p>
<p><strong>morden statical NLP:</strong></p>
<p>distributional semantic——Represent a word meaning by its context(a fix window)</p>
<p>不可能由人类来手工编写：</p>
<h2 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec:"></a>Word2Vec:</h2><h3 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h3><p>给定中心词，最大化周围context 的概率：(i.e. 调整中心词向量)</p>
<p>条件概率，左右词汇的概率连乘</p>
<p>目标是最小化损失函数：</p>
<p>-log avg</p>
<p>log  是因为处理和比积方便</p>
<img src="/2022/07/24/CS224N/image-20220802174103357.png" alt="image-20220802174103357" style="zoom:50%;">

<h3 id="如何计算条件概率？"><a href="#如何计算条件概率？" class="headerlink" title="如何计算条件概率？"></a>如何计算条件概率？</h3><ul>
<li>根据用途 用两个向量表示一个词</li>
<li>正则化除法用的是整个词典的概率</li>
</ul>
<p><img src="/2022/07/24/CS224N/image-20220802181244780.png" alt="image-20220802181244780" style="zoom: 50%;"><img src="/2022/07/24/CS224N/image-20220802184259606.png" alt="image-20220802184259606" style="zoom:50%;"></p>
<h3 id="所以要如何得到向量呢？"><a href="#所以要如何得到向量呢？" class="headerlink" title="所以要如何得到向量呢？"></a>所以要如何得到向量呢？</h3><p>想要得到向量 就是要优化模型，考虑向量维数D，每个词向量个数2和词典大小V，就是有2DV的总参数量需要优化，使用梯度下降</p>
<p><img src="/2022/07/24/CS224N/image-20220802185657846.png" alt="image-20220802185657846"></p>
]]></content>
      <categories>
        <category>Substance</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>Course</tag>
      </tags>
  </entry>
  <entry>
    <title>Philip Larkin 诗鉴赏</title>
    <url>/2022/06/04/Philip-Larkin/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>今天读到了Philip Larkin(菲利普·拉金) 觉得他确实写出了一些人类的共有困境，是超越东西方的 也不好说是现代视角或者古典视角。用诗歌描述一种……symptom？没有意象的堆叠或者是故作惊人之语。恰到好处的建筑与音韵意识又不喧宾夺主，克制的情感流露并着个人色彩。 类似于散文诗？</p>
<span id="more"></span>
<p>现摘在这里这一首，「Love Songs in Age」<br>She kept her songs, they kept so little space,<br>The covers pleased her:<br>One bleached<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="floating, drift,漂白
">1</span></a></sup> from lying in a sunny place,<br>One marked in circles by a vase of water,<br>One mended<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="repair,patch,修补
">2</span></a></sup>, when a tidy fit had seized her,<br>And coloured, by her daughter -<br>So they had waited, till, in widowhood<br>She found them, looking for something else, and stood</p>
<p>Relearning how each frank submissive chord<sup id="fnref:3"><a href="#fn:3" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="和弦
">3</span></a></sup><br>Had ushered<sup id="fnref:4"><a href="#fn:4" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="（迎宾员式的）引导
">4</span></a></sup> in<br>Word after sprawling hyphenated<sup id="fnref:5"><a href="#fn:5" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="连字符
[^6 ]:great brightness,耀眼的
">5</span></a></sup> word,<br>And the unfailing sense of being young<br>Spread out like a spring-woken tree, wherein<br>That hidden freshness sung,<br>That certainty of time laid up in store<br>As when she played them first. But, even more,</p>
<p>The glare<sup id="fnref:6"><a href="#fn:6" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label>6</span></a></sup> of that much-mentionned brilliance, love,<br>Broke out, to show<br>Its bright incipience<sup id="fnref:7"><a href="#fn:7" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="beginning to exist or to be apparent
">7</span></a></sup> sailing above,<br>Still promising to solve, and satisfy,<br>And set unchangeably in order. So<br>To pile them back, to cry,<br>Was hard, without lamely<sup id="fnref:8"><a href="#fn:8" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="in a weak and unconvincing manner">8</span></a></sup> admitting how<br>It had not done so then, and could not now.</p>
<div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">floating, drift,漂白<a href="#fnref:1" rev="footnote">↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">repair,patch,修补<a href="#fnref:2" rev="footnote">↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">和弦<a href="#fnref:3" rev="footnote">↩</a></span></li><li id="fn:4"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">4.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">（迎宾员式的）引导<a href="#fnref:4" rev="footnote">↩</a></span></li><li id="fn:5"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">5.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">连字符
[^6 ]:great brightness,耀眼的<a href="#fnref:5" rev="footnote">↩</a></span></li><li id="fn:7"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">7.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">beginning to exist or to be apparent<a href="#fnref:7" rev="footnote">↩</a></span></li><li id="fn:8"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">8.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">in a weak and unconvincing manner<a href="#fnref:8" rev="footnote">↩</a></span></li></ol></div></div>]]></content>
      <categories>
        <category>Metaphysics</category>
      </categories>
      <tags>
        <tag>Poem</tag>
        <tag>Literary criticism</tag>
        <tag>Philip Larkin</tag>
      </tags>
  </entry>
  <entry>
    <title>SPT-Code</title>
    <url>/2022/05/27/SPT-Code/</url>
    <content><![CDATA[<p>这是《Sequence-to-Sequence Pre-Training for Learning Source Code Representations》的读书笔记</p>
<span id="more"></span>



<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>Pre-trained models 用于代码相关下游任务的应用时的 问题？</p>
<ol>
<li>仅用了Pre-trained encoder 但生成任务需要两个部件都预训练</li>
<li>现在许多Pre-trained model 包括T5，只是简单复用了NL的预训练任务，这要求NL-CODE的corpus 这使得数据受限</li>
</ol>
<p>为了应对这两个问题 提出了SPT-Code ，在微调后可以在5个代码相关任务上SOTA</p>
<p>这是一个seq2seq 预训练模型，通过三个预训练任务使得其能够学习到下面三点，并在下游任务中使用</p>
<ul>
<li>代码知识</li>
<li>对应代码结构</li>
<li>自然语言描述</li>
</ul>
<p>而不需要双语corpus</p>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>作者在第一部分提了自监督训练，然后说预训练模型的成功和这个有关系，下面谈预训练模型到软工SE任务的时候认为的问题是：</p>
<ul>
<li><p>主流预训练模型仅对encoder搞 ，不够理想</p>
<p>最有名的encoder也就是BERT吧 也确实就是用了MLM（Masked Language Modeling） 也确实有好多Pre-trained Bert 下面套个任务头或者另一个decoder就开干的……不过这应该算是蛮荒时代了 T5虽然好 但是对一般研究人员来说想改进模型architecture 不是那么容易？</p>
<ul>
<li><p>别人的解决：</p>
<p>T5-learning , TreeBERT 两个工作使得Encoder-Decoder jointly trainded</p>
</li>
</ul>
</li>
<li><p>这些预训练模型预设输入的是NL-CODE 忽视了代码结构</p>
<p>为什么呢？因为就是简单偷了NLP的拿来用</p>
<ul>
<li><p>别人的解决：</p>
<p>专门的预训练任务 包括预测数据流图中的边与对齐节点和代码 </p>
<p>——dataflow 有语义信息而无语法信息（AST）</p>
</li>
</ul>
</li>
<li><p>而且都假设有严格对齐的双语语料</p>
<ul>
<li><p>T5-learning :</p>
<p>分别处理两种输入，不要求语料库中展示二者的关系</p>
</li>
</ul>
</li>
</ul>
<p>—— 没有一个模型能够统一处理这三个问题</p>
<p>SPT-Code就可以！</p>
<ul>
<li>这是一个encoder-decoder共同预训练的模型</li>
<li>数据实例由CODE,AST,NL三部分构成</li>
<li>使用方法名和调用此方法的方式作为自然语言描述（以避免对bilingual corpus的依赖）</li>
</ul>
<p>方法：</p>
<p>设计了三种预训练任务，每一种获取一种数据信息</p>
<ul>
<li>改进的MASS-用于CODE：遮蔽Seq2Seq恢复</li>
<li>Code-AST Predict CAP：预测code-AST是否匹配</li>
<li>Method Name Generation MNG：生成 方法名的 子token</li>
</ul>
<p>数据集：</p>
<p>CodeSearchNet</p>
<p>贡献：</p>
<ol>
<li>提出了SPT-Code预训练模型，可用于分类和生成任务</li>
<li>使用了线性和简化的AST 第一个使用了NL&amp;AST作为输入对于预训练</li>
<li>通过输入表示和三个与训练任务使得预训练模型不依赖双语语料库（labeled data)</li>
<li>用未标注数据库在五个下游任务实现了SOTA</li>
</ol>
<h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><p>架构，输入和预训练任务，微调</p>
<h2 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h2><p>类似于BART和T5的典型Transformer</p>
<p>分类任务和生成任务，模型采用相同的输入：</p>
<ul>
<li>分类对encoder和decoder输入相同</li>
<li>生成采用传统方法</li>
</ul>
<img src="/2022/05/27/SPT-Code/image-20220527230530594.png" alt="image-20220527230530594" style="zoom:50%;">

<h2 id="输入"><a href="#输入" class="headerlink" title="输入"></a>输入</h2><img src="/2022/05/27/SPT-Code/image-20220527231806850.png" alt="image-20220527231806850" style="zoom:50%;">

<p>由三部分组成，每一部分[SEP]连接</p>
<h3 id="Code："><a href="#Code：" class="headerlink" title="Code："></a>Code：</h3><p>没有使用笨蛋tokenizer，而是用了stl for Python 或者 antlr for Java,Php.etc 其他的用了NLTK</p>
<h3 id="AST"><a href="#AST" class="headerlink" title="AST"></a>AST</h3><p>用的Tree-sitter 搞的 AST </p>
<p> 如何序列化AST?</p>
<ul>
<li><p>传统方法：SBT （Structure-Based Traversal)</p>
<p>比先序遍历之类的更有效，但可能产生过长的序列（可能超过代码三倍长）</p>
<img src="/2022/05/27/SPT-Code/image-20220527232629535.png" alt="image-20220527232629535" style="zoom:50%;">

<center>
    一种类似中序遍历的说法 来自那篇论文忘了 反正绝对看过
</center>

</li>
<li><p>本文的方法：X-SBT：XML-like SBT</p>
<p>可以减少超过一半的长度</p>
<img src="/2022/05/27/SPT-Code/image-20220527233143119.png" alt="image-20220527233143119" style="zoom:67%;">

<p>论文自带的图好看一点 这个创新点……只能说是情理之中，毕竟原来那个也太呆了（作者甚至还装模做样证了一下必然更短）</p>
<p>为了更短: AST——XSBT时，仅取表达式级别以上节点，放弃终结符</p>
<img src="/2022/05/27/SPT-Code/image-20220527233752649.png" alt="image-20220527233752649" style="zoom:50%;">

<p><strong>这种优化为可接受的，为什么呢？下面这个说得很漂亮：</strong></p>
<p>AST中包含了语法信息和词法信息，舍弃掉终结符丢失了词法信息，但之前的token（Input中Code的部分）都是词法单元，所以这个信息是没有丢掉的，因此改进可接受</p>
</li>
</ul>
<h3 id="NL"><a href="#NL" class="headerlink" title="NL"></a>NL</h3><p>难点：从仅有CODE中提取NL</p>
<p>方法：获取方法名与调用的API序列</p>
<p>对驼峰和下划线命名掰开</p>
<p>问题：怎么提取的API序列：从AST里偷出来的？</p>
<p>——应该就是</p>
<h2 id="预训练任务"><a href="#预训练任务" class="headerlink" title="预训练任务"></a>预训练任务</h2><p>𝐼𝑛𝑝𝑢𝑡 &#x3D; 𝐶,[SEP],𝐴,[SEP],𝑁 </p>
<h3 id="Code-AST-Prediction"><a href="#Code-AST-Prediction" class="headerlink" title="Code-AST Prediction."></a>Code-AST Prediction.</h3><p>这是第一个</p>
<p>在构建输入𝐼𝑛𝑝𝑢𝑡 时，一半是对应的AST,一半是随机的AST</p>
<h3 id="MASS"><a href="#MASS" class="headerlink" title="MASS"></a>MASS</h3><p>随机遮蔽C中的一部分，将所有遮蔽的token设置为[MASK]（改进前为对应数量个[MASK])</p>
<p>根据别人的论文，最大遮蔽长度是C长度l的一半</p>
<h3 id="Method-Name-Generation"><a href="#Method-Name-Generation" class="headerlink" title="Method Name Generation"></a>Method Name Generation</h3><p>希望可以通过这个任务学到代码的动机</p>
<p>代码名的词汇和对应代码总结的词汇由高度相关，因此希望通过改善 预测代码名 这一任务提升 代码总结 的能力</p>
<p>此任务的输入时，从𝐼𝑛𝑝𝑢𝑡中的C扣掉对应token，并在N中去掉前s个token（方法名总在最前），作为输入，试图让decoder输出扣掉的前s个token，即方法名</p>
<h2 id="微调"><a href="#微调" class="headerlink" title="微调"></a>微调</h2><p>端到端 根据不同任务分成两类，分类或生成，不同任务就缺掉一点输入</p>
<h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><h2 id="预训练"><a href="#预训练" class="headerlink" title="预训练"></a>预训练</h2><p>交代了数据集的数据使用，任务顺序，epoches，用的cross-entropy loss和Adam-W，batchsize和显卡（……）</p>
<p>Tokenizer Encoding 用的BPE对CODE和NL，在预训练data上干过 每个下游任务照用</p>
<p>预训练任务的任务量都是每个任务几十个Epoches的量级。</p>
<p>问题：不是都有token了 还tokenize？</p>
<p>——低级问题，前面的应该是tokenize，这里进行token&#x3D;&gt;input_ids的步骤</p>
<h2 id="下游任务微调"><a href="#下游任务微调" class="headerlink" title="下游任务微调"></a>下游任务微调</h2><p>介绍了五个任务 其中介绍部分有点尴尬</p>
<h2 id="评估"><a href="#评估" class="headerlink" title="评估"></a>评估</h2><p>RQ1:相比于其他较好的基线 这个性能在下游任务如何？</p>
<p>列个表 比不上人家的扯一点</p>
<p>RQ2:三个预训练任务对五个下游任务分别有什么贡献？</p>
<p>——消融实验</p>
<p>有趣的是 删除掉MNG （生成 方法的token）在代码完成和代码修复上性能有所提高：</p>
<ul>
<li>MNG是目标自然语言的预训练，而这两项任务都是代码到代码</li>
</ul>
<p>分析一下 什么任务对什么下游有影响</p>
<p><strong>稍微有点水平的问题</strong></p>
<p>RQ3: 可以利用更多无标记的资料是不是本模型的优点呢？</p>
<p>相较于别的预训练模型，由于它的设计，可以使用无标注数据。更好的性能是不是来自于更多的数据呢？（而不是模型本身厉害？）</p>
<p>在同样的数据集上训练——把它当作无标注的——其实和别的比还算吃亏——也能够取得相对别的模型更好的结果。</p>
<p>可以说是赢两遍了。</p>
<p>RQ4:微调阶段的数据量对下游任务有什么影响？</p>
<p>虽然越小越坏，但是很小也和别的模型差不多 说明真好</p>
<h2 id="定性分析与定量分析"><a href="#定性分析与定量分析" class="headerlink" title="定性分析与定量分析"></a>定性分析与定量分析</h2><p>定量：志愿者评估，多个样本分类列表个</p>
<p>定性：在哪些任务哪些方面表现好 不好的怎么不好</p>
<h1 id="威胁分析"><a href="#威胁分析" class="headerlink" title="威胁分析"></a>威胁分析</h1><p>构造：数据集可能有重复</p>
<p>内部：没调过超参数：所以可能有更好的</p>
<p>外部：只用了CodesearchNet</p>
<h1 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h1><p>我们介绍了SPT-Code，这是一个基于编码器架构的源代码的大型型号。首先，我们为预训练SPT代码设计了三个特定代码的预训练任务。其次，我们提出了一种新的输入表示形式，它是第一个考虑自然语言和AST形式的方法，我们还提出了AST遍历方法的改进版本XSBT。我们的预训练任务和输入表示形式都允许在完全未标记的数据集上预先训练SPT代码。然后，对五个与代码相关的下游任务进行了微调。结果表明，微调SPT代码使其能够在五个与代码相关的下游任务上实现最新性能。消融实验表明，这三个预训练任务对不同的下游任务具有不同程度的影响，AST和自然语言输入也有助于提高SPTCODE的性能。为了促进未来的研究，我们还可以在<a href="https://github.com/">https://github.com/</a> nougatca&#x2F;spt-code上公开提供代码和其他。</p>
]]></content>
      <categories>
        <category>Substance</category>
      </categories>
      <tags>
        <tag>Note</tag>
        <tag>ICSE 2022</tag>
        <tag>Code generation</tag>
      </tags>
  </entry>
  <entry>
    <title>「政治的人生」读后感</title>
    <url>/2022/07/11/%E3%80%8C%E6%94%BF%E6%B2%BB%E7%9A%84%E4%BA%BA%E7%94%9F%E3%80%8D%E8%AF%BB%E5%90%8E%E6%84%9F/</url>
    <content><![CDATA[<p>本篇为王沪宁所著「政治的人生」读后感</p>
<span id="more"></span>

<p>这是第一篇利用零碎时间读完的书 不过能读完还是拖了看见大段念经就不少跳过的福……写这篇读后感的时候正在等头发干 不然早睡觉去了。欧阳公读书有「三上」之说，我又何妨写字有「三干」一谓？至于后两干，慢慢再补起来。</p>
<p>如果想写成一篇质量又高，读完陌生者又有收获的读后感，大概读的时候就得做上不少笔记，便签；写的时候还得布局谋篇，不时再去重新寻章摘句。然而 读的时候就没有字字珠玑，写的时候便更要信马由缰了。</p>
<p>开头的自序之中 他说他喜欢在夜深人静的时候静静的独自思考思考，「是一种思想享受」，也是「一点新的空间」。他说，整天搞政治学研究之外，还需要有一点自己的空余思考 这种思考于个人于事业都是有益的。这种说法听起来非常像孟子的「养夜气」之说。</p>
<p>什么是夜气呢？这是孟子对于自己善性论的 某种程度上的一个补丁。性善，意即人人本质之中都具有「善」的种子 具有感受善，追求善进而将其发扬光大的能力。既然人人都有皆有善端，那么为什么世界上有那么多的不善之人呢？这就是和后天的涵养功夫有关。如同牛山上的树林一样，天生繁茂，却因为毗邻大国，而被砍成了光秃秃的样子，难道光秃秃就是山的本貌吗？人也是这般，虽有善端，然而放逐了良心，也就失去了他。黎明时呼吸的清气，他与常人好恶相近，然而白天的所作所为又将其丢失了。夜气清明，然而却没有将其涵养，别人看他也就成了禽兽，然而这禽兽却也并不是他的本质。</p>
<p>其实在这段阐述之中，孟子引入了许多模糊而重要的假设，甚至摸到了一些更重要的命题 不知道为什么没有进行更深入的讨论。为什么看起来是个坏蛋，夜风一吹善端又起呢？这个「夜气」究竟是什么东西呢？是某种外于人的东西给予了人善念？这种说法大概可以排除，不仅与学说不符，还沾点自然神信仰。如果说是社会活动，那么，由人组成的社会又何以如同斧斤伤人善念呢？是因为王道不昭善政不行？（笑）一个人既然有善端 为什么在他经过理性的自由无意识活动的结果反而是将其损害呢？难道说，孟子的意思是，人之善并不为零，这样的悲观语调吗？与本篇过远 此处按下不表。</p>
<blockquote>
<p>操则存，舍则亡；出入无时，莫知其乡</p>
</blockquote>
<p>如果果然按照孟子的说法，白天人在忙于各种社会性事务或者是解决生存问题，在晚上独处的时候才会升腾起清气反观自身的话，那么王教授怎么深更半夜写日记都有超过百分之六十的篇幅在讨论相当专业的事情呢？（看到王沪宁甚至在日记里还是XX性XX性成片的排比念经我的心中甚至惊叹盖过了崩溃）虽说，看到他从私事，杂感到「念经」转换的相当自然，我更倾向于这是他生活习惯的一种继续。不过，王沪宁自己也说，期望在这里得到一点闲暇的，个人的时间，从而得到某种启发。那么，这些内容其实已经是他日常以外的第二视角审视了？其实，他要是能稍稍强迫自己不在日记里还是大段地讨论那些「抽象」问题，恐怕还能有更多的收获。就像他自己在日记中举得例子：</p>
<blockquote>
<p>忽然想到一篇法国小说《月光》，说的是一位神 父从来就不能理解他的侄女的行为，也不理解年轻 人为什么要谈情说爱，而不全心全意地把自己奉献 给上帝。一天夜里，他发现侄女又在与一位小伙子相 会，顿时大怒，从教堂里冲出来。突然，他停下了脚 步，他发现月光是那样皎洁，那样美妙，他心中产生 了一个问题“：如果年轻人们干的事情是罪恶，是上帝不允许的，为什么今夜的月光会这样的明亮？为什 么上帝会在今天晚上带来这样通明的月光呢？”于是，他意识到那不是罪恶，而是一种美好。他就回去，安静地入睡了。</p>
</blockquote>
<p>他自己将其归结于「自然」的教育。其实，哪一天晚上没有月光？哪一处又没有幽会的情侣？然而，正是心头偶然闪过的一丝明悟，才能提出那一问题而不是执迷不悟。善端的昭显，长久的涵养与天赐的一点灵光，缺一不可。没有善端，感受不到美，千万条路同归死路；没有长久之涵养，大概会「宁可丢掉百体中的一体，不叫全身下火狱」（逃）；而没有灵光一闪，壁垒就永远无法被打破。</p>
<p>——add 2022.8.8——</p>
<p>中间过的时间实在是太长了 当时想要写的东西许多怕是都想不起来了：那些草蛇灰线，仿佛朝阳斜晖下才能嗅到的隐秘香气……不过照着原来有深刻印象的几个片段继续写还是没问题的。</p>
<p>读着他的书，心里却想着孔子。果然孔子是任何年代一个稍有知识的中国人在试图进行任意主题的深入思考都无法绕过去的主题。看他写孔子：</p>
<blockquote>
<p>对孔子的思想，青年人不太理解，不知道在中国这样一种文化氛围内，儒家的思想有它存在的理由，不是人为力量可以清除。他不仅是一种思想体系，也是一种生活方式的反映。在中国社会中，人们一直强调从人的内心世界来协调人们之间的关系，而不是从外部世界。所有的秩序，政治的也好，经济的也好，社会的也好，最终是一种精神上的秩序。在一种精神上的秩序没有形成之前，任何外在的秩序均不牢固。这就是儒家中最有价值的部分。一个世界不能没有秩序，也不能没有伦理秩序，儒家比较强调着一点。问题不再用什么样的内容来构筑秩序，而在于需要秩序本身。</p>
</blockquote>
<p>他写文章在1994年，那一年中国社会的思想风潮大概是什么样子呢？想了想根本没本事回答这个问题(笑)。从建国后的尊法批孔，到现在的”儒学复兴“，中间如果有一个明确的节点，大概是在什么时候呢？改革开放之后到八十年代末大概不会是答案。</p>
<p>读孔子，王读出来的是秩序：从内心世界来协调人的关系，进而构建出社会的伦理秩序——甚至对秩序的内容也可以不那样在乎，而要强调秩序本身。那么，儒家的秩序又是什么样的呢？也没本事回答这个问题）这里先不说老愤青引用的那个</p>
<blockquote>
<p>人有十等。下所以事上，上所以共神也。故王臣公，公臣大夫，大夫臣士，士臣皁，皁臣舆，舆臣隶，隶臣僚，僚臣仆，仆臣台 </p>
</blockquote>
<p>在我的了解中，儒家秩序的构建核心纽带之一是「亲亲」，有了亲亲下来才有孝悌，长幼尊卑。儒家重视人伦关系，将夫妻视为最重要的社会关系（可能没有之一？）以「礼」作为方式，来维护人伦秩序。礼记有云，「昏礼者，礼之本也」。下来才是如同同心圆一样向外放射进行家国同构。</p>
<p>那么 强调秩序，王心中的秩序是什么样子的呢？作为坚挺三届而不衰的笔杆子，这个倒是不难猜测。此外，他还引用过《停滞的帝国》对乾隆的评论，说他一元体制是维护封建政权的唯一把手，所以牢牢把握不松开，也造成了国家的封闭。对于这样的观点，王没有评论。</p>
<p>王读孔子，读出的是秩序与稳定；然而有人读孔子，却能读出人的尊严与理性。相比较于孔子的润物无声，其实儒家里面孟子是相当能打动人的。如果说「富贵不能淫贫贱不能移威武不能屈」还能从孔子哪里找到「富與貴是人之所欲也，<em>不以其道得之</em>，不處也；貧與賤是人之所惡也，<em>不以其道得之</em>，不去也。」的引证，那么，像「自反而缩，虽千万人吾往矣」这般的浩然正气就几乎让君子成了庄子笔下的冰肌玉肤，餐风饮露，去以六月息的神人。然而潇洒之下的隐忧，却只有引入了现代性的视角才能看得到：论语说，「邦有道，危言危行；邦无道，危行言孙」。养浩然正气，精神上无比自由强大的君子，在无道之国之中却连话也不能说，无比自由的人却可以无比压抑。前一阵子看克里希纳木提的书，就记住他说的一个”人在青年时候最需要的是免于恐惧。“其实，孔子的「仁」也绝不是一门压抑生命力的学说：「麻木不仁」者，无知觉也。然而，就是这样一门崇尚生命发展的理念，却同样无法远离无道之国的恐惧。</p>
<p>来到孟子，这样样一个强调现实关怀与此岸生活的人，在被问及禅让之时，也只能有如下的对话：</p>
<blockquote>
<p>：尧把天下禅让给舜，有这样的事吗？</p>
<p>不行，天子不能把天下给别人。</p>
<p>：那舜怎么有的天下呢？</p>
<p>是天给的</p>
<p>：天又不会说话，怎么给呢？</p>
<p>天通过事情展示出来的。尧把舜考察了二十八年，不是人能做到的，是天。后面尧死了，舜避居多年，人们朝见，诉讼，讴歌，都不去找尧的儿子而去找舜，所以说是天啊。不管将天下给了贤人，还是儿子，都是天的事情。后面的「唐、虞禅，夏后、殷、周继，其义一也。」都是一回事。</p>
</blockquote>
<p>虽说「天视自我民视，天听自我民听」，可只要稍有常识的人就能看出这是在扯出车轱辘话来嗯说，实在是丢人。 「只有人民才能做出选择」可是既没有制度化的保障也没有法治精神，不是一样成了橡皮图章吗？由此，张千帆顺水推舟地论证了宪政的必要性：人民主权，法的精神：人权，制约，法治。令人感慨。</p>
<p>写得时候查资料，果然又查到孟子不支持燕国禅让的黑历史，在这篇文章《先秦儒家禅让思想的演变-孔锐之》（这个名字听起来像极了人民日报风格的笔名）中，还亲切地将孟子称为是”<strong>儒家的保守派</strong>“，真是令人忍俊不禁。不过，这篇文章后面对燕国禅让的评论引起了我的注意：</p>
<blockquote>
<p>　燕国的这次动乱充分说明，在战国时代，通过禅让实现政权交接是不可取的，这样做非但不能实现尊贤、举贤的目的，反而会为权臣篡国铺垫道路。这次禅让实践的失败，给人以很多启示，即便是原先积极主张推行唐虞之道的学者，现在恐怕也不得不重新考虑其现实可行性问题，继而会转向温和的或保守的主张。在这之后，很难再有人大力鼓吹“不禅而能化民者，自生民未之有也”的观点了。</p>
</blockquote>
<p>就事论事，这种论点在文章当中并不算突兀，但如果真的是这样分析问题，还是趁早上观察者网开个号得了。如果用同样的角度生活在十八世纪中叶，大概也会得出：</p>
<blockquote>
<p>在当前这个时代，资产阶级革命以建立共和国的方式是不可取的……法国近五十年的动荡，从波旁王室退位后，雅各宾派杀得人头滚滚；拿破仑从科西嘉到圣赫勒拿岛，流血漂橹；法兰西如同钟摆在两端摇晃，民主共和只会是一团泡影——瞧，现在工人又起来要胡闹了。</p>
</blockquote>
<p>长达近百年的动荡的确让人失去信心，可是，历史终于证明了共和国的荣耀：从大革命到第三共和国，波旁与拿破仑终于退出了巴黎，法国再也不需要皇帝或者第一执政官了；从第三共和国到五月风暴，法国与他的人民在一点一点缓慢却坚定地向前行进。我们常常面对物理学或数学的大厦已经建成，而望洋兴叹，感慨生不逢时。如此看来，无须妄自菲薄或垂头丧气，正得其时。</p>
<p>……扯得太远了，回来回来。</p>
<p>总之，从对儒家的看法来解读王，那么就不难得出上述的结论。如此，他的形象大概也慢慢清晰起来：一个以传统中式家长为底色，有着出色的视野，理解能力，和dedication（热忱？）的干练人物。上一句话的最后中心语没有使用”官僚“或者”知识分子“，是因为这样的标签都与他本身相去甚远。或许用他自己的”政治家“来定义都更合适不少。（虽然他此刻并没有任何官职<em>无来源</em>，但毫不突兀）。他有知识分子的一面，对于艺术的解读能力十分敏锐，还有相当灵的生命意识——对此他有几片非常精彩的散文片段。在规划学院与日常生活中，也能看出几分官僚色彩（不是贬义）——我头一回见到有人说去”个体户“那里吃饭。然而令人某种程度上十分诧异的是，他的家庭生活似乎并不很丰富，直到成文时的96年，没有生育，对他的影响从文中也是一片空白。入阁后虽有再婚，那就不知道了。</p>
<p>王对自己的评价十分精确：</p>
<blockquote>
<p>生活在这个世界上的人：有的是弱者；有的是强者；有的要别人来设定目标，有的给别人设定目标；有的需要感情来支持生活，有的需要意志来支持生活。我大概在每一对概念中都会选择做后一种人。</p>
</blockquote>
<p>我完全同意他三个后者的说法，事实上，看他的照片某种程度上算是一种享受：消瘦的身形，锐利的眼神，完全就是一个现实中的”孙悟空“式的形象，喜欢极了。看他书中提到的加强党的权威，以人为本等思想从入阁后各个时代的指导方针中流露，似乎并没有丝毫动摇。不由得想起潜伏中的那一句台词：</p>
<blockquote>
<p>随风摇摆是一种优秀的能力，但是从一而终也是一种可贵的品质。后者或许会招致毁灭，却也可能诞生奇迹。</p>
</blockquote>
<p>我衷心祝愿他能够同样在这一对概念中迎来后者。</p>
<p>书中还有一处颇为有趣：</p>
<p>他提到某院长从日本访问回来，已经信息到了“信息高速公路”：到那时，人们将在家办公，在家学习，在家购物和看病：药品和X光机都能送上门来。</p>
<p>他说，这将改变人们的生活方式，改变社会的管理方式和组织方式：如果大部分办公楼，报纸，大学和商场都不需要了，那么就业结构会怎么样呢？意识形态怎么处理呢？政府又应当如何管理呢？这些都应该有一个设想。</p>
<p>看到前面的时候，我还不禁感慨，真牛啊，那会不光都能为”写了稿子电脑中病毒而哀嚎“（前文），甚至都能意识到互联网对社会的巨大改变——看到最后一句却不禁哑然失笑。</p>
<p>想起来十九世纪的人们为都住在大楼里的生活绘制美好蓝图：由于上下楼不便，每栋楼里都会有超市，健身房和咖啡屋，大楼之间还可能建起桥梁……可是电梯的发明却让这一切”合理的“设想都破灭了。是啊，就像犀牛看到的世界是以自己的独角为底色一样，王的眼睛里永远是有政府的：他的存在先于一切又贯彻一切。人们不用外出，甚至不用工作：经济平衡的主要矛盾早已由生产的不足成为了需求的不足，马克思能想到劳动都要被消灭了？（对于大多人）如此，就不禁令人感慨。他自己也说：</p>
<blockquote>
<p>生活在今天的人，常常说前人无能，把中国搞成这样，我们希望以后的中国人不会说今天的中国人无能，把中国搞成这样。这是我最大的愿望。</p>
</blockquote>
<p>中国的知识分子，三四千年来在儒家的框框里咚咚打转，能想到的也无非是圣君贤相，勤政爱民；东学西渐，梵学给文化艺术哲学都注入了崭新的活力，却唯独与几政治哲学无动于衷。晚清的知识分子出去绕了一圈，才把大腿都拍麻了：天下为公三代之治怎么在蛮夷那边实现了。所以，自以为是一切之先决者，最容易是一切的漏洞。也正是同样的道理：最谦虚，最自满。</p>
<p>想想前一阵子看禅宗公案，有一个有文化的和尚写了这么一句，相当漂亮：</p>
<blockquote>
<p>万古长空，一朝风月。</p>
</blockquote>
<p>是啊，一朝风月，万古长空。</p>
<p>2022&#x2F;8&#x2F;8</p>
<p>PS：写完之后意识到的自己几乎没有了解的课题：</p>
<ul>
<li>辛亥以后的儒家社会地位沉浮</li>
<li>法国历史</li>
<li>古代中国的外来思想文化影响，尤其是对政治方面</li>
<li>经济学完全是0</li>
</ul>
]]></content>
      <categories>
        <category>Metaphysics</category>
      </categories>
      <tags>
        <tag>Reading</tag>
        <tag>Politics</tag>
      </tags>
  </entry>
  <entry>
    <title>Follow Ace Taffy Meow！</title>
    <url>/2022/05/23/testpic/</url>
    <content><![CDATA[<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Follow Ace Taffy thanks Meow!&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/05/23/testpic/image-20220524021029166.png" alt="image-20220524021029166"></p>
]]></content>
      <categories>
        <category>Meow</category>
      </categories>
      <tags>
        <tag>Ace</tag>
        <tag>Taffy</tag>
      </tags>
  </entry>
  <entry>
    <title>杂谈-别人的书</title>
    <url>/2022/09/17/%E6%9D%82%E8%B0%88-%E5%88%AB%E4%BA%BA%E7%9A%84%E4%B9%A6/</url>
    <content><![CDATA[<p>昨天（其实是今天……）说要和某人互换藏书 浏览其自己的书架才发现好多好多别人的书。细看之下想到好多，不过更多还是没读过的XD。下面一本一本说吧。</p>
<h1 id="Ursula-lee"><a href="#Ursula-lee" class="headerlink" title="Ursula lee"></a>Ursula lee</h1><p>《少有人走的路》 来自 2015年。刚刚翻开扉页，写着，「爱不是感觉 爱是实际行动 是真正的付出」很巧的是 这也就是他现在留给我的几乎全部印象。 记得前几天还是最近一次还和别人提过这本书 印象之中是「有点水平和启发的鸡汤」。事实上这也应该算我对「爱」的启蒙之作？有了这个，才有了后面的「爱不是交易或因信称义」，后面黑夜中偶遇墙上的《哥林多前书》一般的「龙场悟道」……有趣的是，这本书我并没有看完。书的第一章并不是在讲什么爱的艺术，而是我没有任何印象的自律章节。</p>
<p>作为回赠 我送给了他新的《旅行的艺术》，因为听说他很爱旅行，这位作者的下一本书是我在九年后才读到的《拥抱似水年华》可能是没有读过普鲁斯特 反而觉得像是那种诙谐的小报作家了。不过在上一本书的扉页上 我还是很认真地写下了</p>
<blockquote>
<p>天之苍苍 其正色邪？其远而无所至极邪？</p>
</blockquote>
<p>坏了 写到这才发现这个话题不兴说……好多烂事要牵扯上来……先睡觉得了</p>
]]></content>
      <categories>
        <category>Metaphysics</category>
      </categories>
  </entry>
  <entry>
    <title>火柴</title>
    <url>/2022/05/28/%E7%81%AB%E6%9F%B4/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>我在<sup id="fnref:6"><a href="#fn:6" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="火柴梗快要烧完的时候，会因为火焰过分接近而忍不住松手">6</span></a></sup>水中桥下<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="「尾生與女子期於梁下，女子不來，水至不去，抱樑柱而死。」《莊子·盜跖》
">1</span></a></sup> 饮酒<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label=" 「夢飲酒者，旦而哭泣；夢哭泣者，旦而田獵。方其夢也，不知其夢也。」《莊子·齊物論》
">2</span></a></sup>&#x2F;<del>忘相泉涸<sup id="fnref:4"><a href="#fn:4" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="「泉涸，魚相與處於陸，相呴以溼，相濡以沫，不如相忘於江湖。」《莊子·大宗師》">4</span></a></sup>前日的红烛<sup id="fnref:5"><a href="#fn:5" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="「蠟炬成灰淚始幹」 李商隐
">5</span></a></sup>泪眼<sup id="fnref:3"><a href="#fn:3" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="「文侯與虞人期獵。是日，飲酒樂，天雨。文侯將出，左右曰：「今日飲酒樂，天又雨，公將焉之？」文侯曰：「吾與虞人期獵，雖樂，豈可不一會期哉！」乃往，身自罷之。」《戰國策·魏策》
">3</span></a></sup></del>红烛淡忘镜中的泪眼</p>
<p>自己写完还解诗的人肯定是天下第一无聊 但这只不过是记录灵感媾和时的一些脉络和取舍。</p>
<span id="more"></span>


<hr>
<p>2022&#x2F;5&#x2F;30日凌晨睡不着床上对后半句做了修正</p>
<p>第二天起来看前天写得也太迫真了 简直就是笨蛋版本李商隐</p>
<p>当时是怎么想的呢？「忘相」是什么表达？生怕别人看不出来是你直接从庄子里偷来的？「泉涸」同理，太白太直了 反而失去了解读空间。「前日」也不明所以的。都要删掉。「红烛泪眼」是核心意象要留下来。</p>
<p>想要留下来的是什么呢？遗忘肯定是要有的，这是我给出的解答。互相也是要有的，起源就是由防风火柴想到的尾生抱柱抛出的一问。「红烛泪眼」没法共轭，还是拆开好了。</p>
<p>互相的话 就用镜子好了。破妄，吊诡，空间的拓展却又重复，自反中带有异质性。很好。</p>
<p>「红烛在镜中总是泪眼」？当时还开心的把手机翻出来赶紧记下，记完想想又觉得不好 啰啰嗦嗦的。</p>
<p>「红烛向镜中抛去泪眼」？我很喜欢这个动作带有的力量感，和伴随而来的主体性。用什么迎接你？以眼泪，以沉默。这是有力量的沉默。但是汉语还是半通不通的 忘记也没了。不好。</p>
<p>「忘记」这个字其实很好，自反又偏义，但是口语中用得太多了 读来感觉不到妙处 不好。</p>
<p>我想，烛火燃烧时上腾的青烟，蒸腾的雾气凝结成雨，落下化为沙尘，恰好就有一种复调式的演出效果。水汽也好，腾烟也好，怎么放在这里处理”忘“这个要素呢？想到了溶解，融化，但都用不好。这里卡了很久没想出来。</p>
<p>灵光一闪，就用「淡忘」。「淡」字自己就好像是拿来给水墨化开的，要是到token级别就是又有水又有火的自反，不管是前句的湖中还是镜子都能超距作用。「淡忘」本来不是什么僻词，但放在这里就妙得没话说。</p>
<p>「红烛淡忘镜中的泪眼」，真好。</p>
<p>「我」「饮酒」，「烛」「忘眼」。好像比兴一样的氛围，又构成了复调的演奏。「水」与「镜」，「泪」与「眼」，几乎每一个元素都能够进行笛卡尔式呼应。比兴之中，阅读顺序的先后带来的时序性还为文本增添了并列以外的递进因素，自问自答。很好。我很喜欢。</p>
<p>如果说昨天是向义山一样堆叠典故，这次就是处理意象了，也是很好玩呐。</p>
<p>下面又试着加点东西。一方面是平衡语感。这两句佶屈聱牙，像极了祭祀用的七言律诗，但是要是能像冯君一样，神神叨叨念完“长剑归来乎，”，令人不容小觑，立马接上一句“食无鱼”产生节目效果。那就可以说是非常成功了。</p>
<p>此外，也和我想要追求的吊诡氛围有点差异。水中饮酒还有镜子，好像是月光下的水晶湖一样，太明亮通透了些&#x2F;</p>
<p>加什么呢？四个字的好。「烟波浩渺」？我想到洞庭湖，蹭蹭湘君的隐喻正好在调上。「烟涛微茫」？直接偷过来好像也不坏。像是舞台布景的话，很明白的小舞台放在巨大的烟幕里，也是那个意思。</p>
<p>不过怎么放怎么感觉不妙，况且下来也不知怎么接手。先这么放着好了。<div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">「尾生與女子期於梁下，女子不來，水至不去，抱樑柱而死。」《莊子·盜跖》<a href="#fnref:1" rev="footnote">↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">「夢飲酒者，旦而哭泣；夢哭泣者，旦而田獵。方其夢也，不知其夢也。」《莊子·齊物論》<a href="#fnref:2" rev="footnote">↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">「文侯與虞人期獵。是日，飲酒樂，天雨。文侯將出，左右曰：「今日飲酒樂，天又雨，公將焉之？」文侯曰：「吾與虞人期獵，雖樂，豈可不一會期哉！」乃往，身自罷之。」《戰國策·魏策》<a href="#fnref:3" rev="footnote">↩</a></span></li><li id="fn:4"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">4.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">「泉涸，魚相與處於陸，相呴以溼，相濡以沫，不如相忘於江湖。」《莊子·大宗師》<a href="#fnref:4" rev="footnote">↩</a></span></li><li id="fn:5"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">5.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">「蠟炬成灰淚始幹」 李商隐<a href="#fnref:5" rev="footnote">↩</a></span></li><li id="fn:6"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">6.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">火柴梗快要烧完的时候，会因为火焰过分接近而忍不住松手<a href="#fnref:6" rev="footnote">↩</a></span></li></ol></div></div></p>
]]></content>
      <categories>
        <category>Metaphysics</category>
      </categories>
      <tags>
        <tag>Poem</tag>
        <tag>Self</tag>
      </tags>
  </entry>
  <entry>
    <title>论文笔记 AST-Trans ICSE`22</title>
    <url>/2022/07/03/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-AST-Trans-ICSE-22/</url>
    <content><![CDATA[<p>这是《AST-Trans: Code Summarization with Efficient Tree-Structured Attention》的读书笔记</p>
<span id="more"></span>

<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>代码总结是干什么<br>最先进的方法用了E-D架构<br>源代码的特征之一是AST，常用于编码信息<br>AST太长<br>现在的方法忽视了大小限制 直接就塞进去序列化的AST<br>我们认为其问题在于难以提取信息，计算成本大</p>
<p>为了更好地编码AST 提出了AST-Trans，利用了两种节点关系——上下和左右<br>使用了树状注意力动态分配权重给相关的节点<br>还还提出了一种<strong>支持</strong>高效为树状注意力并行运算的实现</p>
<p>在两个相关数据集上，超过了SOTA</p>
<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p> 程序总结分为多个模块 本作聚焦于子例程或方法<br> 短描述可以让开发者快速理解 但是 好的总结需要大量劳动——错配 丢失 过时——自动方法可以避免<br> 传统方法：</p>
<ul>
<li>手工规则来——命名太烂就不行了</li>
<li>信息检索技术：没见过就不行了</li>
</ul>
<p>最近：</p>
<p>开源代码库让数据驱动的神经网络引起了更多注意</p>
<p> 代码需要AST-AST序列化-简单介绍几个方法</p>
<hr>
<p>在introduction就介绍了许多现有工作 更具体地说是提到啥就引用</p>
<hr>
<p>问题：L-AST太长了 </p>
<p>仔细解释</p>
<p>介绍我们的工作</p>
<ul>
<li><p>假设：AST中的节点受影响最大的是：</p>
<ul>
<li>祖先-子孙:不同区块的等级关系</li>
<li>兄弟：时序关系</li>
</ul>
<p>—— 画了个图作证 </p>
<p>捕获这两种关系就可以了 不用用全部注意力为所有节点建模</p>
</li>
</ul>
<p>			</p>
<p>​	提出了 这个东西 是一个Transformer的简单变体来 处理树状AST</p>
<pre><code> 用了 这两种关系的矩阵来代表树结构 然后用了这个矩阵来动态排除不同注意力层中的无关节点
</code></pre>
<p>​	绝对位置嵌入也被换成了由两种关系矩阵的相对位置嵌入</p>
<p>还进一步描述了实现与计算分析</p>
<p>贡献：</p>
<ul>
<li>可以用线性复杂度来编码唱AST 和传统二次复杂度的Transfermer不一样</li>
<li>深入分析：复杂度，经验证据等</li>
<li>在两个数据集上显示大幅度SOTA</li>
<li>比较了多种AST的编码方式并讨论</li>
</ul>
<p>2 Background——AST Transformer</p>
<p>3 elaborates 实现细节</p>
<p>4 不同的实现</p>
<p>5 分析复杂度</p>
<p>6 解释实验步骤 分析结果</p>
<p>7 TtV</p>
<p>8 RW</p>
<p>9 Conclusion</p>
<h1 id="BACKGROUND"><a href="#BACKGROUND" class="headerlink" title="BACKGROUND"></a>BACKGROUND</h1><h2 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h2><p>最开始被提出来做机器翻译 使用了多头栈状encoder-decocer层</p>
<p>介绍了一下普通Transformer的层，可以并行</p>
<h2 id="AST"><a href="#AST" class="headerlink" title="AST"></a>AST</h2><p>真的就是简单介绍 几乎没有引用</p>
<h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><p>先概述一下流程 然后分步骤介绍</p>
<h2 id="序列化"><a href="#序列化" class="headerlink" title="序列化"></a>序列化</h2><p>简单介绍了三种序列化的方法，最后说用了改进的先序遍历序列法达成了SOTA 反而结构化的效果不好 路径分解效果更烂</p>
<ol>
<li>POT 先序遍历序列化：简单的按照先序遍历把节点列出来 坏处在于有损 因为无法靠此重建</li>
<li>SBT 结构序列化 ：</li>
<li>PD 路径分解 ：随机展示出一条某节点到某节点的路径 因为会有超多条 所以需要随机采样</li>
</ol>
<p>前两个都能直接放 第三个不好搞在这个任务</p>
<hr>
<p>问题在于他实际使用的 改进的POT也没介绍啊</p>
<h2 id="关系矩阵"><a href="#关系矩阵" class="headerlink" title="关系矩阵"></a>关系矩阵</h2><p>引入了祖先矩阵和兄弟矩阵两个东西来对节点的关系建模</p>
<p>如果是该关系 则值为有向距离，反之为正无穷</p>
<p>Aij&#x3D;-Aji</p>
<p>还介绍了一个预定义阈值P 有向距离大于P则变成正无穷 没搞懂在干什么：</p>
<p>十代祖先就不算了 设置了一定的视野范围</p>
<p>说是要用两个矩阵来动态改善树状的注意力分配</p>
<h2 id="树状注意力"><a href="#树状注意力" class="headerlink" title="树状注意力"></a>树状注意力</h2><p>介绍了自注意力 相对位置嵌入 	注意力解耦 这三个看起来都是别人的东西 基本跟他没什么关系 有点像Background</p>
<p>然后介绍了树状关系注意力，</p>
<p>用了前面的关系矩阵来代替了相对位置嵌入的一个距离 再经过一些处理 在对新关系建模的同时还能达到更低的复杂度效果。</p>
<h1 id="高效实现"><a href="#高效实现" class="headerlink" title="高效实现"></a>高效实现</h1><p>传统的Transformer 计算复杂度会随着序列长度二次增长 但改良的ASTTrans 只需要对部分节点对计算：某些R距离大于零的 然后逐个分析实验方法：掩码 循环跳过 稀疏向量什么的</p>
<hr>
<p>可以对数据集来个量化的分析</p>
<h1 id="复杂度分析"><a href="#复杂度分析" class="headerlink" title="复杂度分析"></a>复杂度分析</h1><h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><h2 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h2><p>数据集介绍，处理策略</p>
<p>预处理</p>
<p>超参数</p>
<p>评测方式</p>
<h2 id="介绍比较的基线"><a href="#介绍比较的基线" class="headerlink" title="介绍比较的基线"></a>介绍比较的基线</h2><p>根据输入的不同分类介绍</p>
<p>每个模型逐个介绍</p>
<p>输入代码：</p>
<p>输入AST树：有树专用的encoder 用Tree-LSTM或者GNN——想知道是怎么输入的 或者说……encoder是怎么接受这种东西的</p>
<p>输入PD AST：作者甚至还改进了他的模型作为基线：把模型中的LSTM换成Transformer</p>
<p>输入SBT AST </p>
<p>输入POT AST:设计了对照组是接收同样输入的Transformer</p>
<h2 id="主要结果"><a href="#主要结果" class="headerlink" title="主要结果"></a>主要结果</h2><p>也就是分析</p>
<p>比较：</p>
<ul>
<li><p>代码-树状AST-序列化AST</p>
<p>发现树状AST效果最好 结果树状的效果最好 作者的分析是这个信息最完整最多 所以效果最好</p>
<p>还分析了在不同数据集上的影响 觉得是长度的原因</p>
</li>
<li><p>三种序列化AST的比较</p>
<p>SBT在JAVA效果最好 POT在python上效果最好 </p>
<p>SBT 信息多 POT最短</p>
<p>PD效果最烂</p>
</li>
<li><p>关系矩阵的影响</p>
<p>加上之后改善了所有模型的性能</p>
</li>
<li><p>AST-Trans vs GNN</p>
<p>和其他里面最强的那个比一比</p>
<p>分析了为啥会更好一点</p>
</li>
</ul>
<h2 id="消融实验"><a href="#消融实验" class="headerlink" title="消融实验"></a>消融实验</h2><p>消融对象：四种</p>
<ul>
<li><p>仅使用一种对象矩阵：也会造成提升 但是不如都来</p>
</li>
<li><p>注意力头的数量 8个头几个看祖孙 几个看兄弟：更接近一种超参数实验</p>
</li>
<li><p>观察多远的范围来确定祖先-兄弟关系？：</p>
<p>视野越大效果越好，就算一点也能改善很多，具有边际递减</p>
</li>
<li><p>网络层数：越深越好</p>
</li>
</ul>
<h2 id="视觉分析和定量分析"><a href="#视觉分析和定量分析" class="headerlink" title="视觉分析和定量分析"></a>视觉分析和定量分析</h2><h1 id="有效性威胁"><a href="#有效性威胁" class="headerlink" title="有效性威胁"></a>有效性威胁</h1><ul>
<li>选取的公开数据集可能代表性不足</li>
<li>作为基线的其他架构选择和超参数选择可能不是最优</li>
<li>自动评估和手动评估的代表性可能不强</li>
</ul>
<hr>
<p>全都是实验设置 做实验的问题 没提方法本身的问题</p>
<h1 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h1><h2 id="代码总结"><a href="#代码总结" class="headerlink" title="代码总结"></a>代码总结</h2><p>大多数人都把他当seq2seq任务，用Transformer</p>
<p>和传统翻译的唯一区别就是他的输入是无歧义的代码，有语法规则，普遍的方法都是把他当普通文本序列或者结构化序列</p>
<p>介绍了一下处理的方法，以罗列为主</p>
<p>简单比较了一下自己的方法</p>
<h2 id="基于树的神经网络"><a href="#基于树的神经网络" class="headerlink" title="基于树的神经网络"></a>基于树的神经网络</h2><p>现有的树状神经网络可以按照输入分为两种</p>
<p>解析法和采样法</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>我们<strong>通过</strong>高效编码AST<strong>实现</strong>了代码总结：</p>
<p>介绍一下方法，以及带来的好处</p>
<p>只需要关注有关系的节点</p>
<p>可以不被过长的AST困惑</p>
<p>降低复杂度</p>
<p>画画饼：让他可以处理长代码，甚至一个文件</p>
<p>做了实验，比较</p>
<p>我们相信这样的基础理念可以用在别的地方</p>
<p>计划更多特征加入，如API序列或节点类型，来改善注意力机制</p>
]]></content>
      <categories>
        <category>Substance</category>
      </categories>
      <tags>
        <tag>Note</tag>
        <tag>ICSE 2022</tag>
        <tag>Code generation</tag>
      </tags>
  </entry>
  <entry>
    <title>论文笔记 Automated Generation of Constraints From Use Case Specifications to Support System Testing</title>
    <url>/2022/06/20/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Automated-Generation-of-Constraints-from-Use-Case-Specifications-to-Support-System-Testing/</url>
    <content><![CDATA[<p>后记：</p>
<p>这篇文章真没搞懂 看的时候觉得实在有点车轱辘话嗯说 前面说UMTG只要RUCM和OCL 后面说OCLgen的输入还需要类图……</p>
<p>前面说verbnet里也不全是同义词 后面又说都是同义词</p>
<p>前面说这个可以转换所有语言 后面就是简单推测了一下用较少规则可以转换好多 而现在还只实现了较少的较少……</p>
<p>当然 也可能是我水平还不够看得颠三倒四 总之 就只是把他非常「作为手段地」分析了一通</p>
<span id="more"></span>

<ul>
<li><p>这篇文章要解决什么问题？</p>
<ul>
<li>需要从自然语言的需求规范中自动生成可执行测试用例，使用UMTG工具</li>
<li>UMTG需要的东西：RUCM的自然语言规范和OCL写的约束</li>
<li>OCLgen就要生成OCL约束——主要是前后置条件</li>
</ul>
</li>
<li><p>OCLgen的输入输出是什么？</p>
<ul>
<li>输入：UMTG一致(?)——RUCM的NL和UML类图的系统领域模型</li>
<li>输出：每一个用例步骤对应的OCL约束</li>
</ul>
</li>
</ul>
<p><img src="/2022/06/20/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Automated-Generation-of-Constraints-from-Use-Case-Specifications-to-Support-System-Testing/image-20220628194141482.png" alt="image-20220628194141482"></p>
<img src="/2022/06/20/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Automated-Generation-of-Constraints-from-Use-Case-Specifications-to-Support-System-Testing/image-20220628194155803.png" alt="image-20220628194155803" style="zoom: 33%;">

<ul>
<li><p>它厉害在哪里？</p>
<ul>
<li>别人的方法 要使用CNL（受控的自然语言）来写软件要求，即，采用有限的动词</li>
<li>他的方法——需要UMTG格式的语言——但不限制动词，还需要类图</li>
<li>在测试的工业案例中，可以达到75%的正确</li>
<li>使用了语义标注和同义词合并的方法，使得只需要很少的规则就可以实现大范围的转换</li>
</ul>
</li>
<li><p>他的方法是什么？</p>
<ul>
<li>先用SRL（语义角色标注）标记句子中词汇的角色</li>
<li>识别到之后 根据一定的规则再去识别类或自然语言中之中别的属性和操作符</li>
<li>还有通用的——元动词转换规则——因为目的是生成测试用例</li>
<li>然后（如果能生成多个）就打分评估最好的输出</li>
<li>打分方法：完整性和正确性——用例中角色在OCL中出现的比例，变量名和用例名一致(<strong>这里完全没搞懂</strong>)</li>
</ul>
</li>
<li><p>他的评估方法是什么？</p>
<ul>
<li>比较了生成的和手动编写的 比较了正确率</li>
<li>对于需求 说了87个测试中生成除了多少</li>
<li></li>
</ul>
</li>
<li><p>他的不足在哪？</p>
<ul>
<li>聚焦于前后置条件 主要测试场景就是这个工业案例 所以对输入有预设，对输出有范式</li>
<li>吹得很猛 不需要限制 可以处理所有 其实目前只实现了7类规则，可以处理408个动词</li>
<li>对语言的规格还是有需求 需要输入UML类图</li>
<li>实验很弱</li>
</ul>
</li>
<li><p>有什么启发？</p>
<ul>
<li>比较的时候 应用场景 生成的灵活性都会比他好不少</li>
<li>对于方案设计启发不大</li>
</ul>
</li>
</ul>
<p>目的：</p>
<p>从自然语言的需求规范中自动生成系统测试用例</p>
<p>——》</p>
<p>为UMTG生成其需要的正则标准，提出了OCLGEN</p>
<p>使用语义分析技术来识别用力规范的前后置条件</p>
<p>可以75%正确生成前后置条件</p>
<p>系统测试很重要 其测试用例要展示功能和安全需求，</p>
<p>软件需求用NL写，然后由工程师手动转换，昂贵且易错</p>
<p>现有的自动化解决方案依靠限制过的，简单的自然语言解决</p>
<p>别人的方法（生成测试用例的）：</p>
<p>基于特殊关键词侦测，如 if then&#x3D;&gt;抽象，高层次 给测试人员</p>
<p>用受控的自然语言(CNL)写软件规格,再基于规则转换为正则标准&#x3D;&gt;可用语言非常有限</p>
<p>不用CNL 但需要其他的建模工作——UMTG就是这样的</p>
<p>OCLgen——捕获语句中的后置条件或前置条件——采用了文本转换规则，依赖自动语义分析技术，无需受控语言</p>
<p>SRL（语义角色标注）实现词汇的标注，例如，收到动作最直接影响的成员就应该出现在后置条件中</p>
<p>同义词识别，判断不同的词汇能否用相同的规则处理</p>
<p>在测试的工业案例中，75%精度，25%由于精度不足</p>
<p>UMTG: RUCM(用于写用例的一种语言格式 基本流替代流什么的)+OCL——测试用例</p>
<p>语义标注：搞清楚短语的角色对前后置条件的生成是必要的</p>
<p>别人搞的自然语言-用例生成不少用了语法识别 有一定用但搞不清短语作用</p>
<p>SRL有许多种 但用了CNP是因为他是唯一一个还在积极开发的 也有接口</p>
<p>同义检测：</p>
<p>VerbNet不仅包括同义词类，还包括模式，如主语+不及物或主系表结构 </p>
<p>使用了和PropBANK(CNP使用的）不同的模型，也会有不同的标签，但存在映射关系</p>
<p>同一类中的词共用一种模式，帮助定义可重用的转换规则，但不是同一类中的都是同义词（？）用来识别同义词最先进的方法是WordNet</p>
<h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><p>UMTG需要人写ocl捕获这两种信息：</p>
<ul>
<li>行动对于系统状态的影响——后置条件</li>
<li>用例的前置条件</li>
</ul>
<p>OCLgen就要自动化这一步骤</p>
<p>需要的输入与UMTG一致：</p>
<ul>
<li>RUCM写的用例规范</li>
<li>以UML类图形式的 系统的领域模型</li>
</ul>
<p>可以输出每一个用例步骤对应的OCL约束</p>
<p>OCLgen的方法：</p>
<ul>
<li>通过SRL<ul>
<li>挑选要出现在约束中的元素，</li>
<li>决定要使用的比较运算符</li>
<li>额外的操作符，如否定</li>
</ul>
</li>
<li>针对每一个动词 使用不同转换规则转换——为了可行，使用VerbNet合并词类，需要的规则更少</li>
</ul>
<p>转换步骤：</p>
<p>标记——选择规则——转换候选——挑选最高分</p>
<p><img src="/2022/06/20/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Automated-Generation-of-Constraints-from-Use-Case-Specifications-to-Support-System-Testing/image-20220625173337030.png" alt="image-20220625173337030"></p>
<p>可能有多个候选因为选择了多种规则</p>
<p>最高分赋予使用了用例步中最多可用信息的?</p>
<h1 id="OCL约束的格式"><a href="#OCL约束的格式" class="headerlink" title="OCL约束的格式"></a>OCL约束的格式</h1><p>一般就是比较笨蛋的 前置条件与条件步通常就是安全检查确保环境恰当，较容易捕获赋值，相等和不等关系</p>
<p><img src="/2022/06/20/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Automated-Generation-of-Constraints-from-Use-Case-Specifications-to-Support-System-Testing/image-20220625181049650.png" alt="image-20220625181049650"></p>
<p>是这样一套简化的EBNF 其中不少东西都是来自领域模型的，如类名和属性**</p>
<h1 id="转换规则"><a href="#转换规则" class="headerlink" title="转换规则"></a>转换规则</h1><p>所有转换规则共享相同的规则 每个规则和一组动词关联，如果一个动词出现，就执行一个步骤</p>
<p>在第X部分会讨论规则对英语（动词）的覆盖性，在这一部分主要讨论动词 be set enable的规则</p>
<p>SRL会识别出 左手边变量 left-hand side variable，操作符，选择元素和右手边变量right-hand side terms.</p>
<p>A1一般就是lhs varible</p>
<p>有两种转换规则：</p>
<ul>
<li><p>specific verb transformation rule: 对每个动词定制的转换规则</p>
</li>
<li><p>META verb transformation rule: 对任何动词使用的转换规则，</p>
<p>这种规则基于这样一种常见的现象 ，就是语句的LHSvarible是一个名称与其动词匹配或相同的属性(后面还会介绍)</p>
</li>
</ul>
<h1 id="VI-识别要用到OCL左侧的变量"><a href="#VI-识别要用到OCL左侧的变量" class="headerlink" title="VI 识别要用到OCL左侧的变量"></a>VI 识别要用到OCL左侧的变量</h1><p>真没搞懂在干嘛</p>
<h1 id="VII-识别右侧的变量"><a href="#VII-识别右侧的变量" class="headerlink" title="VII 识别右侧的变量"></a>VII 识别右侧的变量</h1><p>根据左侧变量的类型 支持角色来在输入的自然语言和模型中寻找类似的或可匹配到的</p>
<h1 id="VIII-识别操作符"><a href="#VIII-识别操作符" class="headerlink" title="VIII 识别操作符"></a>VIII 识别操作符</h1><p>用了别人的方法[35] 普遍都是类似于 be这样的动词</p>
<p>对于 除了……都……这样的范式 发明了一套方法 也是和语义角色标记有关系的</p>
<h1 id="IX-打分"><a href="#IX-打分" class="headerlink" title="IX 打分"></a>IX 打分</h1><p>从完整性和正确性两个维度</p>
<p>完整性：自然语言中所有概念被说明的程度有关。——用例中角色在OCL中出现的百分比</p>
<p>正确性：OCL中的变量名和用例中的名字一致</p>
<p><img src="/2022/06/20/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Automated-Generation-of-Constraints-from-Use-Case-Specifications-to-Support-System-Testing/image-20220625212958549.png" alt="image-20220625212958549"></p>
<h1 id="X-完整性和普遍性"><a href="#X-完整性和普遍性" class="headerlink" title="X 完整性和普遍性"></a>X 完整性和普遍性</h1><p>为了让更多的动词可以用：</p>
<ul>
<li>使用Verbnet 让同一规则可以用于更多词（同一类的词）</li>
<li>排除了形容人的感受&#x2F;行为&#x2F;动物行为的词</li>
</ul>
<p>经过分析，33个转换规则就可以给87类词转换 目前实现了7类规则 包括元转换规则。可以处理408个动词</p>
<h1 id="XI-经验评估"><a href="#XI-经验评估" class="headerlink" title="XI 经验评估"></a>XI 经验评估</h1><p>RQ1: 生成的OCL约束对吗？</p>
<p>RQ2: 对于用例规范 oclgen的自动生成效果如何？</p>
<p>RQ3: 限制生成效率的要素是什么？</p>
<h2 id="RQ1-生成的OCL约束对吗？"><a href="#RQ1-生成的OCL约束对吗？" class="headerlink" title="RQ1: 生成的OCL约束对吗？"></a>RQ1: 生成的OCL约束对吗？</h2><p>比较了生成的和手动写的</p>
<p>可能会： </p>
<ul>
<li>生成对的</li>
<li>生成错的</li>
<li>没有结果</li>
</ul>
<p>总正确率：66&#x2F;69&#x2F;87</p>
<h2 id="RQ2-对于用例规范-oclgen的自动生成效果如何？"><a href="#RQ2-对于用例规范-oclgen的自动生成效果如何？" class="headerlink" title="RQ2: 对于用例规范 oclgen的自动生成效果如何？"></a>RQ2: 对于用例规范 oclgen的自动生成效果如何？</h2><p>66&#x2F;87</p>
<h2 id="RQ3-限制生成效率的要素是什么？"><a href="#RQ3-限制生成效率的要素是什么？" class="headerlink" title="RQ3: 限制生成效率的要素是什么？"></a>RQ3: 限制生成效率的要素是什么？</h2><p>手动检查没有生成的句子</p>
<ul>
<li>信息不足</li>
<li>在用例规范和领域模型中表述不一样（is valid ——翻译不成——&lt;&gt;Error)</li>
</ul>
<h1 id="Threats-to-Validity"><a href="#Threats-to-Validity" class="headerlink" title="Threats to Validity"></a>Threats to Validity</h1><p>普遍性：只试了这个工业案例 BodySensetM</p>
<h1 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h1><p>自动生成可执行测试案例需要需求规格用CNL（只有有限的动词）写</p>
<p>OCLgen不需要受限的语言，而是RUCM，他引入了一些关键字，但并不限定使用的动名词</p>
<p>NL2OCL </p>
<p>处理UML类图和NL需求来得出类不变性和前后置条件，也用语义分析 来确定角色，靠侦测特定的关键字来确定被动语态或操作符</p>
<p>缺点：</p>
<p>   没有简化众多的动词怎么办——OCLgen的meta verb rule</p>
<p>已经没法拿来比较了 </p>
<p>NL2OCL更能抽取包括简单比较操作符的类不变量，而非生成前后置条件——这个对于测试用例生成更有用</p>
<p>提点问题：</p>
<ul>
<li><p>如何评估生成的OCL的质量？</p>
</li>
<li><p>在回答RQ时，使用了——产生了多少个OCL约束中多少个是正确的——没有给出“正确”的定义</p>
</li>
<li><p>此外 还评估了一下87个需求多少个能生成出来</p>
</li>
<li><p>在进行选择时，使用了打分机制，</p>
</li>
<li></li>
<li><p>输入输出是什么？</p>
</li>
</ul>
<p>输入包括两部分：</p>
<ul>
<li>RUCM格式的自然语言撰写的用例步骤</li>
</ul>
<img src="/2022/06/20/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Automated-Generation-of-Constraints-from-Use-Case-Specifications-to-Support-System-Testing/image-20220626184416896.png" alt="image-20220626184416896">

<ul>
<li>类图</li>
</ul>
<img src="/2022/06/20/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Automated-Generation-of-Constraints-from-Use-Case-Specifications-to-Support-System-Testing/image-20220626184653142.png" alt="image-20220626184653142" style="zoom: 25%;">

<p>输出似乎是有限的一种范式？</p>
<img src="/2022/06/20/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Automated-Generation-of-Constraints-from-Use-Case-Specifications-to-Support-System-Testing/image-20220626184748233.png" alt="image-20220626184748233" style="zoom:25%;">





<ul>
<li><p>创新点在哪？</p>
</li>
<li><p>借鉴 比较在哪</p>
</li>
<li><p>他做了什么事？方法是什么？</p>
</li>
</ul>
]]></content>
      <categories>
        <category>Substance</category>
      </categories>
      <tags>
        <tag>OCL</tag>
        <tag>Code generation</tag>
        <tag>ICST</tag>
      </tags>
  </entry>
  <entry>
    <title>论文笔记-CodeFill-ICSE`22</title>
    <url>/2022/07/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-CodeFill-ICSE-22/</url>
    <content><![CDATA[<p>这是 CodeFill: Multi-token Code Completion by Jointly Learning from Structure and Naming Sequences 这篇文章的读书笔记</p>
<p>不过主要是为了写论文看的XD</p>
<span id="more"></span>



<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>自动代码完成是在给定的上下文中预测开发者下面要输入什么，简单说下意义</p>
<p>讲讲当前的方法：怎么做的 有什么坏处 及时引用：</p>
<p>使用程序语言的特性 比如括号后面就要跟着对应的参数，坏处在于缺乏基于规则的完成</p>
<p>为了应对这些坏处 现在的办法是怎么做的，又引入了什么新的缺陷：</p>
<p>提出了一些基于统计和学习的模型 将其视为自然语言，这种办法丢失了代码结构和语义，不断引入的标识符也带来了巨大的预测空间</p>
<p>在这篇文章中 我们提出了xx 讲讲架构，任务，创新点，实现，应用场景 小吹一下</p>
<p>讲一下实验构造 简要介绍基线，任务和数据集，实验设置</p>
<p>讲讲结果 怎么个SOTA 最后总结贡献</p>
<p>主要贡献：</p>
<ul>
<li>模型：基于结构和名称的模型</li>
<li>代码和数据集及训练过程</li>
<li>广泛的评估</li>
</ul>
<h1 id="背景和相关工作"><a href="#背景和相关工作" class="headerlink" title="背景和相关工作"></a>背景和相关工作</h1><p>写在一起了</p>
<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>写的很水</p>
<ul>
<li><p>语言模型和Transformer</p>
<p>几乎没介绍什么 有点像凑字数</p>
</li>
<li><p>多任务学习 MTL</p>
<p>简单介绍了和自己工作差不多模式的这一小领域</p>
<p>多个任务，联合损失函数 应参数和软参数</p>
</li>
</ul>
<h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><h2 id="基线"><a href="#基线" class="headerlink" title="基线"></a>基线</h2><h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><p>简单介绍概览 可能给个图？</p>
<p><img src="https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9793835/9793541/9794048/9794048-fig-1-source-small.gif" alt="图 1：- CodeFill 工作流程"></p>
<p>包括三个部分  预处理 模型训练和 后处理</p>
<h2 id="预处理"><a href="#预处理" class="headerlink" title="预处理"></a>预处理</h2><p>造出两个不同版本的输入 一个是去掉空行空格注释的原始变量名之类的</p>
<p> 一个是用pythonAST 处理过的token type，记录每个token的类型，值和位置</p>
<p>使用了BPE编码 （这还要单独拿来吹）</p>
<p>为了处理python的缩进 额外引入了类似括号的标记符来标明缩进</p>
<h2 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h2><p>分别在两个层次上进行了三种模型训练，下一个token值预测TVP 下一个token类型预测TTP 和语句完成SC</p>
<p>首先用一个通用的语言模型在无标签数据上取得通用参数，然后再分别再三个任务上训练。</p>
<p>因为token类型太少 所以划分是4：2：4，且在微调阶段只使用另外两个任务</p>
<p>介绍了主要模型的架构 GPT-2 真的就很呆地说了一下有什么什么层</p>
<p>列了一个概率分布函数</p>
<p>列了最优化函数&#x2F;好像也就是普通的Cross-Entropy Loss</p>
<p>简单列了一下式子模型的</p>
<img src="/2022/07/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-CodeFill-ICSE-22/image-20220713152712828.png" alt="image-20220713152712828" style="zoom: 50%;">

<p>训练时每个epoch随机选择一个任务</p>
<p>损失在所有任务间共享，生成类任务使用波束搜索</p>
<p>希望能够最小化<img src="/2022/07/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-CodeFill-ICSE-22/image-20220713153042852.png" alt="image-20220713153042852" style="zoom:50%;"></p>
<p>TVP 和TTP 都是类似的训练方法，使用遮罩单向预测，即看到左侧的上文推断下个token</p>
<p>SC是直到<EOS>生成才会结束</EOS></p>
<p>波束搜索也就介绍了一下， 介绍了3，5，10的宽度尝试</p>
<p>训练方法：</p>
<p>要不要搞个这种大图（？）</p>
<p><img src="/2022/07/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-CodeFill-ICSE-22/image-20220713153349806.png" alt="image-20220713153349806"></p>
<h2 id="后处理"><a href="#后处理" class="headerlink" title="后处理"></a>后处理</h2><p>这个部分很有意思</p>
<p>生成之后会重新排序列表给开发者</p>
<p>大概就是 每一项预测 会以&lt;token, type, probability&gt;的形式给出来，然后根据不同的类型会有不同的处理。</p>
<p>在某一个程度的局部，如果预测的是个函数，候选者中和前面来自同一类的就更有可能。</p>
<p>具体地说，会对前面的进行观察，然后存成一个列表，对预测列表对观察列表进行交叉检查 如果有重复，就根据预测项的类型乘以不同的权重，使其更有可能。</p>
<p>简单地说，就是认为变量和函数名等都有一定的聚集性，如果预测的某个在前面某种关联性出现了 那么就给他酌情更重要一些。</p>
]]></content>
      <categories>
        <category>Substance</category>
      </categories>
      <tags>
        <tag>Note</tag>
        <tag>ICSE 2022</tag>
        <tag>Code generation</tag>
      </tags>
  </entry>
  <entry>
    <title>论文笔记 Highway OCL &amp; Informal to Formal Specications in UML</title>
    <url>/2022/06/28/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-From-Informal-toFormal-Specications-in-UML/</url>
    <content><![CDATA[<p>这篇笔记是文章 From Informal to Formal Specications in UML 和 Automating Utility Permitting within Highway Right-of-Way via a Generic UML&#x2F;OCL Model and Natural Language Processing的不完全笔记</p>
<span id="more"></span>
<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>Purpose :</p>
<p> informal requirements,use cases &#x3D;&gt; formal specifications ,OCL</p>
<h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><p><strong>how to obtain formal specications from informal ones</strong></p>
<p>formal ones can also help to improve informal ones.</p>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>在intro中讲故事的这一部分很有意思 可能是因为是04年的文章 他的引入视角是——UML在OO建模很猛——有了OCL才能足够精确——不知道应该在什么环节集成OCL——还是应该customer来做，毕竟对系统行为清晰——期望所有人都有能力不现实</p>
<p>用例——describe system behaviors</p>
<p>研究了 文字用例中的前后置条件 与 类图中OCL写的操作的前后置条件 的<strong>关系</strong></p>
<p>从分析阶段的用例图——到设计阶段的时序图和类图</p>
<h2 id="用例图"><a href="#用例图" class="headerlink" title="用例图"></a>用例图</h2><p>非正式的本质 可以用在更多的上下文本经当中，但寻求正确的抽象层次和确保一致性困难</p>
<p>在他们的工作中 书写用例描述的风格不太重要 只要他们能够展现出需求与开发设计者的共识，获得前后置条件</p>
<p>考虑了关于替代流的问题 本来想掰开成为别的用例 但觉得违背了为用户满足目标的愿景</p>
<h1 id="状态图"><a href="#状态图" class="headerlink" title="状态图"></a>状态图</h1><p>考虑了用例的所有路径</p>
<h1 id="从用例到操作后置条件"><a href="#从用例到操作后置条件" class="headerlink" title="从用例到操作后置条件"></a>从用例到操作后置条件</h1><p>只讨论用例的后置条件 因为觉得前置差不多 而且有点迷惑——比方说满足了前置条件就相当于进入某些流’</p>
<p>所以就当所有的前置条件都是空</p>
<p>讨论系统中的唯一对象就是系统本身</p>
<p>客户给的自然语言描述后置条件可能不光和终态有关，还可能和事件序列有关 比如输错三次密码</p>
<p>——使用状态图来捕获条件</p>
<h1 id="Automating-Utility-Permitting-within-Highway-Right-of-Way-via-a-Generic-UML-x2F-OCL-Model-and-Natural-Language-Processing"><a href="#Automating-Utility-Permitting-within-Highway-Right-of-Way-via-a-Generic-UML-x2F-OCL-Model-and-Natural-Language-Processing" class="headerlink" title="Automating Utility Permitting within Highway Right-of-Way via a Generic UML&#x2F;OCL Model and Natural Language Processing"></a>Automating Utility Permitting within Highway Right-of-Way via a Generic UML&#x2F;OCL Model and Natural Language Processing</h1><p>OCL用来表达一些方位上的约束</p>
<p>implies的作用：只有前面的条件是真，后面的东西才会被评估</p>
<p>采用了扩展的OCL 增添了一些对空间语义的描述的函数 如不相交 相当 包含等</p>
<p>要将这样的文本约束转化为OCL——首先可以先变成较为形式化的元组</p>
<p>the 6-inch mechanical joint inlet shall be located 5 feet 6 inches below the ground</p>
<p>——》</p>
<p>&lt;mechanical joint inlet (6-inch), below (5 ft 6 in.), ground&gt;.</p>
<p>方位约束使用双层或多层语言层次来提取多种的方位信息</p>
<p>想要自动从方位约束中提取结构化的信息</p>
<ol>
<li>预处理：tokenize 句子划分和语法标注</li>
<li>特殊标注 增添了额外三个词表 城市产品，方位词和比较关系的词会被识别标出</li>
<li>逐级向上归纳 生成句子树</li>
<li>从树型结构中寻找目标信息，生成结构化的元组</li>
<li>从元组中生成所需OCL 不同层次有不同转换方法，主要是针对类不变量——x.allInstances-&gt;forAll ——implies这样的结构</li>
</ol>
<p>NLP的方法：</p>
<p>使用某个NLTK实施标注和句子树的构建，从裸文本生成出对应元组，作为构造OCL约束的输入</p>
<p>UML&#x2F;OCL的方法：</p>
<p>应用：</p>
<p>写在某种对象关系型数据库里 当有数据库的改动时触发触发器，检查是否符合约束</p>
]]></content>
      <categories>
        <category>Substance</category>
      </categories>
      <tags>
        <tag>OCL</tag>
        <tag>Code generation</tag>
      </tags>
  </entry>
  <entry>
    <title>CSAPP- Notes</title>
    <url>/2023/02/15/CSAPP-notes/</url>
    <content><![CDATA[<p>z</p>
<p>这是CSAPP的随手笔记</p>
<span id="more"></span>

]]></content>
      <categories>
        <category>Substance</category>
      </categories>
      <tags>
        <tag>Note</tag>
      </tags>
  </entry>
  <entry>
    <title>Note-CSE4650 - Methods of Data Mining</title>
    <url>/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/</url>
    <content><![CDATA[<p>这是关于数据挖掘方法的课程笔记。以前都是复习周才搞这种东西 但是现在发现不从课程前两周就开始会彻底完蛋…… 希望可以学的好一点</p>
<p>批话：</p>
<ul>
<li>我感觉我全搞明白了（9&#x2F;20，PCA)</li>
</ul>
<span id="more"></span>

<h1 id="学前信息"><a href="#学前信息" class="headerlink" title="学前信息"></a>学前信息</h1><h2 id="前置知识："><a href="#前置知识：" class="headerlink" title="前置知识："></a>前置知识：</h2><ul>
<li>概率：<ul>
<li>交并补概率的计算</li>
</ul>
</li>
<li>线性代数<ul>
<li>特征值与特征向量</li>
</ul>
</li>
<li>图论<ul>
<li>连通分量</li>
<li>团</li>
<li>点度</li>
<li>最短路</li>
</ul>
</li>
<li>算法<ul>
<li>复杂度估计</li>
</ul>
</li>
<li>统计<ul>
<li>向量的平均，中位数，方差和协方差</li>
<li>卡方($chi^2$)检测</li>
</ul>
</li>
</ul>
<h2 id="使用的教材"><a href="#使用的教材" class="headerlink" title="使用的教材"></a>使用的教材</h2><p>Charu C. Aggarwal: Data Mining: The Textbook, Springer 2015</p>
<h2 id="怎么拿分数"><a href="#怎么拿分数" class="headerlink" title="怎么拿分数"></a>怎么拿分数</h2><ul>
<li>5p for 5 exercise groups</li>
<li>10p for 5 homework in groups </li>
<li>24p for exam 12.13, 13:~16:</li>
<li>1p for prerequisite</li>
</ul>
<h2 id="学习目标"><a href="#学习目标" class="headerlink" title="学习目标"></a>学习目标</h2><ul>
<li>数据挖掘的基本问题模式和方法</li>
<li>对应问题，关键词的对策</li>
<li>验证方法</li>
<li>能使用程序完成</li>
<li>能够对问题的计算有所预估和替代方法</li>
<li>实践</li>
</ul>
<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><ul>
<li><p>什么是数据挖掘？</p>
<ul>
<li>没有确切的定义</li>
</ul>
<p>挑战：</p>
<ul>
<li>高效的算法</li>
<li>以假乱真的发现</li>
</ul>
<img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20230917002129599.png" alt="image-20230917002129599" style="zoom:50%;">

<p>和相关领域的关系：</p>
<p>![image-20230917002221082](课程笔记 CSE4650-Methods-of-Data-Mining&#x2F;image-20230917002221082.png)</p>
<ul>
<li>模型更倾向于全局数据，而模式更倾向于局部的</li>
</ul>
</li>
<li><p>过程：</p>
<ul>
<li>定义问题-预处理-mining——验证——展示和总结</li>
<li>经常来说，mine出来的东西会适合进入下一轮data mining</li>
</ul>
</li>
</ul>
<h1 id="预处理"><a href="#预处理" class="headerlink" title="预处理"></a>预处理</h1><ul>
<li>清洗：错误与缺失值</li>
<li>特征提取：结合与变形已有的形成新的特征</li>
<li>数据减少（reduction）：<ul>
<li>采用</li>
<li>特征选择</li>
<li>维度缩减</li>
</ul>
</li>
</ul>
<h2 id="清洗"><a href="#清洗" class="headerlink" title="清洗"></a>清洗</h2><p>Outliers：有时候要搞掉，但有时候很有用</p>
<h3 id="对付错误的办法："><a href="#对付错误的办法：" class="headerlink" title="对付错误的办法："></a>对付错误的办法：</h3><ul>
<li>从多个数据源检查不一致</li>
<li>使用 domain knowledge</li>
<li>检查 outliers 和 extreme value</li>
<li>数据平滑：噪声与随机波动<ul>
<li>scaling</li>
<li>discretization（离散化）</li>
<li>dimension reduction</li>
</ul>
</li>
<li>建模阶段使用 robust方法</li>
</ul>
<h3 id="缺失值"><a href="#缺失值" class="headerlink" title="缺失值"></a>缺失值</h3><p>可以的话，使用正确的替换</p>
<ul>
<li>feature 缺失的太多：不要（ prune）了</li>
<li>record 缺失的太多：整个不要了</li>
<li>估计而填补：<ul>
<li>均值&#x2F;中位数（考虑范围：在整个还是多大的局部？）</li>
<li>通过其他feature预测：随机森林</li>
<li>估算可能会有严重的影响</li>
</ul>
</li>
</ul>
<p>或者使用允许空值的算法</p>
<h2 id="特征提取："><a href="#特征提取：" class="headerlink" title="特征提取："></a>特征提取：</h2><ul>
<li>Scaling， normalization——num-&gt; num<ul>
<li>![image-20230917013505657](课程笔记 CSE4650-Methods-of-Data-Mining&#x2F;image-20230917013505657.png)</li>
<li><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20230917013528927.png" alt="image-20230917013528927" style="zoom:50%;"></li>
</ul>
</li>
<li>discretization： num-&gt; categorical<ul>
<li>将数据范围按照一定间隔分为类（bin), 并给标签</li>
<li>常用方法：<ul>
<li>等宽：尤其适用于均匀分布（uniform distribution）</li>
<li>等深：每类一样多</li>
</ul>
</li>
<li>好处和局限：<ul>
<li>可以对付噪音，个体差异和混合数据</li>
<li>算法更有效，也可以用更多的</li>
<li>丢失信息</li>
<li>优的离散化并不容易——可能取决于其他变量</li>
</ul>
</li>
</ul>
</li>
<li>二元化：cate-&gt; binary<ul>
<li>算是离散化的一种特例</li>
</ul>
</li>
<li>similarity graphs: * -&gt; graph （怎么做此处存疑）<ul>
<li>展现成对的相似性，通过最近邻</li>
<li>好处<ul>
<li>只要能算距离就行，不管种类<ul>
<li>尤其是聚类，推荐等</li>
</ul>
</li>
<li>可以使用很多网络算法</li>
</ul>
</li>
<li>可能复杂度$n^2$ 起跳</li>
<li>方法：<ul>
<li>把每个对象看成点</li>
<li>算每对的距离</li>
<li>如果距离小于$ \epsilon$ 就给他们连上无向边</li>
<li>或者</li>
<li>A是B的$K$个最近邻-&gt;连上B-A有向边（f方向可能可以被忽略）</li>
<li>使用一个权重公式，来衡量边的相似性<ul>
<li>![image-20230917173800899](课程笔记 CSE4650-Methods-of-Data-Mining&#x2F;image-20230917173800899.png)</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>多个有冗余性的特征中创造出一个新的</li>
<li>数据减少：<ul>
<li>样本采样</li>
<li>特征选择</li>
<li>维度缩减：<ul>
<li>旋转轴（PCA,SVD) ?</li>
<li>类型转换</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="Lecture-3-相似性和距离的衡量"><a href="#Lecture-3-相似性和距离的衡量" class="headerlink" title="Lecture 3 相似性和距离的衡量"></a>Lecture 3 相似性和距离的衡量</h1><ul>
<li>什么是距离？</li>
</ul>
<img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20230917174619556.png" alt="image-20230917174619556" style="zoom:50%;">

<p>需要满足四个性质：</p>
<ul>
<li>非负性 （non-negativity）</li>
<li>同一性（coincidence axiom）：<ul>
<li>d&#x3D;0 $iff$ A&#x3D;B</li>
</ul>
</li>
<li>对称性(symmetry)（无向距离）</li>
<li>三角不等式(triangle inequality)</li>
</ul>
<p>几个例子：</p>
<img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20230920162701682.png" alt="image-20230920162701682" style="zoom:50%;">

<p>证明一个玩意是距离：</p>
<ul>
<li>证这四个性质对于任意变量成立</li>
<li>——还是证明不是比较容易</li>
</ul>
<p>fractional $L_p$ 不是metric：不满足三角不等式</p>
<ul>
<li>距离与相似性：<ul>
<li>相似性通常在$[0,1]$</li>
<li>通常采用1-正则化距离 来得到相似性</li>
</ul>
</li>
</ul>
<p>度量空间（Metric Space) $(S,d)$-&gt; 数据和距离</p>
<p>三角不等式优化的一个例子：</p>
<p>给出$n$ 个点和$K$个聚类中心，找到每个点最近的中心</p>
<ul>
<li>朴素做法：$nK$ 次计算距离</li>
<li>剪枝技巧：<ul>
<li>计算所有中心彼此的距离</li>
<li>计算每个点和某个中心的距离，然后迭代查表寻找</li>
<li><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20230917180041406.png" alt="image-20230917180041406" style="zoom:50%;"></li>
<li>所以应该只是剪枝？</li>
</ul>
</li>
</ul>
<h2 id="不同数据类型的距离度量"><a href="#不同数据类型的距离度量" class="headerlink" title="不同数据类型的距离度量"></a>不同数据类型的距离度量</h2><h3 id="多维数类"><a href="#多维数类" class="headerlink" title="多维数类:"></a>多维数类:</h3><h4 id="Lp-范数：衡量向量空间中大小或者距离"><a href="#Lp-范数：衡量向量空间中大小或者距离" class="headerlink" title="Lp 范数：衡量向量空间中大小或者距离"></a>Lp 范数：衡量向量空间中大小或者距离</h4><p>Minkowski距离——Lp范数的一种特例</p>
<img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20230917183053294.png" alt="image-20230917183053294" style="zoom:50%;">

<p>P&#x3D;1 曼哈顿距离</p>
<p>P&#x3D;2 欧几里得距离</p>
<img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20230917183549887.png" alt="image-20230917183549887" style="zoom:50%;">

<ul>
<li>可以看出来 维度越高，就越由差最大的那一个决定</li>
</ul>
<p>Lp-norm的一个问题：</p>
<p>向量空间维度高了 就不好使了：</p>
<p>纬度越高，</p>
<ul>
<li>区分性就越小——随机数据集上最大和最小差的不多</li>
<li>就越关心单一维度的差异——999维度相同，一个不一样 就完全反映的是不一样的</li>
<li>解决的可能办法：给某维度加上权重</li>
</ul>
<h4 id="Match-based-similarity-with-proximity-thresholding"><a href="#Match-based-similarity-with-proximity-thresholding" class="headerlink" title="Match-based similarity with proximity thresholding"></a>Match-based similarity with proximity thresholding</h4><p>（有临近阈值约束的相似性衡量）</p>
<p>这个东西是为了解决以下的两个问题：</p>
<ol>
<li>特征只在局部有相关性（糖尿病人的血糖而不是瘫痪的）</li>
<li>大维度下，两个对象不太可能一样，除非某些特征相近</li>
</ol>
<p>手段是 强调其相似的维度，</p>
<p>具体做法是，对每一个维度进行等深离散化，只关注每个相同bin中的距离</p>
<img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20230917205442693.png" alt="image-20230917205442693" style="zoom:50%;">

<p>这张图中就只关注$1，3$两个维度的距离差</p>
<img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20230917205716976.png" alt="image-20230917205716976" style="zoom:50%;">

<p>挑选参数：维度$K$越大，$m$ 分的bin就越多</p>
<h4 id="Cos-相似性"><a href="#Cos-相似性" class="headerlink" title="Cos 相似性"></a>Cos 相似性</h4><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20230917205856749.png" alt="image-20230917205856749" style="zoom:50%;">

<ul>
<li>适用性：<ul>
<li>数类（整数，实数</li>
<li>二进制数</li>
</ul>
</li>
<li>[ − 1, 1]</li>
<li>常用于数字表示的文本文档</li>
<li>如果向量都被正则化了，那么和L2是有关系的</li>
</ul>
<p>![image-20230917210131237](课程笔记 CSE4650-Methods-of-Data-Mining&#x2F;image-20230917210131237.png)</p>
<p>新问题：</p>
<p>距离是否应该反应数据分布呢？ 换句话说，在更稀疏的方向上，同样的差昭示着更大的差异：</p>
<h4 id="Mahalanobis-distance"><a href="#Mahalanobis-distance" class="headerlink" title="Mahalanobis distance"></a>Mahalanobis distance</h4><p>马哈拉诺比斯距离， 考虑了各个特征之间的相关性和不同特征的方差</p>
<p>公式：</p>
<img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20230917210619748.png" alt="image-20230917210619748" style="zoom:50%;">

<p>​	$\Sigma^-1$  协方差矩阵的逆，描述了数据特征间的相关性，逆矩阵用来给特征差异加权</p>
<p>场景：</p>
<img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20230917210800750.png" alt="image-20230917210800750" style="zoom:67%;">



<p>类似的推断：</p>
<p>如果距离测量只能通过一定路径呢？ 比如 最近邻图</p>
<h4 id="ISOMAP"><a href="#ISOMAP" class="headerlink" title="ISOMAP"></a>ISOMAP</h4><p>Isometric Mapping 等距映射</p>
<p>创造一个近邻图，对于每个点，对于其K个近邻链接</p>
<p>距离为两点间的最短路</p>
<p>可选步骤：嵌入数据，降维表示</p>
<p>![image-20230917213449689](课程笔记 CSE4650-Methods-of-Data-Mining&#x2F;image-20230917213449689.png)</p>
<p>实际上使用了最近邻建图才会发现其实很远</p>
<h3 id="类数据"><a href="#类数据" class="headerlink" title="类数据"></a>类数据</h3><p>常用方程：</p>
<img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20230917213718931.png" alt="image-20230917213718931" style="zoom:50%;">

<ul>
<li>Wi&#x3D; $1&#x2F;k$</li>
<li>S采用二元表示</li>
</ul>
<p>如果考虑到频率的话，就使用</p>
<h4 id="Goodall-measure"><a href="#Goodall-measure" class="headerlink" title="Goodall measure"></a>Goodall measure</h4><p>其中，越常见的特征对于总体相似贡献越低</p>
<img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20230917214229908.png" alt="image-20230917214229908" style="zoom:50%;">

<img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20230917214253794.png" alt="image-20230917214253794" style="zoom:50%;">

<p>总体分数：</p>
<img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20230917214240957.png" alt="image-20230917214240957" style="zoom:50%;">

<p>下面除以特征数，上面统计每一个特征的贡献，其中越稀有的特征贡献越接近1，不同的特征相似为0</p>
<h3 id="混合数据（无需变形）"><a href="#混合数据（无需变形）" class="headerlink" title="混合数据（无需变形）"></a>混合数据（无需变形）</h3><p>给与权重，再分别计算</p>
<p>![image-20230917231626261](课程笔记 CSE4650-Methods-of-Data-Mining&#x2F;image-20230917231626261-16949817904071.png)</p>
<p>通常范畴不同，所以还需要使用标准差进行标准化</p>
<h3 id="二元数据"><a href="#二元数据" class="headerlink" title="二元数据"></a>二元数据</h3><p>直接算：</p>
<p>Hamming Distance</p>
<img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20230917225205567.png" alt="image-20230917225205567" style="zoom:50%;">



<p>通常可以将set变形，得到长二元数据</p>
<p>-&gt; set通常很稀疏，常见元素就会显得很一致，即，忽略了集合本身的大小</p>
<img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20230917225350551.png" alt="image-20230917225350551" style="zoom:50%;">

<p>这两个距离都是10 有点失真</p>
<p>使用 <strong>Jaccard coefficient</strong>  进行计算，这是一个常用于集合相似性的计算方式</p>
<img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20230917225520590.png" alt="image-20230917225520590" style="zoom:50%;">

<p>其中0和1的权重不一样</p>
<h3 id="字符串数据"><a href="#字符串数据" class="headerlink" title="字符串数据"></a>字符串数据</h3><p>Hamming Distance就不好用了：</p>
<ul>
<li>必须等长</li>
<li>typo惩罚太大</li>
</ul>
<p>使用<strong>Levenshtein distance</strong> ，即编辑距离，包括以下三种操作：</p>
<ul>
<li>插入</li>
<li>删除</li>
<li>替换</li>
</ul>
<p>可以给不同的操作辅以权重</p>
<p><strong>当其满足以下条件，才是mertic：</strong></p>
<ul>
<li>操作代价为正</li>
<li>反操作存在且代价一致</li>
</ul>
<h3 id="文本数据"><a href="#文本数据" class="headerlink" title="文本数据"></a>文本数据</h3><p>将文档转化为m长度的向量（词典大小）</p>
<p>统计每个词的频数</p>
<p>然后计算<strong>cos相似性</strong> （Jaccard coefficient 也不是不能用</p>
<h2 id="注意："><a href="#注意：" class="headerlink" title="注意："></a>注意：</h2><p>名字和参数一般都不太确定，所以最好还是加点文献引用</p>
<p>重点：</p>
<ul>
<li>高维Lp的不好使</li>
<li>cos和阈值约束的距离</li>
<li>non-metrics可能对于高位的相似性计算更好（还没学吧）</li>
</ul>
<h1 id="Lecture3-Dimension-reduction-PCA-SVD"><a href="#Lecture3-Dimension-reduction-PCA-SVD" class="headerlink" title="Lecture3 Dimension reduction(PCA,SVD)"></a>Lecture3 Dimension reduction(PCA,SVD)</h1><p> 主成分分析（Principal component analysis (PCA) </p>
<p>这章的目的应该是对数据降维，手段包括特征<strong>eigen</strong> 和奇异 <strong>singular</strong></p>
<h2 id="动机："><a href="#动机：" class="headerlink" title="动机："></a>动机：</h2><ul>
<li>高维数据比较困难<ul>
<li>难以聚类</li>
<li>对于特征强相关的 可以用少数特征来表示同样的信息</li>
</ul>
</li>
</ul>
<p>手段：缩减维度来减少冗余</p>
<ul>
<li><p>只知道成对的距离</p>
</li>
<li><p>降低开销</p>
</li>
<li><p>去除噪声</p>
</li>
<li><p>结果更好理解</p>
<pre><code>- 早期做会丢失信息
</code></pre>
</li>
</ul>
<img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20230920215126603.png" alt="image-20230920215126603" style="zoom:50%;">



<h2 id="前置数学知识"><a href="#前置数学知识" class="headerlink" title="前置数学知识"></a>前置数学知识</h2><p>需要的数学知识比较多，包括协方差矩阵(covariance matrix) 拉普拉斯矩阵，半正定矩阵，矩阵的奇异值向量等</p>
<p><strong>向量$x$一般说的都是列向量</strong></p>
<p>下面先对协方差矩阵进行一些学习。</p>
<h3 id="协方差矩阵"><a href="#协方差矩阵" class="headerlink" title="协方差矩阵"></a>协方差矩阵</h3><p>相关系数用来描述两个变量之间的<strong>线性</strong>相关关系，范围从[-1,1]<br>$$<br>ρ &#x3D; (Σ((X - μX)(Y - μY))) &#x2F; (σX * σY)<br>$$<br>相关系数可以认为是标准化了的协方差</p>
<p>协方差矩阵是这么算的</p>
<p>注意其中的$X,Y$ 可以理解为多元变量，即每一项是一个一维向量（一个变量）的二位变量<br>$$<br>Cov(X, Y) &#x3D; Σ((X - μX)(Y - μY)) &#x2F; N-1<br>$$<br>![img](课程笔记 CSE4650-Methods-of-Data-Mining&#x2F;Correlation55-16952207836256.png)</p>
<p>![img](课程笔记 CSE4650-Methods-of-Data-Mining&#x2F;Correlation66.png)<br>$$<br>A&#x3D;\left{\begin{matrix}X &amp; Y &amp; Z\end{matrix}\right}&#x3D;\left{ \begin{matrix} x_1-x &amp; y_1-y &amp; z_1-z \ … &amp; … &amp; …\ x_n-x &amp; y_n-y &amp; z_n-z \end{matrix} \right} \tag{2}<br>$$<br>其中$X,Y,Z$都是随机变量，$x_i$可以理解为一次观测值</p>
<p>$$<br>C(X,Y,Z)&#x3D;{A^TA \over n}<br>$$</p>
<ul>
<li><p>协方差矩阵的性质：</p>
<ul>
<li>对角线上是每个变量的方差</li>
<li>显然是一个对称矩阵，$C&#x3D;C^T$，<ul>
<li>因为$Cov(X,Y)&#x3D;Cov(Y,X)$</li>
</ul>
</li>
<li>他还是一个半正定矩阵</li>
</ul>
<blockquote>
<p>至于为什么正定 暂且还没有搞懂</p>
</blockquote>
</li>
</ul>
<p><strong>对于零均值（mean-centered)的数据，协方差矩阵的特征向量矩阵是正交矩阵</strong></p>
<p>update: 对称矩阵的特征向量都是正交的——正定矩阵的特征向量都是正交的</p>
<h3 id="正交矩阵"><a href="#正交矩阵" class="headerlink" title="正交矩阵"></a>正交矩阵</h3><p>对于矩阵 Q，以下条件等价：</p>
<ol>
<li>Q 是正交矩阵。</li>
<li>Q 的列向量是单位向量，并且两两正交（即内积为零）。</li>
<li>Q 的转置$ Q^T $是其逆矩阵（$Q^TQ &#x3D; QQ^T &#x3D; I$，其中 I 是单位矩阵）</li>
</ol>
<h3 id="半正定矩阵"><a href="#半正定矩阵" class="headerlink" title="半正定矩阵"></a>半正定矩阵</h3><p>首先，在实数范畴内，矩阵的正定性都是在其为对称矩阵的前提下研究的，即：</p>
<center>
    （半）正定矩阵一定是对称矩阵
</center>

<p>定义：</p>
<p> 对对称矩阵A，若对任意非零向量$x$，有$x^TAx&gt;0$,则称$A$为正定矩阵，若能取到0，则为半正定矩阵</p>
<p>性质：</p>
<ul>
<li>特征值大于（等于0） 半正定的时候能取到</li>
<li>对角线元素非负</li>
</ul>
<h3 id="特征值分解"><a href="#特征值分解" class="headerlink" title="特征值分解"></a>特征值分解</h3><p>一般来说，求解特征值是为了实现矩阵的对角化：<br>$$<br>A &#x3D; PDP⁻¹<br>$$<br>$A$是方阵</p>
<p>$P$是可逆矩阵，列向量为$A$的特征向量</p>
<p>$D$是对角阵，对角线是$A$的特征值，顺序与$P$对应</p>
<p>对角化的矩阵具有很多应用，如快速计算A的高次幂。</p>
<p><strong>对角化步骤：</strong></p>
<p>首先对特征值，特征向量的一般形式进行了解：<br>$$<br>对方阵A,有Ax&#x3D;\lambda x ，称x是A的一个特征向量，\lambda 是x对应的特征值<br>$$</p>
<h4 id="特征值求解"><a href="#特征值求解" class="headerlink" title="特征值求解"></a>特征值求解</h4><ol>
<li>求解det(行列式) $det(A-\lambda I)&#x3D;0$,求出若干特征值</li>
<li>求解$Ax&#x3D;\lambda x$,求出每个特征值对应的特征向量（非零）</li>
</ol>
<blockquote>
<p>特征值有很多良好的性质：</p>
<img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20230920200335261.png" alt="image-20230920200335261" style="zoom:50%;">
</blockquote>
<p>​	此时，如果</p>
<pre><code>    1. $A$满秩
    1. $X=&#123;x_1,...,x_n&#125;$特征向量是一组线性无关的
</code></pre>
<p>​	那么矩阵可以对角化为<br>$$<br>A &#x3D; PDP⁻¹<br>$$<br>对于零均值（mean-centered)的数据，协方差矩阵的特征向量矩阵是正交矩阵,即<br>$$<br>P^{-1}&#x3D;P^T<br>$$<br>所以也可以写成<br>$$<br>C&#x3D;PDP^T<br>$$</p>
<h3 id="奇异值分解"><a href="#奇异值分解" class="headerlink" title="奇异值分解"></a>奇异值分解</h3><p>SVD能够适用于任何矩阵分解，</p>
<p>对于任意矩阵$A$,</p>
<p>对$AA^T$求特征值，用单位化特征向量构成$U$</p>
<p>对$A^TA$求特征值，用单位化特征向量构成$V$</p>
<p>对$AA^T或A^TA$求特征值的平方根，构成对角阵$\Sigma$<br>$$<br>A&#x3D;U\Sigma V^T<br>$$<br>U和V分别成为左右奇异值矩阵</p>
<h2 id="假设"><a href="#假设" class="headerlink" title="假设"></a>假设</h2><ul>
<li>高方差可以展示数据的结构<ul>
<li>为什么？</li>
<li>如果在这一维度（i.e.旋转后的坐标轴投影）上，数据具有较大的方差，就可以认为这一个维度能够反映作为信号的信息，而其他轴可能就是它的噪声</li>
</ul>
</li>
<li>数据可以被正交基向量的线性组合表示</li>
</ul>
<p>希望可以用一个变形的矩阵$d<em>r的P$，来将$n</em>d的 D$表示为$n<em>r$的$D</em>P_r$(r&lt;d)</p>
<p><strong>即，希望把数据由d维降到r维</strong></p>
<blockquote>
<p>这r维是全新的正交特征，也成为主成分，希望可以有相对最大的数据方差，而舍弃的维度中，方差几乎为0</p>
</blockquote>
<h2 id="特征值分解-1"><a href="#特征值分解-1" class="headerlink" title="特征值分解"></a>特征值分解</h2><p>怎么让方差最大呢？就需要用协方差矩阵，对应的特征值所对应的$k$个特征向量所组成的矩阵</p>
<p>首先考虑零均值（mean-centered)的数据$D$,</p>
<p>一般数据的零均值过程如下：</p>
<img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20230920214837704.png" alt="image-20230920214837704" style="zoom:50%;">

<p>每一列计算其均值，分别减掉</p>
<p>计算协方差矩阵<br>$$<br>C &#x3D; {1\over n − 1} D^T D<br>$$<br>将其对角化为<br>$$<br>C&#x3D;PΛP^T<br>$$<br>选取$P$的前$r$大的特征值对应的向量，组成新的特征矩阵<br>$$<br>P_r<br>$$<br>目标降维矩阵即为<br>$$<br>D^&#96;&#x3D;DP_r<br>$$</p>
<h2 id="奇异值SVD分解"><a href="#奇异值SVD分解" class="headerlink" title="奇异值SVD分解"></a>奇异值SVD分解</h2><p>步骤与上面类似，区别在于在对角化的一步使用奇异值分解，</p>
<p>$$<br>C&#x3D;Q\Sigma P^T<br>$$<br>对$P$选择前r大的特征向量构成$P_r$</p>
<p>目标降维矩阵即为<br>$$<br>D^&#96;&#x3D;DP_r<br>$$</p>
<blockquote>
<p>此处仅使用了右奇异矩阵，就是对样本的列进行了压缩。而使用左奇异矩阵可以完成对行数的压缩</p>
<p>（来自参考）</p>
</blockquote>
<p>使用左奇异矩阵$D^TQ_r$ describes items by r latent components</p>
<ul>
<li>可以大幅降维</li>
<li>LSA（潜在成分分析使用）</li>
<li>文档格式的矩阵</li>
<li>减少同义词噪声</li>
<li><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20230920220923499.png" alt="image-20230920220923499" style="zoom:50%;"></li>
</ul>
<p>??noise reduction: truncated SVD tends to correct inconsistencies??</p>
<h2 id="怎么挑选r"><a href="#怎么挑选r" class="headerlink" title="怎么挑选r"></a>怎么挑选r</h2><p>降维后的维度r（主成分个数)</p>
<p>有一个类似于置信度的东西，笔记就不说了</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>启发性， Work well only if the underlying assumptions are true.</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>这一篇讲得肯定比我一下午写得明白</p>
<p><a href="https://medium.com/ai%E5%8F%8D%E6%96%97%E5%9F%8E/preprocessing-data-%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90-pca-%E5%8E%9F%E7%90%86%E8%A9%B3%E8%A7%A3-afe1fd044d4f">https://medium.com/ai%E5%8F%8D%E6%96%97%E5%9F%8E/preprocessing-data-%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90-pca-%E5%8E%9F%E7%90%86%E8%A9%B3%E8%A7%A3-afe1fd044d4f</a></p>
<h1 id="谱聚类-Spectral-clustering"><a href="#谱聚类-Spectral-clustering" class="headerlink" title="谱聚类 Spectral clustering"></a>谱聚类 Spectral clustering</h1><p>9.19</p>
<blockquote>
<p>它的主要思想是把所有的数据看做空间中的点，这些点之间可以用边连接起来。距离较远的两个点之间的边权重值较低，而距离较近的两个点之间的边权重值较高，通过对所有数据点组成的图进行切图，让切图后不同的子图间边权重和尽可能的低，而子图内的边权重和尽可能的高，从而达到聚类的目的。</p>
</blockquote>
<p>主要关注点之间的距离</p>
<p>所需要的东西：</p>
<ul>
<li>相似图$G$<ul>
<li>数据点</li>
<li>边权$w_{ij}$ 两点间的相似性</li>
</ul>
</li>
</ul>
<h2 id="图"><a href="#图" class="headerlink" title="图"></a>图</h2><p>需要四个矩阵</p>
<ol>
<li>权重矩阵$W$</li>
<li>点度对角阵Λ</li>
<li>拉普拉斯矩阵 $L &#x3D; Λ − W$</li>
<li>可能需要正则化的拉着普拉斯矩阵</li>
</ol>
<p>做法：</p>
<p>​	<img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20230919165658041.png" alt="image-20230919165658041" style="zoom:50%;"></p>
<p>首先有这么一个相似度（边权）的邻接矩阵$W$</p>
<ul>
<li>无向图</li>
<li>没有权重就0，1</li>
</ul>
<p>然后得到点度对角阵 显示每个点的度（边权的和）</p>
<img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20230919165835601.png" alt="image-20230919165835601" style="zoom:50%;">

<p>然后得到拉普拉斯矩阵 $L &#x3D; Λ − W$</p>
<img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20230919170046657.png" alt="image-20230919170046657" style="zoom:50%;">

<p>$L$ 的好性质：</p>
<ul>
<li><p>对称矩阵</p>
<ul>
<li>特征值都是实数（为啥）</li>
</ul>
</li>
<li><p>半正定的，最小的特征值等于0</p>
</li>
<li></li>
<li><p>将数据展现在低微向量空间中</p>
</li>
</ul>
<h1 id="聚类验证"><a href="#聚类验证" class="headerlink" title="聚类验证"></a>聚类验证</h1><p>9。19</p>
]]></content>
      <categories>
        <category>Substance</category>
      </categories>
      <tags>
        <tag>Note</tag>
        <tag>Course</tag>
      </tags>
  </entry>
</search>
