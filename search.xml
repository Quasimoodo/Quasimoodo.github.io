<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>About Moby Dick</title>
    <url>/2022/05/23/About-Moby-Dick/</url>
    <content><![CDATA[<blockquote>
<p>现在，除了一艘轻轻摇晃的船赋予你的摇摆不定的生命，你没有生命；船的生命借自于大海；大海的生命借自于上帝神秘难测的潮汐。可是，当这睡眠，这幻梦将你笼罩，你的手或脚要是稍微挪动一下——你的双手彻底松开——你就会在惊恐中能够恢复自己的本性。你就盘旋在笛卡尔的涡流之上了。而也许，恰当正午，又是响晴的天气，你便随着一声半带窒息的尖叫，穿过透明的空气，坠入夏天的海洋，再也没有浮上来。好好留神吧，你们这些泛神论者！——《白鲸》桅顶瞭望</p>
</blockquote>
<blockquote>
<p>There is no life in thee, now, expect that rocking life imparted by a gently rolling ship; by her, borrowed from the sea; by the sea, from the inscrutable tides of God. But while this sleep, this dream is on ye, move your foot or hand an inch, slip your hold at all; and your identity comes back in horror. Over Descartian vortices you hover. And perhaps, at mid-day, in the fairest weather, with one half-throttled shriek you drop through that transparent air into the summer sea, no more to rise for ever. Heed it well, ye Pantheists! ——&lt;Moby dick&gt; CHAPTER 35 The Mast-Head</p>
</blockquote>
]]></content>
      <categories>
        <category>Metaphysics</category>
      </categories>
      <tags>
        <tag>Moby Dick</tag>
        <tag>Reading</tag>
      </tags>
  </entry>
  <entry>
    <title>Bajwa OCL 相关笔记</title>
    <url>/2022/07/31/Bajwa-OCL-%E7%9B%B8%E5%85%B3%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<p>这是Bajwa 有关OCL 生成工作的笔记</p>
<span id="more"></span>

<p>endow 资助 赋予</p>
<p>expertise 专业知识</p>
<h1 id="2010"><a href="#2010" class="headerlink" title="2010"></a>2010</h1><p>OCL Constraints Generation from Natural Language Specification</p>
<p>通过英语生成约束和前后条件</p>
<p>主要问题：将自然语言形式化：SBVR</p>
<p>使用 LESSA [14]（用于语义分析的语言工程系统）方法对 NL 表示进行语义分析</p>
<p>解析自然语言，将自然语言的部分映射到对应的SBVR规则上</p>
<p>将名词，动词，形容词映射到类，实例，方法和属性，也有规则</p>
<p><img src="/2022/07/31/Bajwa-OCL-%E7%9B%B8%E5%85%B3%E7%AC%94%E8%AE%B0/image-20220806213704877.png" alt="image-20220806213704877"></p>
<p>读取XMI作为目标UML类模型的输入，将SBVR映射到类模型</p>
<h2 id="2012"><a href="#2012" class="headerlink" title="2012"></a>2012</h2><p>Translating natural language constraints to OCL</p>
<p>基本没什么区别 方法说的更详细一点 还补了几个经验性的判断</p>
<h1 id="2017"><a href="#2017" class="headerlink" title="2017"></a>2017</h1><p><strong>Automated Generation of OCL Constraints: NL based Approach vs Pattern Based Approach</strong></p>
<p>通过对自然语言的语法和语义分析 得到SBVR为基础的半正则表达，下来就很容易翻译成别的了。</p>
<p>研究趋势：</p>
<p>手动将英语约束映射到OCL</p>
<p>遇到的问题：</p>
<ol>
<li>用到的英文词汇必须是系统中有的</li>
<li>英语同样具有歧义性</li>
<li>如果英语没有问题 那么就可以自动生成了</li>
</ol>
<p>限制&#x2F;不足：</p>
<ul>
<li>NL不应包括UML类图以外的词汇，名字也应该一致</li>
<li>不完整，无效的就不行</li>
<li>不能包括UML聚合</li>
<li>不能有带参数的函数调用</li>
<li>异或关系不支持</li>
<li><img src="/2022/07/31/Bajwa-OCL-%E7%9B%B8%E5%85%B3%E7%AC%94%E8%AE%B0/image-20220803222739734.png" alt="image-20220803222739734"></li>
<li>query based 不可以</li>
<li>对于有歧义的句子不太行</li>
<li>需要用户手动纠正</li>
<li>只能处理一个句子</li>
</ul>
<p>OCL用的少：</p>
<p>introducing a template based language：</p>
<p>3 Wahler, M., “Patterns to Develop Consistent Design Constraints”, Ph.D. Thesis, ETH Zurich, Switzerland, 2007.</p>
<p>OCL困难的语法：</p>
<p>4 Gogolla, M., Büttner, F., and Richters, M., “USE: A UML-Based Specification Environment for Validating UML and OCL”, Science of Computer Programming, Volume 69, No. 1, pp. 27-34, 2007.</p>
<p>同义可以有多个实现</p>
<p>[5] Cabot, J., “Ambiguity Issues in OCL Postconditions”, Proceedings of 6th Conference OCL Workshop at the UML&#x2F;MoDELS, pp. 194-204, 2006. </p>
<p>[6] Kristofer, J., “Disambiguation Implicit Constructions in OCL”, Conference on OCL and Model Driven Engineering, Lisbon, Portugal, pp. 30-44, October 12, 2004.</p>
<p>大型软件建模中可能有OCL的理解性问题</p>
<p>[7] Correa A., Werner, C., and Barros, M., “An Empirical Study of the Impact of OCL Smells and Refactorings on the Understandability of OCL Specifications”, MODELS, LNCS 4735, pp. 76-90, 2007.</p>
<p>template based approach (the Copacabana [27] too</p>
<p>Raj, A., Prabharkar, T., and Hendryx, S., “Transformation of SBVR Business Design to UML Models”, ACM Conference on India Software Engineering, pp. 29-38, 2008.</p>
<h1 id="Constraint-Detection-in-Natural-Language-Problem-Descriptions"><a href="#Constraint-Detection-in-Natural-Language-Problem-Descriptions" class="headerlink" title="Constraint Detection in Natural Language Problem Descriptions"></a>Constraint Detection in Natural Language Problem Descriptions</h1><p>rigorous 严密的</p>
<p>alleviate 减轻 缓和</p>
<p>tailored 定做的 时尚的</p>
<p>substantial 重要的</p>
<p>insubstantial 毫无疑问的</p>
<p>目的：</p>
<p>Automated model reformulation aims at assisting a naive user in modeling constraint problems.</p>
<p>做的事：</p>
<p>detecting constraints in natural language problem descriptions using a structured-output classifier.</p>
<p>检测文本中描述约束的部分？？</p>
<p>主要的工作内容就是使用SVM-HMM进行了对于输入文本中，标注出描述的约束部分，算是一个文本提取&#x2F;标记问题</p>
<h1 id="Automating-Inference-of-OCL-Business-Rules-from-User-Scenarios"><a href="#Automating-Inference-of-OCL-Business-Rules-from-User-Scenarios" class="headerlink" title="Automating Inference of OCL Business Rules from User Scenarios"></a>Automating Inference of OCL Business Rules from User Scenarios</h1><p>OCL的作用：</p>
<p>OCL could be employed in several other cases such as (1) to describe pre- and post conditions of operations, (2) to give restrictions as guards in a state transition system and (3) to query over a given system state with OCL queries [4].</p>
<p>做的事：</p>
<p>generate the ocl business rules, invariants  from the conceptual model for the designer,use the predefined ocl invariant pattern and the snapshots of the conceputal model</p>
<h1 id="Bidirectional-Translation-between-OCL-and-JML-for-Round-trip-Engineering"><a href="#Bidirectional-Translation-between-OCL-and-JML-for-Round-trip-Engineering" class="headerlink" title="Bidirectional Translation between OCL and JML for Round-trip Engineering"></a>Bidirectional Translation between OCL and JML for Round-trip Engineering</h1><p>OCL 和 JML 的双向翻译 保持OCL的原有形态</p>
<h1 id="OCL-Constraints-Automatic-Generation-for-UML-Class-Diagram"><a href="#OCL-Constraints-Automatic-Generation-for-UML-Class-Diagram" class="headerlink" title="OCL Constraints Automatic Generation for UML Class Diagram"></a>OCL Constraints Automatic Generation for UML Class Diagram</h1><p>use predefined template and lexcial analyse to generate simple ocl constraints for input uml xmi</p>
<h1 id="Generating-OCL-Constraints-from-Test-Case-Schemas-for-Testing-Model-Behavior"><a href="#Generating-OCL-Constraints-from-Test-Case-Schemas-for-Testing-Model-Behavior" class="headerlink" title="Generating OCL Constraints from Test Case Schemas for Testing Model Behavior"></a>Generating OCL Constraints from Test Case Schemas for Testing Model Behavior</h1><p>形式化合约的重要性——引出OCL</p>
<p>项目验证，测试用例生成</p>
<p>OCL是轻量化的合约——OCL重要性</p>
<p>现状：难写难读，</p>
<p>需要一个辅助写对的东西</p>
<p>干这个事情的难点：</p>
<p>NL的二义性</p>
<p>本身不那么直接对应——OO-类</p>
<p>六点……</p>
<p><img src="/2022/07/31/Bajwa-OCL-%E7%9B%B8%E5%85%B3%E7%AC%94%E8%AE%B0/image-20220807203515349.png" alt="image-20220807203515349"></p>
<p>现有工作对难点的应对情况：那些解决好，哪些解决不好：限制分析清楚</p>
<p>我们的工作提出方法 应对他们的缺陷</p>
<p><img src="/2022/07/31/Bajwa-OCL-%E7%9B%B8%E5%85%B3%E7%AC%94%E8%AE%B0/image-20220807203635745.png" alt="image-20220807203635745"></p>
<p><img src="/2022/07/31/Bajwa-OCL-%E7%9B%B8%E5%85%B3%E7%AC%94%E8%AE%B0/image-20220809130755249.png" alt="image-20220809130755249"></p>
<p><img src="/2022/07/31/Bajwa-OCL-%E7%9B%B8%E5%85%B3%E7%AC%94%E8%AE%B0/image-20220814202922555.png" alt="image-20220814202922555"><img src="/2022/07/31/Bajwa-OCL-%E7%9B%B8%E5%85%B3%E7%AC%94%E8%AE%B0/image-20220814202948326.png" alt="image-20220814202948326"></p>
]]></content>
      <categories>
        <category>Substance</category>
      </categories>
      <tags>
        <tag>OCL</tag>
        <tag>Note</tag>
      </tags>
  </entry>
  <entry>
    <title>CS224N</title>
    <url>/2022/07/24/CS224N/</url>
    <content><![CDATA[<p>这是Stanford 课程CS224N的学习笔记 可能还有一些别的。<br>Winter, 2021 <a href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1214/">https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1214/</a></p>
<span id="more"></span>
<p>enamor 迷恋</p>
<p>zoom in&#x2F;out 缩放</p>
<p>fiddle 调整，伪造</p>
<p>likehood 似然——给定结果求某参数的可能性</p>
<p>open region 开区间</p>
<p>amplify 放大（扩增）</p>
<p>partial derivative 偏导数</p>
<p>denominator 分母</p>
<p>arithmetic 算术</p>
<p>analogy 类比 比喻</p>
<h1 id="L1"><a href="#L1" class="headerlink" title="L1"></a>L1</h1><p>最早：类似于WordNet：手工编写的词汇关系-hierathy and synomy ——依靠劳动，无法随时更新，无法衡量相似性</p>
<p>——离散的单独符号表示——one hot encode 高维向量——希望可以用稠密向量dense vector衡量相似性</p>
<p><strong>morden statical NLP:</strong></p>
<p>distributional semantic——Represent a word meaning by its context(a fix window)</p>
<p>不可能由人类来手工编写：</p>
<h2 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec:"></a>Word2Vec:</h2><h3 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h3><p>给定中心词，最大化周围context 的概率：(i.e. 调整中心词向量)</p>
<p>条件概率，左右词汇的概率连乘</p>
<p>目标是最小化损失函数：</p>
<p>-log avg</p>
<p>log  是因为处理和比积方便</p>
<img src="/2022/07/24/CS224N/image-20220802174103357.png" alt="image-20220802174103357" style="zoom:50%;">

<h3 id="如何计算条件概率？"><a href="#如何计算条件概率？" class="headerlink" title="如何计算条件概率？"></a>如何计算条件概率？</h3><ul>
<li>根据用途 用两个向量表示一个词</li>
<li>正则化除法用的是整个词典的概率</li>
</ul>
<p><img src="/2022/07/24/CS224N/image-20220802181244780.png" alt="image-20220802181244780" style="zoom: 50%;"><img src="/2022/07/24/CS224N/image-20220802184259606.png" alt="image-20220802184259606" style="zoom:50%;"></p>
<h3 id="所以要如何得到向量呢？"><a href="#所以要如何得到向量呢？" class="headerlink" title="所以要如何得到向量呢？"></a>所以要如何得到向量呢？</h3><p>想要得到向量 就是要优化模型，考虑向量维数D，每个词向量个数2和词典大小V，就是有2DV的总参数量需要优化，使用梯度下降</p>
<p><img src="/2022/07/24/CS224N/image-20220802185657846.png" alt="image-20220802185657846"></p>
]]></content>
      <categories>
        <category>Substance</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>Course</tag>
      </tags>
  </entry>
  <entry>
    <title>Philip Larkin 诗鉴赏</title>
    <url>/2022/06/04/Philip-Larkin/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>今天读到了Philip Larkin(菲利普·拉金) 觉得他确实写出了一些人类的共有困境，是超越东西方的 也不好说是现代视角或者古典视角。用诗歌描述一种……symptom？没有意象的堆叠或者是故作惊人之语。恰到好处的建筑与音韵意识又不喧宾夺主，克制的情感流露并着个人色彩。 类似于散文诗？</p>
<span id="more"></span>
<p>现摘在这里这一首，「Love Songs in Age」<br>She kept her songs, they kept so little space,<br>The covers pleased her:<br>One bleached<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="floating, drift,漂白
">1</span></a></sup> from lying in a sunny place,<br>One marked in circles by a vase of water,<br>One mended<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="repair,patch,修补
">2</span></a></sup>, when a tidy fit had seized her,<br>And coloured, by her daughter -<br>So they had waited, till, in widowhood<br>She found them, looking for something else, and stood</p>
<p>Relearning how each frank submissive chord<sup id="fnref:3"><a href="#fn:3" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="和弦
">3</span></a></sup><br>Had ushered<sup id="fnref:4"><a href="#fn:4" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="（迎宾员式的）引导
">4</span></a></sup> in<br>Word after sprawling hyphenated<sup id="fnref:5"><a href="#fn:5" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="连字符
[^6 ]:great brightness,耀眼的
">5</span></a></sup> word,<br>And the unfailing sense of being young<br>Spread out like a spring-woken tree, wherein<br>That hidden freshness sung,<br>That certainty of time laid up in store<br>As when she played them first. But, even more,</p>
<p>The glare<sup id="fnref:6"><a href="#fn:6" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label>6</span></a></sup> of that much-mentionned brilliance, love,<br>Broke out, to show<br>Its bright incipience<sup id="fnref:7"><a href="#fn:7" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="beginning to exist or to be apparent
">7</span></a></sup> sailing above,<br>Still promising to solve, and satisfy,<br>And set unchangeably in order. So<br>To pile them back, to cry,<br>Was hard, without lamely<sup id="fnref:8"><a href="#fn:8" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="in a weak and unconvincing manner">8</span></a></sup> admitting how<br>It had not done so then, and could not now.</p>
<div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">floating, drift,漂白<a href="#fnref:1" rev="footnote">↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">repair,patch,修补<a href="#fnref:2" rev="footnote">↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">和弦<a href="#fnref:3" rev="footnote">↩</a></span></li><li id="fn:4"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">4.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">（迎宾员式的）引导<a href="#fnref:4" rev="footnote">↩</a></span></li><li id="fn:5"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">5.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">连字符
[^6 ]:great brightness,耀眼的<a href="#fnref:5" rev="footnote">↩</a></span></li><li id="fn:7"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">7.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">beginning to exist or to be apparent<a href="#fnref:7" rev="footnote">↩</a></span></li><li id="fn:8"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">8.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">in a weak and unconvincing manner<a href="#fnref:8" rev="footnote">↩</a></span></li></ol></div></div>]]></content>
      <categories>
        <category>Metaphysics</category>
      </categories>
      <tags>
        <tag>Poem</tag>
        <tag>Literary criticism</tag>
        <tag>Philip Larkin</tag>
      </tags>
  </entry>
  <entry>
    <title>SPT-Code</title>
    <url>/2022/05/27/SPT-Code/</url>
    <content><![CDATA[<p>这是《Sequence-to-Sequence Pre-Training for Learning Source Code Representations》的读书笔记</p>
<span id="more"></span>



<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>Pre-trained models 用于代码相关下游任务的应用时的 问题？</p>
<ol>
<li>仅用了Pre-trained encoder 但生成任务需要两个部件都预训练</li>
<li>现在许多Pre-trained model 包括T5，只是简单复用了NL的预训练任务，这要求NL-CODE的corpus 这使得数据受限</li>
</ol>
<p>为了应对这两个问题 提出了SPT-Code ，在微调后可以在5个代码相关任务上SOTA</p>
<p>这是一个seq2seq 预训练模型，通过三个预训练任务使得其能够学习到下面三点，并在下游任务中使用</p>
<ul>
<li>代码知识</li>
<li>对应代码结构</li>
<li>自然语言描述</li>
</ul>
<p>而不需要双语corpus</p>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>作者在第一部分提了自监督训练，然后说预训练模型的成功和这个有关系，下面谈预训练模型到软工SE任务的时候认为的问题是：</p>
<ul>
<li><p>主流预训练模型仅对encoder搞 ，不够理想</p>
<p>最有名的encoder也就是BERT吧 也确实就是用了MLM（Masked Language Modeling） 也确实有好多Pre-trained Bert 下面套个任务头或者另一个decoder就开干的……不过这应该算是蛮荒时代了 T5虽然好 但是对一般研究人员来说想改进模型architecture 不是那么容易？</p>
<ul>
<li><p>别人的解决：</p>
<p>T5-learning , TreeBERT 两个工作使得Encoder-Decoder jointly trainded</p>
</li>
</ul>
</li>
<li><p>这些预训练模型预设输入的是NL-CODE 忽视了代码结构</p>
<p>为什么呢？因为就是简单偷了NLP的拿来用</p>
<ul>
<li><p>别人的解决：</p>
<p>专门的预训练任务 包括预测数据流图中的边与对齐节点和代码 </p>
<p>——dataflow 有语义信息而无语法信息（AST）</p>
</li>
</ul>
</li>
<li><p>而且都假设有严格对齐的双语语料</p>
<ul>
<li><p>T5-learning :</p>
<p>分别处理两种输入，不要求语料库中展示二者的关系</p>
</li>
</ul>
</li>
</ul>
<p>—— 没有一个模型能够统一处理这三个问题</p>
<p>SPT-Code就可以！</p>
<ul>
<li>这是一个encoder-decoder共同预训练的模型</li>
<li>数据实例由CODE,AST,NL三部分构成</li>
<li>使用方法名和调用此方法的方式作为自然语言描述（以避免对bilingual corpus的依赖）</li>
</ul>
<p>方法：</p>
<p>设计了三种预训练任务，每一种获取一种数据信息</p>
<ul>
<li>改进的MASS-用于CODE：遮蔽Seq2Seq恢复</li>
<li>Code-AST Predict CAP：预测code-AST是否匹配</li>
<li>Method Name Generation MNG：生成 方法名的 子token</li>
</ul>
<p>数据集：</p>
<p>CodeSearchNet</p>
<p>贡献：</p>
<ol>
<li>提出了SPT-Code预训练模型，可用于分类和生成任务</li>
<li>使用了线性和简化的AST 第一个使用了NL&amp;AST作为输入对于预训练</li>
<li>通过输入表示和三个与训练任务使得预训练模型不依赖双语语料库（labeled data)</li>
<li>用未标注数据库在五个下游任务实现了SOTA</li>
</ol>
<h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><p>架构，输入和预训练任务，微调</p>
<h2 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h2><p>类似于BART和T5的典型Transformer</p>
<p>分类任务和生成任务，模型采用相同的输入：</p>
<ul>
<li>分类对encoder和decoder输入相同</li>
<li>生成采用传统方法</li>
</ul>
<img src="/2022/05/27/SPT-Code/image-20220527230530594.png" alt="image-20220527230530594" style="zoom:50%;">

<h2 id="输入"><a href="#输入" class="headerlink" title="输入"></a>输入</h2><img src="/2022/05/27/SPT-Code/image-20220527231806850.png" alt="image-20220527231806850" style="zoom:50%;">

<p>由三部分组成，每一部分[SEP]连接</p>
<h3 id="Code："><a href="#Code：" class="headerlink" title="Code："></a>Code：</h3><p>没有使用笨蛋tokenizer，而是用了stl for Python 或者 antlr for Java,Php.etc 其他的用了NLTK</p>
<h3 id="AST"><a href="#AST" class="headerlink" title="AST"></a>AST</h3><p>用的Tree-sitter 搞的 AST </p>
<p> 如何序列化AST?</p>
<ul>
<li><p>传统方法：SBT （Structure-Based Traversal)</p>
<p>比先序遍历之类的更有效，但可能产生过长的序列（可能超过代码三倍长）</p>
<img src="/2022/05/27/SPT-Code/image-20220527232629535.png" alt="image-20220527232629535" style="zoom:50%;">

<center>
    一种类似中序遍历的说法 来自那篇论文忘了 反正绝对看过
</center>

</li>
<li><p>本文的方法：X-SBT：XML-like SBT</p>
<p>可以减少超过一半的长度</p>
<img src="/2022/05/27/SPT-Code/image-20220527233143119.png" alt="image-20220527233143119" style="zoom:67%;">

<p>论文自带的图好看一点 这个创新点……只能说是情理之中，毕竟原来那个也太呆了（作者甚至还装模做样证了一下必然更短）</p>
<p>为了更短: AST——XSBT时，仅取表达式级别以上节点，放弃终结符</p>
<img src="/2022/05/27/SPT-Code/image-20220527233752649.png" alt="image-20220527233752649" style="zoom:50%;">

<p><strong>这种优化为可接受的，为什么呢？下面这个说得很漂亮：</strong></p>
<p>AST中包含了语法信息和词法信息，舍弃掉终结符丢失了词法信息，但之前的token（Input中Code的部分）都是词法单元，所以这个信息是没有丢掉的，因此改进可接受</p>
</li>
</ul>
<h3 id="NL"><a href="#NL" class="headerlink" title="NL"></a>NL</h3><p>难点：从仅有CODE中提取NL</p>
<p>方法：获取方法名与调用的API序列</p>
<p>对驼峰和下划线命名掰开</p>
<p>问题：怎么提取的API序列：从AST里偷出来的？</p>
<p>——应该就是</p>
<h2 id="预训练任务"><a href="#预训练任务" class="headerlink" title="预训练任务"></a>预训练任务</h2><p>𝐼𝑛𝑝𝑢𝑡 &#x3D; 𝐶,[SEP],𝐴,[SEP],𝑁 </p>
<h3 id="Code-AST-Prediction"><a href="#Code-AST-Prediction" class="headerlink" title="Code-AST Prediction."></a>Code-AST Prediction.</h3><p>这是第一个</p>
<p>在构建输入𝐼𝑛𝑝𝑢𝑡 时，一半是对应的AST,一半是随机的AST</p>
<h3 id="MASS"><a href="#MASS" class="headerlink" title="MASS"></a>MASS</h3><p>随机遮蔽C中的一部分，将所有遮蔽的token设置为[MASK]（改进前为对应数量个[MASK])</p>
<p>根据别人的论文，最大遮蔽长度是C长度l的一半</p>
<h3 id="Method-Name-Generation"><a href="#Method-Name-Generation" class="headerlink" title="Method Name Generation"></a>Method Name Generation</h3><p>希望可以通过这个任务学到代码的动机</p>
<p>代码名的词汇和对应代码总结的词汇由高度相关，因此希望通过改善 预测代码名 这一任务提升 代码总结 的能力</p>
<p>此任务的输入时，从𝐼𝑛𝑝𝑢𝑡中的C扣掉对应token，并在N中去掉前s个token（方法名总在最前），作为输入，试图让decoder输出扣掉的前s个token，即方法名</p>
<h2 id="微调"><a href="#微调" class="headerlink" title="微调"></a>微调</h2><p>端到端 根据不同任务分成两类，分类或生成，不同任务就缺掉一点输入</p>
<h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><h2 id="预训练"><a href="#预训练" class="headerlink" title="预训练"></a>预训练</h2><p>交代了数据集的数据使用，任务顺序，epoches，用的cross-entropy loss和Adam-W，batchsize和显卡（……）</p>
<p>Tokenizer Encoding 用的BPE对CODE和NL，在预训练data上干过 每个下游任务照用</p>
<p>预训练任务的任务量都是每个任务几十个Epoches的量级。</p>
<p>问题：不是都有token了 还tokenize？</p>
<p>——低级问题，前面的应该是tokenize，这里进行token&#x3D;&gt;input_ids的步骤</p>
<h2 id="下游任务微调"><a href="#下游任务微调" class="headerlink" title="下游任务微调"></a>下游任务微调</h2><p>介绍了五个任务 其中介绍部分有点尴尬</p>
<h2 id="评估"><a href="#评估" class="headerlink" title="评估"></a>评估</h2><p>RQ1:相比于其他较好的基线 这个性能在下游任务如何？</p>
<p>列个表 比不上人家的扯一点</p>
<p>RQ2:三个预训练任务对五个下游任务分别有什么贡献？</p>
<p>——消融实验</p>
<p>有趣的是 删除掉MNG （生成 方法的token）在代码完成和代码修复上性能有所提高：</p>
<ul>
<li>MNG是目标自然语言的预训练，而这两项任务都是代码到代码</li>
</ul>
<p>分析一下 什么任务对什么下游有影响</p>
<p><strong>稍微有点水平的问题</strong></p>
<p>RQ3: 可以利用更多无标记的资料是不是本模型的优点呢？</p>
<p>相较于别的预训练模型，由于它的设计，可以使用无标注数据。更好的性能是不是来自于更多的数据呢？（而不是模型本身厉害？）</p>
<p>在同样的数据集上训练——把它当作无标注的——其实和别的比还算吃亏——也能够取得相对别的模型更好的结果。</p>
<p>可以说是赢两遍了。</p>
<p>RQ4:微调阶段的数据量对下游任务有什么影响？</p>
<p>虽然越小越坏，但是很小也和别的模型差不多 说明真好</p>
<h2 id="定性分析与定量分析"><a href="#定性分析与定量分析" class="headerlink" title="定性分析与定量分析"></a>定性分析与定量分析</h2><p>定量：志愿者评估，多个样本分类列表个</p>
<p>定性：在哪些任务哪些方面表现好 不好的怎么不好</p>
<h1 id="威胁分析"><a href="#威胁分析" class="headerlink" title="威胁分析"></a>威胁分析</h1><p>构造：数据集可能有重复</p>
<p>内部：没调过超参数：所以可能有更好的</p>
<p>外部：只用了CodesearchNet</p>
<h1 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h1><p>我们介绍了SPT-Code，这是一个基于编码器架构的源代码的大型型号。首先，我们为预训练SPT代码设计了三个特定代码的预训练任务。其次，我们提出了一种新的输入表示形式，它是第一个考虑自然语言和AST形式的方法，我们还提出了AST遍历方法的改进版本XSBT。我们的预训练任务和输入表示形式都允许在完全未标记的数据集上预先训练SPT代码。然后，对五个与代码相关的下游任务进行了微调。结果表明，微调SPT代码使其能够在五个与代码相关的下游任务上实现最新性能。消融实验表明，这三个预训练任务对不同的下游任务具有不同程度的影响，AST和自然语言输入也有助于提高SPTCODE的性能。为了促进未来的研究，我们还可以在<a href="https://github.com/">https://github.com/</a> nougatca&#x2F;spt-code上公开提供代码和其他。</p>
]]></content>
      <categories>
        <category>Substance</category>
      </categories>
      <tags>
        <tag>Note</tag>
        <tag>ICSE 2022</tag>
        <tag>Code generation</tag>
      </tags>
  </entry>
  <entry>
    <title>「政治的人生」读后感</title>
    <url>/2022/07/11/%E3%80%8C%E6%94%BF%E6%B2%BB%E7%9A%84%E4%BA%BA%E7%94%9F%E3%80%8D%E8%AF%BB%E5%90%8E%E6%84%9F/</url>
    <content><![CDATA[<p>本篇为王沪宁所著「政治的人生」读后感</p>
<span id="more"></span>

<p>这是第一篇利用零碎时间读完的书 不过能读完还是拖了看见大段念经就不少跳过的福……写这篇读后感的时候正在等头发干 不然早睡觉去了。欧阳公读书有「三上」之说，我又何妨写字有「三干」一谓？至于后两干，慢慢再补起来。</p>
<p>如果想写成一篇质量又高，读完陌生者又有收获的读后感，大概读的时候就得做上不少笔记，便签；写的时候还得布局谋篇，不时再去重新寻章摘句。然而 读的时候就没有字字珠玑，写的时候便更要信马由缰了。</p>
<p>开头的自序之中 他说他喜欢在夜深人静的时候静静的独自思考思考，「是一种思想享受」，也是「一点新的空间」。他说，整天搞政治学研究之外，还需要有一点自己的空余思考 这种思考于个人于事业都是有益的。这种说法听起来非常像孟子的「养夜气」之说。</p>
<p>什么是夜气呢？这是孟子对于自己善性论的 某种程度上的一个补丁。性善，意即人人本质之中都具有「善」的种子 具有感受善，追求善进而将其发扬光大的能力。既然人人都有皆有善端，那么为什么世界上有那么多的不善之人呢？这就是和后天的涵养功夫有关。如同牛山上的树林一样，天生繁茂，却因为毗邻大国，而被砍成了光秃秃的样子，难道光秃秃就是山的本貌吗？人也是这般，虽有善端，然而放逐了良心，也就失去了他。黎明时呼吸的清气，他与常人好恶相近，然而白天的所作所为又将其丢失了。夜气清明，然而却没有将其涵养，别人看他也就成了禽兽，然而这禽兽却也并不是他的本质。</p>
<p>其实在这段阐述之中，孟子引入了许多模糊而重要的假设，甚至摸到了一些更重要的命题 不知道为什么没有进行更深入的讨论。为什么看起来是个坏蛋，夜风一吹善端又起呢？这个「夜气」究竟是什么东西呢？是某种外于人的东西给予了人善念？这种说法大概可以排除，不仅与学说不符，还沾点自然神信仰。如果说是社会活动，那么，由人组成的社会又何以如同斧斤伤人善念呢？是因为王道不昭善政不行？（笑）一个人既然有善端 为什么在他经过理性的自由无意识活动的结果反而是将其损害呢？难道说，孟子的意思是，人之善并不为零，这样的悲观语调吗？与本篇过远 此处按下不表。</p>
<blockquote>
<p>操则存，舍则亡；出入无时，莫知其乡</p>
</blockquote>
<p>如果果然按照孟子的说法，白天人在忙于各种社会性事务或者是解决生存问题，在晚上独处的时候才会升腾起清气反观自身的话，那么王教授怎么深更半夜写日记都有超过百分之六十的篇幅在讨论相当专业的事情呢？（看到王沪宁甚至在日记里还是XX性XX性成片的排比念经我的心中甚至惊叹盖过了崩溃）虽说，看到他从私事，杂感到「念经」转换的相当自然，我更倾向于这是他生活习惯的一种继续。不过，王沪宁自己也说，期望在这里得到一点闲暇的，个人的时间，从而得到某种启发。那么，这些内容其实已经是他日常以外的第二视角审视了？其实，他要是能稍稍强迫自己不在日记里还是大段地讨论那些「抽象」问题，恐怕还能有更多的收获。就像他自己在日记中举得例子：</p>
<blockquote>
<p>忽然想到一篇法国小说《月光》，说的是一位神 父从来就不能理解他的侄女的行为，也不理解年轻 人为什么要谈情说爱，而不全心全意地把自己奉献 给上帝。一天夜里，他发现侄女又在与一位小伙子相 会，顿时大怒，从教堂里冲出来。突然，他停下了脚 步，他发现月光是那样皎洁，那样美妙，他心中产生 了一个问题“：如果年轻人们干的事情是罪恶，是上帝不允许的，为什么今夜的月光会这样的明亮？为什 么上帝会在今天晚上带来这样通明的月光呢？”于是，他意识到那不是罪恶，而是一种美好。他就回去，安静地入睡了。</p>
</blockquote>
<p>他自己将其归结于「自然」的教育。其实，哪一天晚上没有月光？哪一处又没有幽会的情侣？然而，正是心头偶然闪过的一丝明悟，才能提出那一问题而不是执迷不悟。善端的昭显，长久的涵养与天赐的一点灵光，缺一不可。没有善端，感受不到美，千万条路同归死路；没有长久之涵养，大概会「宁可丢掉百体中的一体，不叫全身下火狱」（逃）；而没有灵光一闪，壁垒就永远无法被打破。</p>
<p>——add 2022.8.8——</p>
<p>中间过的时间实在是太长了 当时想要写的东西许多怕是都想不起来了：那些草蛇灰线，仿佛朝阳斜晖下才能嗅到的隐秘香气……不过照着原来有深刻印象的几个片段继续写还是没问题的。</p>
<p>读着他的书，心里却想着孔子。果然孔子是任何年代一个稍有知识的中国人在试图进行任意主题的深入思考都无法绕过去的主题。看他写孔子：</p>
<blockquote>
<p>对孔子的思想，青年人不太理解，不知道在中国这样一种文化氛围内，儒家的思想有它存在的理由，不是人为力量可以清除。他不仅是一种思想体系，也是一种生活方式的反映。在中国社会中，人们一直强调从人的内心世界来协调人们之间的关系，而不是从外部世界。所有的秩序，政治的也好，经济的也好，社会的也好，最终是一种精神上的秩序。在一种精神上的秩序没有形成之前，任何外在的秩序均不牢固。这就是儒家中最有价值的部分。一个世界不能没有秩序，也不能没有伦理秩序，儒家比较强调着一点。问题不再用什么样的内容来构筑秩序，而在于需要秩序本身。</p>
</blockquote>
<p>他写文章在1994年，那一年中国社会的思想风潮大概是什么样子呢？想了想根本没本事回答这个问题(笑)。从建国后的尊法批孔，到现在的”儒学复兴“，中间如果有一个明确的节点，大概是在什么时候呢？改革开放之后到八十年代末大概不会是答案。</p>
<p>读孔子，王读出来的是秩序：从内心世界来协调人的关系，进而构建出社会的伦理秩序——甚至对秩序的内容也可以不那样在乎，而要强调秩序本身。那么，儒家的秩序又是什么样的呢？也没本事回答这个问题）这里先不说老愤青引用的那个</p>
<blockquote>
<p>人有十等。下所以事上，上所以共神也。故王臣公，公臣大夫，大夫臣士，士臣皁，皁臣舆，舆臣隶，隶臣僚，僚臣仆，仆臣台 </p>
</blockquote>
<p>在我的了解中，儒家秩序的构建核心纽带之一是「亲亲」，有了亲亲下来才有孝悌，长幼尊卑。儒家重视人伦关系，将夫妻视为最重要的社会关系（可能没有之一？）以「礼」作为方式，来维护人伦秩序。礼记有云，「昏礼者，礼之本也」。下来才是如同同心圆一样向外放射进行家国同构。</p>
<p>那么 强调秩序，王心中的秩序是什么样子的呢？作为坚挺三届而不衰的笔杆子，这个倒是不难猜测。此外，他还引用过《停滞的帝国》对乾隆的评论，说他一元体制是维护封建政权的唯一把手，所以牢牢把握不松开，也造成了国家的封闭。对于这样的观点，王没有评论。</p>
<p>王读孔子，读出的是秩序与稳定；然而有人读孔子，却能读出人的尊严与理性。相比较于孔子的润物无声，其实儒家里面孟子是相当能打动人的。如果说「富贵不能淫贫贱不能移威武不能屈」还能从孔子哪里找到「富與貴是人之所欲也，<em>不以其道得之</em>，不處也；貧與賤是人之所惡也，<em>不以其道得之</em>，不去也。」的引证，那么，像「自反而缩，虽千万人吾往矣」这般的浩然正气就几乎让君子成了庄子笔下的冰肌玉肤，餐风饮露，去以六月息的神人。然而潇洒之下的隐忧，却只有引入了现代性的视角才能看得到：论语说，「邦有道，危言危行；邦无道，危行言孙」。养浩然正气，精神上无比自由强大的君子，在无道之国之中却连话也不能说，无比自由的人却可以无比压抑。前一阵子看克里希纳木提的书，就记住他说的一个”人在青年时候最需要的是免于恐惧。“其实，孔子的「仁」也绝不是一门压抑生命力的学说：「麻木不仁」者，无知觉也。然而，就是这样一门崇尚生命发展的理念，却同样无法远离无道之国的恐惧。</p>
<p>来到孟子，这样样一个强调现实关怀与此岸生活的人，在被问及禅让之时，也只能有如下的对话：</p>
<blockquote>
<p>：尧把天下禅让给舜，有这样的事吗？</p>
<p>不行，天子不能把天下给别人。</p>
<p>：那舜怎么有的天下呢？</p>
<p>是天给的</p>
<p>：天又不会说话，怎么给呢？</p>
<p>天通过事情展示出来的。尧把舜考察了二十八年，不是人能做到的，是天。后面尧死了，舜避居多年，人们朝见，诉讼，讴歌，都不去找尧的儿子而去找舜，所以说是天啊。不管将天下给了贤人，还是儿子，都是天的事情。后面的「唐、虞禅，夏后、殷、周继，其义一也。」都是一回事。</p>
</blockquote>
<p>虽说「天视自我民视，天听自我民听」，可只要稍有常识的人就能看出这是在扯出车轱辘话来嗯说，实在是丢人。 「只有人民才能做出选择」可是既没有制度化的保障也没有法治精神，不是一样成了橡皮图章吗？由此，张千帆顺水推舟地论证了宪政的必要性：人民主权，法的精神：人权，制约，法治。令人感慨。</p>
<p>写得时候查资料，果然又查到孟子不支持燕国禅让的黑历史，在这篇文章《先秦儒家禅让思想的演变-孔锐之》（这个名字听起来像极了人民日报风格的笔名）中，还亲切地将孟子称为是”<strong>儒家的保守派</strong>“，真是令人忍俊不禁。不过，这篇文章后面对燕国禅让的评论引起了我的注意：</p>
<blockquote>
<p>　燕国的这次动乱充分说明，在战国时代，通过禅让实现政权交接是不可取的，这样做非但不能实现尊贤、举贤的目的，反而会为权臣篡国铺垫道路。这次禅让实践的失败，给人以很多启示，即便是原先积极主张推行唐虞之道的学者，现在恐怕也不得不重新考虑其现实可行性问题，继而会转向温和的或保守的主张。在这之后，很难再有人大力鼓吹“不禅而能化民者，自生民未之有也”的观点了。</p>
</blockquote>
<p>就事论事，这种论点在文章当中并不算突兀，但如果真的是这样分析问题，还是趁早上观察者网开个号得了。如果用同样的角度生活在十八世纪中叶，大概也会得出：</p>
<blockquote>
<p>在当前这个时代，资产阶级革命以建立共和国的方式是不可取的……法国近五十年的动荡，从波旁王室退位后，雅各宾派杀得人头滚滚；拿破仑从科西嘉到圣赫勒拿岛，流血漂橹；法兰西如同钟摆在两端摇晃，民主共和只会是一团泡影——瞧，现在工人又起来要胡闹了。</p>
</blockquote>
<p>长达近百年的动荡的确让人失去信心，可是，历史终于证明了共和国的荣耀：从大革命到第三共和国，波旁与拿破仑终于退出了巴黎，法国再也不需要皇帝或者第一执政官了；从第三共和国到五月风暴，法国与他的人民在一点一点缓慢却坚定地向前行进。我们常常面对物理学或数学的大厦已经建成，而望洋兴叹，感慨生不逢时。如此看来，无须妄自菲薄或垂头丧气，正得其时。</p>
<p>……扯得太远了，回来回来。</p>
<p>总之，从对儒家的看法来解读王，那么就不难得出上述的结论。如此，他的形象大概也慢慢清晰起来：一个以传统中式家长为底色，有着出色的视野，理解能力，和dedication（热忱？）的干练人物。上一句话的最后中心语没有使用”官僚“或者”知识分子“，是因为这样的标签都与他本身相去甚远。或许用他自己的”政治家“来定义都更合适不少。（虽然他此刻并没有任何官职<em>无来源</em>，但毫不突兀）。他有知识分子的一面，对于艺术的解读能力十分敏锐，还有相当灵的生命意识——对此他有几片非常精彩的散文片段。在规划学院与日常生活中，也能看出几分官僚色彩（不是贬义）——我头一回见到有人说去”个体户“那里吃饭。然而令人某种程度上十分诧异的是，他的家庭生活似乎并不很丰富，直到成文时的96年，没有生育，对他的影响从文中也是一片空白。入阁后虽有再婚，那就不知道了。</p>
<p>王对自己的评价十分精确：</p>
<blockquote>
<p>生活在这个世界上的人：有的是弱者；有的是强者；有的要别人来设定目标，有的给别人设定目标；有的需要感情来支持生活，有的需要意志来支持生活。我大概在每一对概念中都会选择做后一种人。</p>
</blockquote>
<p>我完全同意他三个后者的说法，事实上，看他的照片某种程度上算是一种享受：消瘦的身形，锐利的眼神，完全就是一个现实中的”孙悟空“式的形象，喜欢极了。看他书中提到的加强党的权威，以人为本等思想从入阁后各个时代的指导方针中流露，似乎并没有丝毫动摇。不由得想起潜伏中的那一句台词：</p>
<blockquote>
<p>随风摇摆是一种优秀的能力，但是从一而终也是一种可贵的品质。后者或许会招致毁灭，却也可能诞生奇迹。</p>
</blockquote>
<p>我衷心祝愿他能够同样在这一对概念中迎来后者。</p>
<p>书中还有一处颇为有趣：</p>
<p>他提到某院长从日本访问回来，已经信息到了“信息高速公路”：到那时，人们将在家办公，在家学习，在家购物和看病：药品和X光机都能送上门来。</p>
<p>他说，这将改变人们的生活方式，改变社会的管理方式和组织方式：如果大部分办公楼，报纸，大学和商场都不需要了，那么就业结构会怎么样呢？意识形态怎么处理呢？政府又应当如何管理呢？这些都应该有一个设想。</p>
<p>看到前面的时候，我还不禁感慨，真牛啊，那会不光都能为”写了稿子电脑中病毒而哀嚎“（前文），甚至都能意识到互联网对社会的巨大改变——看到最后一句却不禁哑然失笑。</p>
<p>想起来十九世纪的人们为都住在大楼里的生活绘制美好蓝图：由于上下楼不便，每栋楼里都会有超市，健身房和咖啡屋，大楼之间还可能建起桥梁……可是电梯的发明却让这一切”合理的“设想都破灭了。是啊，就像犀牛看到的世界是以自己的独角为底色一样，王的眼睛里永远是有政府的：他的存在先于一切又贯彻一切。人们不用外出，甚至不用工作：经济平衡的主要矛盾早已由生产的不足成为了需求的不足，马克思能想到劳动都要被消灭了？（对于大多人）如此，就不禁令人感慨。他自己也说：</p>
<blockquote>
<p>生活在今天的人，常常说前人无能，把中国搞成这样，我们希望以后的中国人不会说今天的中国人无能，把中国搞成这样。这是我最大的愿望。</p>
</blockquote>
<p>中国的知识分子，三四千年来在儒家的框框里咚咚打转，能想到的也无非是圣君贤相，勤政爱民；东学西渐，梵学给文化艺术哲学都注入了崭新的活力，却唯独与几政治哲学无动于衷。晚清的知识分子出去绕了一圈，才把大腿都拍麻了：天下为公三代之治怎么在蛮夷那边实现了。所以，自以为是一切之先决者，最容易是一切的漏洞。也正是同样的道理：最谦虚，最自满。</p>
<p>想想前一阵子看禅宗公案，有一个有文化的和尚写了这么一句，相当漂亮：</p>
<blockquote>
<p>万古长空，一朝风月。</p>
</blockquote>
<p>是啊，一朝风月，万古长空。</p>
<p>2022&#x2F;8&#x2F;8</p>
<p>PS：写完之后意识到的自己几乎没有了解的课题：</p>
<ul>
<li>辛亥以后的儒家社会地位沉浮</li>
<li>法国历史</li>
<li>古代中国的外来思想文化影响，尤其是对政治方面</li>
<li>经济学完全是0</li>
</ul>
]]></content>
      <categories>
        <category>Metaphysics</category>
      </categories>
      <tags>
        <tag>Reading</tag>
        <tag>Politics</tag>
      </tags>
  </entry>
  <entry>
    <title>Follow Ace Taffy Meow！</title>
    <url>/2022/05/23/testpic/</url>
    <content><![CDATA[<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Follow Ace Taffy thanks Meow!&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2022/05/23/testpic/image-20220524021029166.png" alt="image-20220524021029166"></p>
]]></content>
      <categories>
        <category>Meow</category>
      </categories>
      <tags>
        <tag>Ace</tag>
        <tag>Taffy</tag>
      </tags>
  </entry>
  <entry>
    <title>杂谈-别人的书</title>
    <url>/2022/09/17/%E6%9D%82%E8%B0%88-%E5%88%AB%E4%BA%BA%E7%9A%84%E4%B9%A6/</url>
    <content><![CDATA[<p>昨天（其实是今天……）说要和某人互换藏书 浏览其自己的书架才发现好多好多别人的书。细看之下想到好多，不过更多还是没读过的XD。下面一本一本说吧。</p>
<h1 id="Ursula-lee"><a href="#Ursula-lee" class="headerlink" title="Ursula lee"></a>Ursula lee</h1><p>《少有人走的路》 来自 2015年。刚刚翻开扉页，写着，「爱不是感觉 爱是实际行动 是真正的付出」很巧的是 这也就是他现在留给我的几乎全部印象。 记得前几天还是最近一次还和别人提过这本书 印象之中是「有点水平和启发的鸡汤」。事实上这也应该算我对「爱」的启蒙之作？有了这个，才有了后面的「爱不是交易或因信称义」，后面黑夜中偶遇墙上的《哥林多前书》一般的「龙场悟道」……有趣的是，这本书我并没有看完。书的第一章并不是在讲什么爱的艺术，而是我没有任何印象的自律章节。</p>
<p>作为回赠 我送给了他新的《旅行的艺术》，因为听说他很爱旅行，这位作者的下一本书是我在九年后才读到的《拥抱似水年华》可能是没有读过普鲁斯特 反而觉得像是那种诙谐的小报作家了。不过在上一本书的扉页上 我还是很认真地写下了</p>
<blockquote>
<p>天之苍苍 其正色邪？其远而无所至极邪？</p>
</blockquote>
<p>坏了 写到这才发现这个话题不兴说……好多烂事要牵扯上来……先睡觉得了</p>
]]></content>
      <categories>
        <category>Metaphysics</category>
      </categories>
  </entry>
  <entry>
    <title>火柴</title>
    <url>/2022/05/28/%E7%81%AB%E6%9F%B4/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>我在<sup id="fnref:6"><a href="#fn:6" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="火柴梗快要烧完的时候，会因为火焰过分接近而忍不住松手">6</span></a></sup>水中桥下<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="「尾生與女子期於梁下，女子不來，水至不去，抱樑柱而死。」《莊子·盜跖》
">1</span></a></sup> 饮酒<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label=" 「夢飲酒者，旦而哭泣；夢哭泣者，旦而田獵。方其夢也，不知其夢也。」《莊子·齊物論》
">2</span></a></sup>&#x2F;<del>忘相泉涸<sup id="fnref:4"><a href="#fn:4" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="「泉涸，魚相與處於陸，相呴以溼，相濡以沫，不如相忘於江湖。」《莊子·大宗師》">4</span></a></sup>前日的红烛<sup id="fnref:5"><a href="#fn:5" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="「蠟炬成灰淚始幹」 李商隐
">5</span></a></sup>泪眼<sup id="fnref:3"><a href="#fn:3" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="「文侯與虞人期獵。是日，飲酒樂，天雨。文侯將出，左右曰：「今日飲酒樂，天又雨，公將焉之？」文侯曰：「吾與虞人期獵，雖樂，豈可不一會期哉！」乃往，身自罷之。」《戰國策·魏策》
">3</span></a></sup></del>红烛淡忘镜中的泪眼</p>
<p>自己写完还解诗的人肯定是天下第一无聊 但这只不过是记录灵感媾和时的一些脉络和取舍。</p>
<span id="more"></span>


<hr>
<p>2022&#x2F;5&#x2F;30日凌晨睡不着床上对后半句做了修正</p>
<p>第二天起来看前天写得也太迫真了 简直就是笨蛋版本李商隐</p>
<p>当时是怎么想的呢？「忘相」是什么表达？生怕别人看不出来是你直接从庄子里偷来的？「泉涸」同理，太白太直了 反而失去了解读空间。「前日」也不明所以的。都要删掉。「红烛泪眼」是核心意象要留下来。</p>
<p>想要留下来的是什么呢？遗忘肯定是要有的，这是我给出的解答。互相也是要有的，起源就是由防风火柴想到的尾生抱柱抛出的一问。「红烛泪眼」没法共轭，还是拆开好了。</p>
<p>互相的话 就用镜子好了。破妄，吊诡，空间的拓展却又重复，自反中带有异质性。很好。</p>
<p>「红烛在镜中总是泪眼」？当时还开心的把手机翻出来赶紧记下，记完想想又觉得不好 啰啰嗦嗦的。</p>
<p>「红烛向镜中抛去泪眼」？我很喜欢这个动作带有的力量感，和伴随而来的主体性。用什么迎接你？以眼泪，以沉默。这是有力量的沉默。但是汉语还是半通不通的 忘记也没了。不好。</p>
<p>「忘记」这个字其实很好，自反又偏义，但是口语中用得太多了 读来感觉不到妙处 不好。</p>
<p>我想，烛火燃烧时上腾的青烟，蒸腾的雾气凝结成雨，落下化为沙尘，恰好就有一种复调式的演出效果。水汽也好，腾烟也好，怎么放在这里处理”忘“这个要素呢？想到了溶解，融化，但都用不好。这里卡了很久没想出来。</p>
<p>灵光一闪，就用「淡忘」。「淡」字自己就好像是拿来给水墨化开的，要是到token级别就是又有水又有火的自反，不管是前句的湖中还是镜子都能超距作用。「淡忘」本来不是什么僻词，但放在这里就妙得没话说。</p>
<p>「红烛淡忘镜中的泪眼」，真好。</p>
<p>「我」「饮酒」，「烛」「忘眼」。好像比兴一样的氛围，又构成了复调的演奏。「水」与「镜」，「泪」与「眼」，几乎每一个元素都能够进行笛卡尔式呼应。比兴之中，阅读顺序的先后带来的时序性还为文本增添了并列以外的递进因素，自问自答。很好。我很喜欢。</p>
<p>如果说昨天是向义山一样堆叠典故，这次就是处理意象了，也是很好玩呐。</p>
<p>下面又试着加点东西。一方面是平衡语感。这两句佶屈聱牙，像极了祭祀用的七言律诗，但是要是能像冯君一样，神神叨叨念完“长剑归来乎，”，令人不容小觑，立马接上一句“食无鱼”产生节目效果。那就可以说是非常成功了。</p>
<p>此外，也和我想要追求的吊诡氛围有点差异。水中饮酒还有镜子，好像是月光下的水晶湖一样，太明亮通透了些&#x2F;</p>
<p>加什么呢？四个字的好。「烟波浩渺」？我想到洞庭湖，蹭蹭湘君的隐喻正好在调上。「烟涛微茫」？直接偷过来好像也不坏。像是舞台布景的话，很明白的小舞台放在巨大的烟幕里，也是那个意思。</p>
<p>不过怎么放怎么感觉不妙，况且下来也不知怎么接手。先这么放着好了。<div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">「尾生與女子期於梁下，女子不來，水至不去，抱樑柱而死。」《莊子·盜跖》<a href="#fnref:1" rev="footnote">↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">「夢飲酒者，旦而哭泣；夢哭泣者，旦而田獵。方其夢也，不知其夢也。」《莊子·齊物論》<a href="#fnref:2" rev="footnote">↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">「文侯與虞人期獵。是日，飲酒樂，天雨。文侯將出，左右曰：「今日飲酒樂，天又雨，公將焉之？」文侯曰：「吾與虞人期獵，雖樂，豈可不一會期哉！」乃往，身自罷之。」《戰國策·魏策》<a href="#fnref:3" rev="footnote">↩</a></span></li><li id="fn:4"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">4.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">「泉涸，魚相與處於陸，相呴以溼，相濡以沫，不如相忘於江湖。」《莊子·大宗師》<a href="#fnref:4" rev="footnote">↩</a></span></li><li id="fn:5"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">5.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">「蠟炬成灰淚始幹」 李商隐<a href="#fnref:5" rev="footnote">↩</a></span></li><li id="fn:6"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">6.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">火柴梗快要烧完的时候，会因为火焰过分接近而忍不住松手<a href="#fnref:6" rev="footnote">↩</a></span></li></ol></div></div></p>
]]></content>
      <categories>
        <category>Metaphysics</category>
      </categories>
      <tags>
        <tag>Poem</tag>
        <tag>Self</tag>
      </tags>
  </entry>
  <entry>
    <title>论文笔记 AST-Trans ICSE`22</title>
    <url>/2022/07/03/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-AST-Trans-ICSE-22/</url>
    <content><![CDATA[<p>这是《AST-Trans: Code Summarization with Efficient Tree-Structured Attention》的读书笔记</p>
<span id="more"></span>

<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>代码总结是干什么<br>最先进的方法用了E-D架构<br>源代码的特征之一是AST，常用于编码信息<br>AST太长<br>现在的方法忽视了大小限制 直接就塞进去序列化的AST<br>我们认为其问题在于难以提取信息，计算成本大</p>
<p>为了更好地编码AST 提出了AST-Trans，利用了两种节点关系——上下和左右<br>使用了树状注意力动态分配权重给相关的节点<br>还还提出了一种<strong>支持</strong>高效为树状注意力并行运算的实现</p>
<p>在两个相关数据集上，超过了SOTA</p>
<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p> 程序总结分为多个模块 本作聚焦于子例程或方法<br> 短描述可以让开发者快速理解 但是 好的总结需要大量劳动——错配 丢失 过时——自动方法可以避免<br> 传统方法：</p>
<ul>
<li>手工规则来——命名太烂就不行了</li>
<li>信息检索技术：没见过就不行了</li>
</ul>
<p>最近：</p>
<p>开源代码库让数据驱动的神经网络引起了更多注意</p>
<p> 代码需要AST-AST序列化-简单介绍几个方法</p>
<hr>
<p>在introduction就介绍了许多现有工作 更具体地说是提到啥就引用</p>
<hr>
<p>问题：L-AST太长了 </p>
<p>仔细解释</p>
<p>介绍我们的工作</p>
<ul>
<li><p>假设：AST中的节点受影响最大的是：</p>
<ul>
<li>祖先-子孙:不同区块的等级关系</li>
<li>兄弟：时序关系</li>
</ul>
<p>—— 画了个图作证 </p>
<p>捕获这两种关系就可以了 不用用全部注意力为所有节点建模</p>
</li>
</ul>
<p>			</p>
<p>​	提出了 这个东西 是一个Transformer的简单变体来 处理树状AST</p>
<pre><code> 用了 这两种关系的矩阵来代表树结构 然后用了这个矩阵来动态排除不同注意力层中的无关节点
</code></pre>
<p>​	绝对位置嵌入也被换成了由两种关系矩阵的相对位置嵌入</p>
<p>还进一步描述了实现与计算分析</p>
<p>贡献：</p>
<ul>
<li>可以用线性复杂度来编码唱AST 和传统二次复杂度的Transfermer不一样</li>
<li>深入分析：复杂度，经验证据等</li>
<li>在两个数据集上显示大幅度SOTA</li>
<li>比较了多种AST的编码方式并讨论</li>
</ul>
<p>2 Background——AST Transformer</p>
<p>3 elaborates 实现细节</p>
<p>4 不同的实现</p>
<p>5 分析复杂度</p>
<p>6 解释实验步骤 分析结果</p>
<p>7 TtV</p>
<p>8 RW</p>
<p>9 Conclusion</p>
<h1 id="BACKGROUND"><a href="#BACKGROUND" class="headerlink" title="BACKGROUND"></a>BACKGROUND</h1><h2 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h2><p>最开始被提出来做机器翻译 使用了多头栈状encoder-decocer层</p>
<p>介绍了一下普通Transformer的层，可以并行</p>
<h2 id="AST"><a href="#AST" class="headerlink" title="AST"></a>AST</h2><p>真的就是简单介绍 几乎没有引用</p>
<h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><p>先概述一下流程 然后分步骤介绍</p>
<h2 id="序列化"><a href="#序列化" class="headerlink" title="序列化"></a>序列化</h2><p>简单介绍了三种序列化的方法，最后说用了改进的先序遍历序列法达成了SOTA 反而结构化的效果不好 路径分解效果更烂</p>
<ol>
<li>POT 先序遍历序列化：简单的按照先序遍历把节点列出来 坏处在于有损 因为无法靠此重建</li>
<li>SBT 结构序列化 ：</li>
<li>PD 路径分解 ：随机展示出一条某节点到某节点的路径 因为会有超多条 所以需要随机采样</li>
</ol>
<p>前两个都能直接放 第三个不好搞在这个任务</p>
<hr>
<p>问题在于他实际使用的 改进的POT也没介绍啊</p>
<h2 id="关系矩阵"><a href="#关系矩阵" class="headerlink" title="关系矩阵"></a>关系矩阵</h2><p>引入了祖先矩阵和兄弟矩阵两个东西来对节点的关系建模</p>
<p>如果是该关系 则值为有向距离，反之为正无穷</p>
<p>Aij&#x3D;-Aji</p>
<p>还介绍了一个预定义阈值P 有向距离大于P则变成正无穷 没搞懂在干什么：</p>
<p>十代祖先就不算了 设置了一定的视野范围</p>
<p>说是要用两个矩阵来动态改善树状的注意力分配</p>
<h2 id="树状注意力"><a href="#树状注意力" class="headerlink" title="树状注意力"></a>树状注意力</h2><p>介绍了自注意力 相对位置嵌入 	注意力解耦 这三个看起来都是别人的东西 基本跟他没什么关系 有点像Background</p>
<p>然后介绍了树状关系注意力，</p>
<p>用了前面的关系矩阵来代替了相对位置嵌入的一个距离 再经过一些处理 在对新关系建模的同时还能达到更低的复杂度效果。</p>
<h1 id="高效实现"><a href="#高效实现" class="headerlink" title="高效实现"></a>高效实现</h1><p>传统的Transformer 计算复杂度会随着序列长度二次增长 但改良的ASTTrans 只需要对部分节点对计算：某些R距离大于零的 然后逐个分析实验方法：掩码 循环跳过 稀疏向量什么的</p>
<hr>
<p>可以对数据集来个量化的分析</p>
<h1 id="复杂度分析"><a href="#复杂度分析" class="headerlink" title="复杂度分析"></a>复杂度分析</h1><h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><h2 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h2><p>数据集介绍，处理策略</p>
<p>预处理</p>
<p>超参数</p>
<p>评测方式</p>
<h2 id="介绍比较的基线"><a href="#介绍比较的基线" class="headerlink" title="介绍比较的基线"></a>介绍比较的基线</h2><p>根据输入的不同分类介绍</p>
<p>每个模型逐个介绍</p>
<p>输入代码：</p>
<p>输入AST树：有树专用的encoder 用Tree-LSTM或者GNN——想知道是怎么输入的 或者说……encoder是怎么接受这种东西的</p>
<p>输入PD AST：作者甚至还改进了他的模型作为基线：把模型中的LSTM换成Transformer</p>
<p>输入SBT AST </p>
<p>输入POT AST:设计了对照组是接收同样输入的Transformer</p>
<h2 id="主要结果"><a href="#主要结果" class="headerlink" title="主要结果"></a>主要结果</h2><p>也就是分析</p>
<p>比较：</p>
<ul>
<li><p>代码-树状AST-序列化AST</p>
<p>发现树状AST效果最好 结果树状的效果最好 作者的分析是这个信息最完整最多 所以效果最好</p>
<p>还分析了在不同数据集上的影响 觉得是长度的原因</p>
</li>
<li><p>三种序列化AST的比较</p>
<p>SBT在JAVA效果最好 POT在python上效果最好 </p>
<p>SBT 信息多 POT最短</p>
<p>PD效果最烂</p>
</li>
<li><p>关系矩阵的影响</p>
<p>加上之后改善了所有模型的性能</p>
</li>
<li><p>AST-Trans vs GNN</p>
<p>和其他里面最强的那个比一比</p>
<p>分析了为啥会更好一点</p>
</li>
</ul>
<h2 id="消融实验"><a href="#消融实验" class="headerlink" title="消融实验"></a>消融实验</h2><p>消融对象：四种</p>
<ul>
<li><p>仅使用一种对象矩阵：也会造成提升 但是不如都来</p>
</li>
<li><p>注意力头的数量 8个头几个看祖孙 几个看兄弟：更接近一种超参数实验</p>
</li>
<li><p>观察多远的范围来确定祖先-兄弟关系？：</p>
<p>视野越大效果越好，就算一点也能改善很多，具有边际递减</p>
</li>
<li><p>网络层数：越深越好</p>
</li>
</ul>
<h2 id="视觉分析和定量分析"><a href="#视觉分析和定量分析" class="headerlink" title="视觉分析和定量分析"></a>视觉分析和定量分析</h2><h1 id="有效性威胁"><a href="#有效性威胁" class="headerlink" title="有效性威胁"></a>有效性威胁</h1><ul>
<li>选取的公开数据集可能代表性不足</li>
<li>作为基线的其他架构选择和超参数选择可能不是最优</li>
<li>自动评估和手动评估的代表性可能不强</li>
</ul>
<hr>
<p>全都是实验设置 做实验的问题 没提方法本身的问题</p>
<h1 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h1><h2 id="代码总结"><a href="#代码总结" class="headerlink" title="代码总结"></a>代码总结</h2><p>大多数人都把他当seq2seq任务，用Transformer</p>
<p>和传统翻译的唯一区别就是他的输入是无歧义的代码，有语法规则，普遍的方法都是把他当普通文本序列或者结构化序列</p>
<p>介绍了一下处理的方法，以罗列为主</p>
<p>简单比较了一下自己的方法</p>
<h2 id="基于树的神经网络"><a href="#基于树的神经网络" class="headerlink" title="基于树的神经网络"></a>基于树的神经网络</h2><p>现有的树状神经网络可以按照输入分为两种</p>
<p>解析法和采样法</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>我们<strong>通过</strong>高效编码AST<strong>实现</strong>了代码总结：</p>
<p>介绍一下方法，以及带来的好处</p>
<p>只需要关注有关系的节点</p>
<p>可以不被过长的AST困惑</p>
<p>降低复杂度</p>
<p>画画饼：让他可以处理长代码，甚至一个文件</p>
<p>做了实验，比较</p>
<p>我们相信这样的基础理念可以用在别的地方</p>
<p>计划更多特征加入，如API序列或节点类型，来改善注意力机制</p>
]]></content>
      <categories>
        <category>Substance</category>
      </categories>
      <tags>
        <tag>Note</tag>
        <tag>ICSE 2022</tag>
        <tag>Code generation</tag>
      </tags>
  </entry>
  <entry>
    <title>论文笔记 Automated Generation of Constraints From Use Case Specifications to Support System Testing</title>
    <url>/2022/06/20/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Automated-Generation-of-Constraints-from-Use-Case-Specifications-to-Support-System-Testing/</url>
    <content><![CDATA[<p>后记：</p>
<p>这篇文章真没搞懂 看的时候觉得实在有点车轱辘话嗯说 前面说UMTG只要RUCM和OCL 后面说OCLgen的输入还需要类图……</p>
<p>前面说verbnet里也不全是同义词 后面又说都是同义词</p>
<p>前面说这个可以转换所有语言 后面就是简单推测了一下用较少规则可以转换好多 而现在还只实现了较少的较少……</p>
<p>当然 也可能是我水平还不够看得颠三倒四 总之 就只是把他非常「作为手段地」分析了一通</p>
<span id="more"></span>

<ul>
<li><p>这篇文章要解决什么问题？</p>
<ul>
<li>需要从自然语言的需求规范中自动生成可执行测试用例，使用UMTG工具</li>
<li>UMTG需要的东西：RUCM的自然语言规范和OCL写的约束</li>
<li>OCLgen就要生成OCL约束——主要是前后置条件</li>
</ul>
</li>
<li><p>OCLgen的输入输出是什么？</p>
<ul>
<li>输入：UMTG一致(?)——RUCM的NL和UML类图的系统领域模型</li>
<li>输出：每一个用例步骤对应的OCL约束</li>
</ul>
</li>
</ul>
<p><img src="/2022/06/20/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Automated-Generation-of-Constraints-from-Use-Case-Specifications-to-Support-System-Testing/image-20220628194141482.png" alt="image-20220628194141482"></p>
<img src="/2022/06/20/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Automated-Generation-of-Constraints-from-Use-Case-Specifications-to-Support-System-Testing/image-20220628194155803.png" alt="image-20220628194155803" style="zoom: 33%;">

<ul>
<li><p>它厉害在哪里？</p>
<ul>
<li>别人的方法 要使用CNL（受控的自然语言）来写软件要求，即，采用有限的动词</li>
<li>他的方法——需要UMTG格式的语言——但不限制动词，还需要类图</li>
<li>在测试的工业案例中，可以达到75%的正确</li>
<li>使用了语义标注和同义词合并的方法，使得只需要很少的规则就可以实现大范围的转换</li>
</ul>
</li>
<li><p>他的方法是什么？</p>
<ul>
<li>先用SRL（语义角色标注）标记句子中词汇的角色</li>
<li>识别到之后 根据一定的规则再去识别类或自然语言中之中别的属性和操作符</li>
<li>还有通用的——元动词转换规则——因为目的是生成测试用例</li>
<li>然后（如果能生成多个）就打分评估最好的输出</li>
<li>打分方法：完整性和正确性——用例中角色在OCL中出现的比例，变量名和用例名一致(<strong>这里完全没搞懂</strong>)</li>
</ul>
</li>
<li><p>他的评估方法是什么？</p>
<ul>
<li>比较了生成的和手动编写的 比较了正确率</li>
<li>对于需求 说了87个测试中生成除了多少</li>
<li></li>
</ul>
</li>
<li><p>他的不足在哪？</p>
<ul>
<li>聚焦于前后置条件 主要测试场景就是这个工业案例 所以对输入有预设，对输出有范式</li>
<li>吹得很猛 不需要限制 可以处理所有 其实目前只实现了7类规则，可以处理408个动词</li>
<li>对语言的规格还是有需求 需要输入UML类图</li>
<li>实验很弱</li>
</ul>
</li>
<li><p>有什么启发？</p>
<ul>
<li>比较的时候 应用场景 生成的灵活性都会比他好不少</li>
<li>对于方案设计启发不大</li>
</ul>
</li>
</ul>
<p>目的：</p>
<p>从自然语言的需求规范中自动生成系统测试用例</p>
<p>——》</p>
<p>为UMTG生成其需要的正则标准，提出了OCLGEN</p>
<p>使用语义分析技术来识别用力规范的前后置条件</p>
<p>可以75%正确生成前后置条件</p>
<p>系统测试很重要 其测试用例要展示功能和安全需求，</p>
<p>软件需求用NL写，然后由工程师手动转换，昂贵且易错</p>
<p>现有的自动化解决方案依靠限制过的，简单的自然语言解决</p>
<p>别人的方法（生成测试用例的）：</p>
<p>基于特殊关键词侦测，如 if then&#x3D;&gt;抽象，高层次 给测试人员</p>
<p>用受控的自然语言(CNL)写软件规格,再基于规则转换为正则标准&#x3D;&gt;可用语言非常有限</p>
<p>不用CNL 但需要其他的建模工作——UMTG就是这样的</p>
<p>OCLgen——捕获语句中的后置条件或前置条件——采用了文本转换规则，依赖自动语义分析技术，无需受控语言</p>
<p>SRL（语义角色标注）实现词汇的标注，例如，收到动作最直接影响的成员就应该出现在后置条件中</p>
<p>同义词识别，判断不同的词汇能否用相同的规则处理</p>
<p>在测试的工业案例中，75%精度，25%由于精度不足</p>
<p>UMTG: RUCM(用于写用例的一种语言格式 基本流替代流什么的)+OCL——测试用例</p>
<p>语义标注：搞清楚短语的角色对前后置条件的生成是必要的</p>
<p>别人搞的自然语言-用例生成不少用了语法识别 有一定用但搞不清短语作用</p>
<p>SRL有许多种 但用了CNP是因为他是唯一一个还在积极开发的 也有接口</p>
<p>同义检测：</p>
<p>VerbNet不仅包括同义词类，还包括模式，如主语+不及物或主系表结构 </p>
<p>使用了和PropBANK(CNP使用的）不同的模型，也会有不同的标签，但存在映射关系</p>
<p>同一类中的词共用一种模式，帮助定义可重用的转换规则，但不是同一类中的都是同义词（？）用来识别同义词最先进的方法是WordNet</p>
<h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><p>UMTG需要人写ocl捕获这两种信息：</p>
<ul>
<li>行动对于系统状态的影响——后置条件</li>
<li>用例的前置条件</li>
</ul>
<p>OCLgen就要自动化这一步骤</p>
<p>需要的输入与UMTG一致：</p>
<ul>
<li>RUCM写的用例规范</li>
<li>以UML类图形式的 系统的领域模型</li>
</ul>
<p>可以输出每一个用例步骤对应的OCL约束</p>
<p>OCLgen的方法：</p>
<ul>
<li>通过SRL<ul>
<li>挑选要出现在约束中的元素，</li>
<li>决定要使用的比较运算符</li>
<li>额外的操作符，如否定</li>
</ul>
</li>
<li>针对每一个动词 使用不同转换规则转换——为了可行，使用VerbNet合并词类，需要的规则更少</li>
</ul>
<p>转换步骤：</p>
<p>标记——选择规则——转换候选——挑选最高分</p>
<p><img src="/2022/06/20/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Automated-Generation-of-Constraints-from-Use-Case-Specifications-to-Support-System-Testing/image-20220625173337030.png" alt="image-20220625173337030"></p>
<p>可能有多个候选因为选择了多种规则</p>
<p>最高分赋予使用了用例步中最多可用信息的?</p>
<h1 id="OCL约束的格式"><a href="#OCL约束的格式" class="headerlink" title="OCL约束的格式"></a>OCL约束的格式</h1><p>一般就是比较笨蛋的 前置条件与条件步通常就是安全检查确保环境恰当，较容易捕获赋值，相等和不等关系</p>
<p><img src="/2022/06/20/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Automated-Generation-of-Constraints-from-Use-Case-Specifications-to-Support-System-Testing/image-20220625181049650.png" alt="image-20220625181049650"></p>
<p>是这样一套简化的EBNF 其中不少东西都是来自领域模型的，如类名和属性**</p>
<h1 id="转换规则"><a href="#转换规则" class="headerlink" title="转换规则"></a>转换规则</h1><p>所有转换规则共享相同的规则 每个规则和一组动词关联，如果一个动词出现，就执行一个步骤</p>
<p>在第X部分会讨论规则对英语（动词）的覆盖性，在这一部分主要讨论动词 be set enable的规则</p>
<p>SRL会识别出 左手边变量 left-hand side variable，操作符，选择元素和右手边变量right-hand side terms.</p>
<p>A1一般就是lhs varible</p>
<p>有两种转换规则：</p>
<ul>
<li><p>specific verb transformation rule: 对每个动词定制的转换规则</p>
</li>
<li><p>META verb transformation rule: 对任何动词使用的转换规则，</p>
<p>这种规则基于这样一种常见的现象 ，就是语句的LHSvarible是一个名称与其动词匹配或相同的属性(后面还会介绍)</p>
</li>
</ul>
<h1 id="VI-识别要用到OCL左侧的变量"><a href="#VI-识别要用到OCL左侧的变量" class="headerlink" title="VI 识别要用到OCL左侧的变量"></a>VI 识别要用到OCL左侧的变量</h1><p>真没搞懂在干嘛</p>
<h1 id="VII-识别右侧的变量"><a href="#VII-识别右侧的变量" class="headerlink" title="VII 识别右侧的变量"></a>VII 识别右侧的变量</h1><p>根据左侧变量的类型 支持角色来在输入的自然语言和模型中寻找类似的或可匹配到的</p>
<h1 id="VIII-识别操作符"><a href="#VIII-识别操作符" class="headerlink" title="VIII 识别操作符"></a>VIII 识别操作符</h1><p>用了别人的方法[35] 普遍都是类似于 be这样的动词</p>
<p>对于 除了……都……这样的范式 发明了一套方法 也是和语义角色标记有关系的</p>
<h1 id="IX-打分"><a href="#IX-打分" class="headerlink" title="IX 打分"></a>IX 打分</h1><p>从完整性和正确性两个维度</p>
<p>完整性：自然语言中所有概念被说明的程度有关。——用例中角色在OCL中出现的百分比</p>
<p>正确性：OCL中的变量名和用例中的名字一致</p>
<p><img src="/2022/06/20/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Automated-Generation-of-Constraints-from-Use-Case-Specifications-to-Support-System-Testing/image-20220625212958549.png" alt="image-20220625212958549"></p>
<h1 id="X-完整性和普遍性"><a href="#X-完整性和普遍性" class="headerlink" title="X 完整性和普遍性"></a>X 完整性和普遍性</h1><p>为了让更多的动词可以用：</p>
<ul>
<li>使用Verbnet 让同一规则可以用于更多词（同一类的词）</li>
<li>排除了形容人的感受&#x2F;行为&#x2F;动物行为的词</li>
</ul>
<p>经过分析，33个转换规则就可以给87类词转换 目前实现了7类规则 包括元转换规则。可以处理408个动词</p>
<h1 id="XI-经验评估"><a href="#XI-经验评估" class="headerlink" title="XI 经验评估"></a>XI 经验评估</h1><p>RQ1: 生成的OCL约束对吗？</p>
<p>RQ2: 对于用例规范 oclgen的自动生成效果如何？</p>
<p>RQ3: 限制生成效率的要素是什么？</p>
<h2 id="RQ1-生成的OCL约束对吗？"><a href="#RQ1-生成的OCL约束对吗？" class="headerlink" title="RQ1: 生成的OCL约束对吗？"></a>RQ1: 生成的OCL约束对吗？</h2><p>比较了生成的和手动写的</p>
<p>可能会： </p>
<ul>
<li>生成对的</li>
<li>生成错的</li>
<li>没有结果</li>
</ul>
<p>总正确率：66&#x2F;69&#x2F;87</p>
<h2 id="RQ2-对于用例规范-oclgen的自动生成效果如何？"><a href="#RQ2-对于用例规范-oclgen的自动生成效果如何？" class="headerlink" title="RQ2: 对于用例规范 oclgen的自动生成效果如何？"></a>RQ2: 对于用例规范 oclgen的自动生成效果如何？</h2><p>66&#x2F;87</p>
<h2 id="RQ3-限制生成效率的要素是什么？"><a href="#RQ3-限制生成效率的要素是什么？" class="headerlink" title="RQ3: 限制生成效率的要素是什么？"></a>RQ3: 限制生成效率的要素是什么？</h2><p>手动检查没有生成的句子</p>
<ul>
<li>信息不足</li>
<li>在用例规范和领域模型中表述不一样（is valid ——翻译不成——&lt;&gt;Error)</li>
</ul>
<h1 id="Threats-to-Validity"><a href="#Threats-to-Validity" class="headerlink" title="Threats to Validity"></a>Threats to Validity</h1><p>普遍性：只试了这个工业案例 BodySensetM</p>
<h1 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h1><p>自动生成可执行测试案例需要需求规格用CNL（只有有限的动词）写</p>
<p>OCLgen不需要受限的语言，而是RUCM，他引入了一些关键字，但并不限定使用的动名词</p>
<p>NL2OCL </p>
<p>处理UML类图和NL需求来得出类不变性和前后置条件，也用语义分析 来确定角色，靠侦测特定的关键字来确定被动语态或操作符</p>
<p>缺点：</p>
<p>   没有简化众多的动词怎么办——OCLgen的meta verb rule</p>
<p>已经没法拿来比较了 </p>
<p>NL2OCL更能抽取包括简单比较操作符的类不变量，而非生成前后置条件——这个对于测试用例生成更有用</p>
<p>提点问题：</p>
<ul>
<li><p>如何评估生成的OCL的质量？</p>
</li>
<li><p>在回答RQ时，使用了——产生了多少个OCL约束中多少个是正确的——没有给出“正确”的定义</p>
</li>
<li><p>此外 还评估了一下87个需求多少个能生成出来</p>
</li>
<li><p>在进行选择时，使用了打分机制，</p>
</li>
<li></li>
<li><p>输入输出是什么？</p>
</li>
</ul>
<p>输入包括两部分：</p>
<ul>
<li>RUCM格式的自然语言撰写的用例步骤</li>
</ul>
<img src="/2022/06/20/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Automated-Generation-of-Constraints-from-Use-Case-Specifications-to-Support-System-Testing/image-20220626184416896.png" alt="image-20220626184416896">

<ul>
<li>类图</li>
</ul>
<img src="/2022/06/20/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Automated-Generation-of-Constraints-from-Use-Case-Specifications-to-Support-System-Testing/image-20220626184653142.png" alt="image-20220626184653142" style="zoom: 25%;">

<p>输出似乎是有限的一种范式？</p>
<img src="/2022/06/20/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Automated-Generation-of-Constraints-from-Use-Case-Specifications-to-Support-System-Testing/image-20220626184748233.png" alt="image-20220626184748233" style="zoom:25%;">





<ul>
<li><p>创新点在哪？</p>
</li>
<li><p>借鉴 比较在哪</p>
</li>
<li><p>他做了什么事？方法是什么？</p>
</li>
</ul>
]]></content>
      <categories>
        <category>Substance</category>
      </categories>
      <tags>
        <tag>OCL</tag>
        <tag>Code generation</tag>
        <tag>ICST</tag>
      </tags>
  </entry>
  <entry>
    <title>论文笔记-CodeFill-ICSE`22</title>
    <url>/2022/07/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-CodeFill-ICSE-22/</url>
    <content><![CDATA[<p>这是 CodeFill: Multi-token Code Completion by Jointly Learning from Structure and Naming Sequences 这篇文章的读书笔记</p>
<p>不过主要是为了写论文看的XD</p>
<span id="more"></span>



<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>自动代码完成是在给定的上下文中预测开发者下面要输入什么，简单说下意义</p>
<p>讲讲当前的方法：怎么做的 有什么坏处 及时引用：</p>
<p>使用程序语言的特性 比如括号后面就要跟着对应的参数，坏处在于缺乏基于规则的完成</p>
<p>为了应对这些坏处 现在的办法是怎么做的，又引入了什么新的缺陷：</p>
<p>提出了一些基于统计和学习的模型 将其视为自然语言，这种办法丢失了代码结构和语义，不断引入的标识符也带来了巨大的预测空间</p>
<p>在这篇文章中 我们提出了xx 讲讲架构，任务，创新点，实现，应用场景 小吹一下</p>
<p>讲一下实验构造 简要介绍基线，任务和数据集，实验设置</p>
<p>讲讲结果 怎么个SOTA 最后总结贡献</p>
<p>主要贡献：</p>
<ul>
<li>模型：基于结构和名称的模型</li>
<li>代码和数据集及训练过程</li>
<li>广泛的评估</li>
</ul>
<h1 id="背景和相关工作"><a href="#背景和相关工作" class="headerlink" title="背景和相关工作"></a>背景和相关工作</h1><p>写在一起了</p>
<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>写的很水</p>
<ul>
<li><p>语言模型和Transformer</p>
<p>几乎没介绍什么 有点像凑字数</p>
</li>
<li><p>多任务学习 MTL</p>
<p>简单介绍了和自己工作差不多模式的这一小领域</p>
<p>多个任务，联合损失函数 应参数和软参数</p>
</li>
</ul>
<h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><h2 id="基线"><a href="#基线" class="headerlink" title="基线"></a>基线</h2><h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><p>简单介绍概览 可能给个图？</p>
<p><img src="https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9793835/9793541/9794048/9794048-fig-1-source-small.gif" alt="图 1：- CodeFill 工作流程"></p>
<p>包括三个部分  预处理 模型训练和 后处理</p>
<h2 id="预处理"><a href="#预处理" class="headerlink" title="预处理"></a>预处理</h2><p>造出两个不同版本的输入 一个是去掉空行空格注释的原始变量名之类的</p>
<p> 一个是用pythonAST 处理过的token type，记录每个token的类型，值和位置</p>
<p>使用了BPE编码 （这还要单独拿来吹）</p>
<p>为了处理python的缩进 额外引入了类似括号的标记符来标明缩进</p>
<h2 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h2><p>分别在两个层次上进行了三种模型训练，下一个token值预测TVP 下一个token类型预测TTP 和语句完成SC</p>
<p>首先用一个通用的语言模型在无标签数据上取得通用参数，然后再分别再三个任务上训练。</p>
<p>因为token类型太少 所以划分是4：2：4，且在微调阶段只使用另外两个任务</p>
<p>介绍了主要模型的架构 GPT-2 真的就很呆地说了一下有什么什么层</p>
<p>列了一个概率分布函数</p>
<p>列了最优化函数&#x2F;好像也就是普通的Cross-Entropy Loss</p>
<p>简单列了一下式子模型的</p>
<img src="/2022/07/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-CodeFill-ICSE-22/image-20220713152712828.png" alt="image-20220713152712828" style="zoom: 50%;">

<p>训练时每个epoch随机选择一个任务</p>
<p>损失在所有任务间共享，生成类任务使用波束搜索</p>
<p>希望能够最小化<img src="/2022/07/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-CodeFill-ICSE-22/image-20220713153042852.png" alt="image-20220713153042852" style="zoom:50%;"></p>
<p>TVP 和TTP 都是类似的训练方法，使用遮罩单向预测，即看到左侧的上文推断下个token</p>
<p>SC是直到<EOS>生成才会结束</EOS></p>
<p>波束搜索也就介绍了一下， 介绍了3，5，10的宽度尝试</p>
<p>训练方法：</p>
<p>要不要搞个这种大图（？）</p>
<p><img src="/2022/07/12/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-CodeFill-ICSE-22/image-20220713153349806.png" alt="image-20220713153349806"></p>
<h2 id="后处理"><a href="#后处理" class="headerlink" title="后处理"></a>后处理</h2><p>这个部分很有意思</p>
<p>生成之后会重新排序列表给开发者</p>
<p>大概就是 每一项预测 会以&lt;token, type, probability&gt;的形式给出来，然后根据不同的类型会有不同的处理。</p>
<p>在某一个程度的局部，如果预测的是个函数，候选者中和前面来自同一类的就更有可能。</p>
<p>具体地说，会对前面的进行观察，然后存成一个列表，对预测列表对观察列表进行交叉检查 如果有重复，就根据预测项的类型乘以不同的权重，使其更有可能。</p>
<p>简单地说，就是认为变量和函数名等都有一定的聚集性，如果预测的某个在前面某种关联性出现了 那么就给他酌情更重要一些。</p>
]]></content>
      <categories>
        <category>Substance</category>
      </categories>
      <tags>
        <tag>Note</tag>
        <tag>ICSE 2022</tag>
        <tag>Code generation</tag>
      </tags>
  </entry>
  <entry>
    <title>论文笔记 Highway OCL &amp; Informal to Formal Specications in UML</title>
    <url>/2022/06/28/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-From-Informal-toFormal-Specications-in-UML/</url>
    <content><![CDATA[<p>这篇笔记是文章 From Informal to Formal Specications in UML 和 Automating Utility Permitting within Highway Right-of-Way via a Generic UML&#x2F;OCL Model and Natural Language Processing的不完全笔记</p>
<span id="more"></span>
<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>Purpose :</p>
<p> informal requirements,use cases &#x3D;&gt; formal specifications ,OCL</p>
<h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><p><strong>how to obtain formal specications from informal ones</strong></p>
<p>formal ones can also help to improve informal ones.</p>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>在intro中讲故事的这一部分很有意思 可能是因为是04年的文章 他的引入视角是——UML在OO建模很猛——有了OCL才能足够精确——不知道应该在什么环节集成OCL——还是应该customer来做，毕竟对系统行为清晰——期望所有人都有能力不现实</p>
<p>用例——describe system behaviors</p>
<p>研究了 文字用例中的前后置条件 与 类图中OCL写的操作的前后置条件 的<strong>关系</strong></p>
<p>从分析阶段的用例图——到设计阶段的时序图和类图</p>
<h2 id="用例图"><a href="#用例图" class="headerlink" title="用例图"></a>用例图</h2><p>非正式的本质 可以用在更多的上下文本经当中，但寻求正确的抽象层次和确保一致性困难</p>
<p>在他们的工作中 书写用例描述的风格不太重要 只要他们能够展现出需求与开发设计者的共识，获得前后置条件</p>
<p>考虑了关于替代流的问题 本来想掰开成为别的用例 但觉得违背了为用户满足目标的愿景</p>
<h1 id="状态图"><a href="#状态图" class="headerlink" title="状态图"></a>状态图</h1><p>考虑了用例的所有路径</p>
<h1 id="从用例到操作后置条件"><a href="#从用例到操作后置条件" class="headerlink" title="从用例到操作后置条件"></a>从用例到操作后置条件</h1><p>只讨论用例的后置条件 因为觉得前置差不多 而且有点迷惑——比方说满足了前置条件就相当于进入某些流’</p>
<p>所以就当所有的前置条件都是空</p>
<p>讨论系统中的唯一对象就是系统本身</p>
<p>客户给的自然语言描述后置条件可能不光和终态有关，还可能和事件序列有关 比如输错三次密码</p>
<p>——使用状态图来捕获条件</p>
<h1 id="Automating-Utility-Permitting-within-Highway-Right-of-Way-via-a-Generic-UML-x2F-OCL-Model-and-Natural-Language-Processing"><a href="#Automating-Utility-Permitting-within-Highway-Right-of-Way-via-a-Generic-UML-x2F-OCL-Model-and-Natural-Language-Processing" class="headerlink" title="Automating Utility Permitting within Highway Right-of-Way via a Generic UML&#x2F;OCL Model and Natural Language Processing"></a>Automating Utility Permitting within Highway Right-of-Way via a Generic UML&#x2F;OCL Model and Natural Language Processing</h1><p>OCL用来表达一些方位上的约束</p>
<p>implies的作用：只有前面的条件是真，后面的东西才会被评估</p>
<p>采用了扩展的OCL 增添了一些对空间语义的描述的函数 如不相交 相当 包含等</p>
<p>要将这样的文本约束转化为OCL——首先可以先变成较为形式化的元组</p>
<p>the 6-inch mechanical joint inlet shall be located 5 feet 6 inches below the ground</p>
<p>——》</p>
<p>&lt;mechanical joint inlet (6-inch), below (5 ft 6 in.), ground&gt;.</p>
<p>方位约束使用双层或多层语言层次来提取多种的方位信息</p>
<p>想要自动从方位约束中提取结构化的信息</p>
<ol>
<li>预处理：tokenize 句子划分和语法标注</li>
<li>特殊标注 增添了额外三个词表 城市产品，方位词和比较关系的词会被识别标出</li>
<li>逐级向上归纳 生成句子树</li>
<li>从树型结构中寻找目标信息，生成结构化的元组</li>
<li>从元组中生成所需OCL 不同层次有不同转换方法，主要是针对类不变量——x.allInstances-&gt;forAll ——implies这样的结构</li>
</ol>
<p>NLP的方法：</p>
<p>使用某个NLTK实施标注和句子树的构建，从裸文本生成出对应元组，作为构造OCL约束的输入</p>
<p>UML&#x2F;OCL的方法：</p>
<p>应用：</p>
<p>写在某种对象关系型数据库里 当有数据库的改动时触发触发器，检查是否符合约束</p>
]]></content>
      <categories>
        <category>Substance</category>
      </categories>
      <tags>
        <tag>OCL</tag>
        <tag>Code generation</tag>
      </tags>
  </entry>
  <entry>
    <title>CSAPP- Notes</title>
    <url>/2023/02/15/CSAPP-notes/</url>
    <content><![CDATA[<p>z</p>
<p>这是CSAPP的随手笔记</p>
<span id="more"></span>

]]></content>
      <categories>
        <category>Substance</category>
      </categories>
      <tags>
        <tag>Note</tag>
      </tags>
  </entry>
  <entry>
    <title>Note-CSE4650 - Methods of Data Mining</title>
    <url>/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/</url>
    <content><![CDATA[<p>这是关于数据挖掘方法的课程笔记。以前都是复习周才搞这种东西
但是现在发现不从课程前两周就开始会彻底完蛋…… 希望可以学的好一点</p>
<p>批话：</p>
<ul>
<li>我感觉我全搞明白了（9/20，PCA)</li>
</ul>
<span id="more"></span>
<h1 id="学前信息">学前信息</h1>
<h2 id="前置知识">前置知识：</h2>
<ul>
<li>概率：
<ul>
<li>交并补概率的计算</li>
</ul></li>
<li>线性代数
<ul>
<li>特征值与特征向量</li>
</ul></li>
<li>图论
<ul>
<li>连通分量</li>
<li>团</li>
<li>点度</li>
<li>最短路</li>
</ul></li>
<li>算法
<ul>
<li>复杂度估计</li>
</ul></li>
<li>统计
<ul>
<li>向量的平均，中位数，方差和协方差</li>
<li>卡方(<span class="math inline">\(chi^2\)</span>)检测</li>
</ul></li>
</ul>
<h2 id="使用的教材">使用的教材</h2>
<p>Charu C. Aggarwal: Data Mining: The Textbook, Springer 2015</p>
<h2 id="怎么拿分数">怎么拿分数</h2>
<ul>
<li>5p for 5 exercise groups</li>
<li>10p for 5 homework in groups</li>
<li>24p for exam 12.13, 13:~16:</li>
<li>1p for prerequisite</li>
</ul>
<h2 id="学习目标">学习目标</h2>
<ul>
<li>数据挖掘的基本问题模式和方法</li>
<li>对应问题，关键词的对策</li>
<li>验证方法</li>
<li>能使用程序完成</li>
<li>能够对问题的计算有所预估和替代方法</li>
<li>实践</li>
</ul>
<h1 id="简介">简介</h1>
<ul>
<li><p>什么是数据挖掘？</p>
<ul>
<li>没有确切的定义</li>
</ul>
<p>挑战：</p>
<ul>
<li>高效的算法</li>
<li>以假乱真的发现</li>
</ul>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20230917002129599.png" alt="image-20230917002129599" style="zoom:50%;"></p>
<p>和相关领域的关系：</p>
<figure>
<img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20230917002221082-169530761252420.png" alt="image-20230917002221082">
<figcaption aria-hidden="true">image-20230917002221082</figcaption>
</figure>
<ul>
<li>模型更倾向于全局数据，而模式更倾向于局部的</li>
</ul></li>
<li><p>过程：</p>
<ul>
<li>定义问题-预处理-mining——验证——展示和总结</li>
<li>经常来说，mine出来的东西会适合进入下一轮data mining</li>
</ul></li>
</ul>
<h1 id="预处理">预处理</h1>
<ul>
<li>清洗：错误与缺失值</li>
<li>特征提取：结合与变形已有的形成新的特征</li>
<li>数据减少（reduction）：
<ul>
<li>采用</li>
<li>特征选择</li>
<li>维度缩减</li>
</ul></li>
</ul>
<h2 id="清洗">清洗</h2>
<p>Outliers：有时候要搞掉，但有时候很有用</p>
<h3 id="对付错误的办法">对付错误的办法：</h3>
<ul>
<li>从多个数据源检查不一致</li>
<li>使用 domain knowledge</li>
<li>检查 outliers 和 extreme value</li>
<li>数据平滑：噪声与随机波动
<ul>
<li>scaling</li>
<li>discretization（离散化）</li>
<li>dimension reduction</li>
</ul></li>
<li>建模阶段使用 robust方法</li>
</ul>
<h3 id="缺失值">缺失值</h3>
<p>可以的话，使用正确的替换</p>
<ul>
<li>feature 缺失的太多：不要（ prune）了</li>
<li>record 缺失的太多：整个不要了</li>
<li>估计而填补：
<ul>
<li>均值/中位数（考虑范围：在整个还是多大的局部？）</li>
<li>通过其他feature预测：随机森林</li>
<li>估算可能会有严重的影响</li>
</ul></li>
</ul>
<p>或者使用允许空值的算法</p>
<h2 id="特征提取">特征提取：</h2>
<h3 id="scaling-normalizationnum--num">Scaling( normalization):num-&gt;
num</h3>
<h4 id="min-max-scaling">min-max scaling</h4>
<h4 id="mean-normalization">mean normalization:</h4>
<ul>
<li>Scaling， normalization——num-&gt; num
<ul>
<li><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20230917013505657-169530762209821.png" alt="image-20230917013505657" style="zoom:50%;"></li>
</ul>
<h4 id="standardization-or-z-score-normalization">Standardization or
z-score normalization</h4>
<ul>
<li>这个称作「Standardization or z-score normalization」
<ul>
<li>stdev-&gt; standard deviation 标准差</li>
<li><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20230917013528927-169530762523922.png" alt="image-20230917013528927" style="zoom:50%;"></li>
<li>作用：
<ul>
<li><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231201211018355.png" alt="image-20231201211018355" style="zoom:50%;"></li>
</ul></li>
</ul></li>
</ul></li>
</ul>
<h3 id="discretization-numcate--categorical">discretization：
num(cate)-&gt; categorical</h3>
<ul>
<li>discretization： num-&gt; categorical
<ul>
<li>将数据范围按照一定间隔分为类（bin), 并给标签</li>
<li>常用方法：
<ul>
<li>等宽：尤其适用于均匀分布（uniform distribution）</li>
<li>等深：每类一样多</li>
</ul></li>
<li>好处和局限：
<ul>
<li>可以对付噪音，个体差异和混合数据</li>
<li>算法更有效，也可以用更多的</li>
<li>丢失信息</li>
<li>优的离散化并不容易——可能取决于其他变量</li>
</ul></li>
</ul></li>
</ul>
<h4 id="binarization">Binarization:</h4>
<ul>
<li>二元化：cate-&gt; binary
<ul>
<li>算是离散化的一种特例</li>
</ul></li>
</ul>
<h3 id="similarity-graphs---graph">similarity graphs: * -&gt; graph</h3>
<h4 id="大致步骤">大致步骤：</h4>
<ol type="1">
<li><p>计算每个对象之间的距离</p></li>
<li><p>1 无向边： 如果两者距离小于阈值</p>
<p>2 有向边：如果j是i的K近邻： i-&gt;j</p></li>
<li><p>将距离映射为边权（语焉不详）</p></li>
</ol>
<ul>
<li>similarity graphs: * -&gt; graph （怎么做此处存疑）
<ul>
<li>展现成对的相似性，通过最近邻</li>
<li>好处
<ul>
<li>只要能算距离就行，不管种类
<ul>
<li>尤其是聚类，推荐等</li>
</ul></li>
<li>可以使用很多网络算法</li>
</ul></li>
<li>可能复杂度<span class="math inline">\(n^2\)</span> 起跳</li>
<li>方法：
<ul>
<li>把每个对象看成点</li>
<li>算每对的距离</li>
<li>如果距离小于$ $ 就给他们连上无向边</li>
<li>或者</li>
<li>A是B的<span class="math inline">\(K\)</span>个最近邻-&gt;连上B-A有向边（f方向可能可以被忽略）</li>
<li>使用一个权重公式，来衡量边的相似性
<ul>
<li><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20230917173800899-169530764266123.png" alt="image-20230917173800899" style="zoom:50%;"></li>
</ul></li>
</ul></li>
</ul></li>
</ul>
<h2 id="data-reduction-数据降维">Data reduction 数据降维</h2>
<ol type="1">
<li>sampling： only choose some of records</li>
<li>feature selection: 根据应用选择
<ol type="1">
<li><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231201212335472.png" alt="image-20231201212335472" style="zoom:50%;"></li>
</ol></li>
<li>dimension reduction，降维</li>
</ol>
<hr>
<ul>
<li>多个有冗余性的特征中创造出一个新的</li>
<li>数据减少：
<ul>
<li>样本采样</li>
<li>特征选择</li>
<li>维度缩减：
<ul>
<li>旋转轴（PCA,SVD) ?</li>
<li>类型转换</li>
</ul></li>
</ul></li>
</ul>
<h1 id="lecture-3-相似性和距离的衡量">Lecture 3 相似性和距离的衡量</h1>
<ul>
<li>什么是距离？</li>
</ul>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20230917174619556.png" alt="image-20230917174619556" style="zoom:50%;"></p>
<p>需要满足四个性质：<strong>才是metric</strong></p>
<ul>
<li>非负性 （non-negativity）</li>
<li>同一性（coincidence axiom）：
<ul>
<li>d=0 <span class="math inline">\(iff\)</span> A=B</li>
</ul></li>
<li>对称性(symmetry)（无向距离）</li>
<li>三角不等式(triangle inequality)</li>
</ul>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231204214905480.png" alt="image-20231204214905480" style="zoom:50%;"></p>
<p>几个例子：</p>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20230920162701682.png" alt="image-20230920162701682" style="zoom:50%;"></p>
<p>证明一个玩意是距离：</p>
<ul>
<li>证这四个性质对于任意变量成立</li>
<li>——还是证明不是比较容易</li>
</ul>
<p>fractional <span class="math inline">\(L_p\)</span>
不是metric：不满足三角不等式</p>
<ul>
<li>距离与相似性：
<ul>
<li>相似性通常在<span class="math inline">\([0,1]\)</span></li>
<li>通常采用1-正则化距离 来得到相似性</li>
</ul></li>
</ul>
<p>度量空间（Metric Space) <span class="math inline">\((S,d)\)</span>-&gt; 数据和距离</p>
<p>三角不等式优化的一个例子：</p>
<p>给出<span class="math inline">\(n\)</span> 个点和<span class="math inline">\(K\)</span>个聚类中心，找到每个点最近的中心</p>
<ul>
<li>朴素做法：<span class="math inline">\(nK\)</span> 次计算距离</li>
<li>剪枝技巧：
<ul>
<li>计算所有中心彼此的距离</li>
<li>计算每个点和某个中心的距离，然后迭代查表寻找</li>
<li><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20230917180041406.png" alt="image-20230917180041406" style="zoom:50%;"></li>
<li>所以应该只是剪枝？</li>
</ul></li>
</ul>
<h2 id="不同数据类型的距离度量">不同数据类型的距离度量</h2>
<h3 id="多维数类">多维数类:</h3>
<h4 id="lp-范数衡量向量空间中大小或者距离">Lp
范数：衡量向量空间中大小或者距离</h4>
<p>Minkowski距离——Lp范数的一种特例</p>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20230917183053294.png" alt="image-20230917183053294" style="zoom:50%;"></p>
<p>P=1 曼哈顿距离</p>
<p>P=2 欧几里得距离</p>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20230917183549887.png" alt="image-20230917183549887" style="zoom:50%;"></p>
<ul>
<li>可以看出来 维度越高，就越由差最大的那一个决定</li>
</ul>
<p>Lp-norm的一个问题：</p>
<p>向量空间维度高了 就不好使了：</p>
<p><strong>纬度越高，数据越稀疏</strong></p>
<p>Larger p will emphasize irrelevant features</p>
<p>General rule: the larger the dimensionality, the smaller value of p
to use</p>
<p>纬度越高，</p>
<ul>
<li>区分性就越小——随机数据集上最大和最小差的不多</li>
<li>就越关心单一维度的差异——999维度相同，一个不一样
就完全反映的是不一样的</li>
<li>解决的可能办法：给某维度加上权重</li>
</ul>
<h4 id="维度灾难">维度灾难：</h4>
<p>Curse of dimensionality：</p>
<p>原因： 向量空间</p>
<h4 id="match-based-similarity-with-proximity-thresholding">Match-based
similarity with proximity thresholding</h4>
<p>这玩意能治 curse of dimensionality？</p>
<p><strong>衡量的是相似性，所以局部接近才有意义</strong></p>
<p>（有临近阈值约束的相似性衡量）</p>
<p>这个东西是为了解决以下的两个问题：</p>
<ol type="1">
<li>特征只在局部有相关性（糖尿病人的血糖而不是瘫痪的）</li>
<li>大维度下，两个对象不太可能一样，除非某些特征相近</li>
</ol>
<p>手段是 强调其相似的维度，</p>
<p>具体做法是，对每一个维度进行等深离散化，只关注每个相同bin中的距离</p>
<p><strong>什么是equal depth？ 每一个bin中都有<span class="math inline">\(1/m\)</span>的records</strong></p>
<p>所以每个维度上的bins也不一样</p>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20230917205442693.png" alt="image-20230917205442693" style="zoom:50%;"></p>
<p>这张图中就只关注<span class="math inline">\(1，3\)</span>两个维度的距离差</p>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20230917205716976.png" alt="image-20230917205716976" style="zoom:50%;"></p>
<p>挑选参数：维度<span class="math inline">\(K\)</span>越大，<span class="math inline">\(m\)</span> 分的bin就越多</p>
<p><strong>注意，这里是similarity，所以完全相同的算出来是<span class="math inline">\(K^{1/P}\)</span>，完全不同的（没有一个bins相同）算出来是0</strong></p>
<h4 id="cos-相似性">Cos 相似性</h4>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20230917205856749.png" alt="image-20230917205856749" style="zoom:50%;"></p>
<ul>
<li>适用性：
<ul>
<li>数类（整数，实数）</li>
<li>二进制数</li>
</ul></li>
<li>[ − 1, 1]</li>
<li>常用于数字表示的文本文档</li>
<li>如果向量都被正则化了，那么和L2是有关系的</li>
</ul>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20230917210131237-169530766242824.png" alt="image-20230917210131237" style="zoom:50%;"></p>
<p>新问题：</p>
<p>距离是否应该反应数据分布呢？
换句话说，在更稀疏的方向上，同样的差昭示着更大的差异：</p>
<h4 id="mahalanobis-distance">Mahalanobis distance</h4>
<p>马哈拉诺比斯距离， 考虑了各个特征之间的相关性和不同特征的方差</p>
<p>公式：</p>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20230917210619748.png" alt="image-20230917210619748" style="zoom:50%;"></p>
<p>​ <span class="math inline">\(\Sigma^-1\)</span>
协方差矩阵的逆，描述了数据特征间的相关性，逆矩阵用来给特征差异加权</p>
<p><strong>更密的方向（higher variance diretion)
更近，下图中A比B近</strong></p>
<p>场景：</p>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20230917210800750.png" alt="image-20230917210800750" style="zoom:67%;"></p>
<p>类似的推断：</p>
<p>如果距离测量只能通过一定路径呢？ 比如 最近邻图</p>
<h4 id="isomap">ISOMAP</h4>
<p>Isometric Mapping 等距映射</p>
<p>创造一个近邻图，对于每个点，对于其K个近邻链接</p>
<p>距离为两点间的最短路</p>
<p>可选步骤：嵌入数据，降维表示</p>
<figure>
<img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20230917213449689-169530767273325.png" alt="image-20230917213449689">
<figcaption aria-hidden="true">image-20230917213449689</figcaption>
</figure>
<p>实际上使用了最近邻建图才会发现其实很远</p>
<p>这也是一种embedding？</p>
<p>像是这样的情况，可以用局部mahalanobis 距离
但是分组其实本身就需要距离，有点循环悖论的感觉了。</p>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231204210922229.png" alt="image-20231204210922229" style="zoom:50%;"></p>
<h3 id="类数据">类数据</h3>
<p>常用方程：</p>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20230917213718931.png" alt="image-20230917213718931" style="zoom:50%;"></p>
<ul>
<li>Wi= <span class="math inline">\(1/k\)</span></li>
<li>S采用二元表示</li>
</ul>
<p>如果考虑到频率的话，就使用</p>
<h4 id="goodall-measure">Goodall measure</h4>
<p>其中，越常见的特征对于总体相似贡献越低</p>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20230917214229908.png" alt="image-20230917214229908" style="zoom:50%;"></p>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20230917214253794.png" alt="image-20230917214253794" style="zoom:50%;"></p>
<p>总体分数：</p>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20230917214240957.png" alt="image-20230917214240957" style="zoom:50%;"></p>
<p>下面除以特征数，上面统计每一个特征的贡献，其中越稀有的特征贡献越接近1，<strong>越常见的特征贡献越接近0</strong>，不同的特征相似为0——如果大家在这个方面都一样，那么就没贡献什么信息。</p>
<p>例子：</p>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231204212229379.png" alt="image-20231204212229379" style="zoom: 33%;"><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231204212651712.png" alt="image-20231204212651712" style="zoom: 33%;"></p>
<p><strong>这里的「similarity」意味着特征的重叠数，即有几个特征能匹配上</strong></p>
<h3 id="混合数据无需变形">混合数据（无需变形）</h3>
<p>给与权重，再分别计算</p>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20230917231626261-16949817904071-169530768182626.png" alt="image-20230917231626261" style="zoom:50%;"></p>
<p>通常范畴不同，所以还需要使用标准差进行标准化</p>
<h3 id="二元数据">二元数据</h3>
<p>直接算：</p>
<h4 id="hamming-distance">Hamming Distance</h4>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20230917225205567.png" alt="image-20230917225205567" style="zoom:50%;"></p>
<p>缺点：忽视了数据本身的长度，只考虑差异项</p>
<p>通常可以将set变形，得到长二元数据</p>
<p>-&gt;
set通常很稀疏，常见元素就会显得很一致，即，忽略了集合本身的大小</p>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20230917225350551.png" alt="image-20230917225350551" style="zoom:50%;"></p>
<p>这两个距离都是10 有点失真</p>
<h4 id="jaccard-coefficient">Jaccard coefficient</h4>
<p>这个东西是相似度，越高越好</p>
<p>使用 <strong>Jaccard coefficient</strong>
进行计算，这是一个常用于集合相似性的计算方式</p>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20230917225520590.png" alt="image-20230917225520590" style="zoom:50%;"></p>
<p>其中0和1的<strong>待遇</strong>不一样—<strong>稀疏矩阵!</strong></p>
<h3 id="字符串数据">字符串数据</h3>
<p>Hamming Distance就不好用了：</p>
<ul>
<li>必须等长</li>
<li>typo惩罚太大</li>
</ul>
<h4 id="levenshtein-distance">Levenshtein distance</h4>
<p>使用<strong>Levenshtein distance</strong>
，即最小编辑距离，包括以下三种操作：</p>
<ul>
<li>插入</li>
<li>删除</li>
<li>替换</li>
</ul>
<p>都是一个字符</p>
<p>可以给不同的操作辅以权重</p>
<p><strong>当其满足以下条件，才是mertic：</strong></p>
<ul>
<li>操作代价为正</li>
<li>反操作存在且代价一致</li>
</ul>
<h3 id="文本数据">文本数据</h3>
<p>将文档转化为m长度的向量（词典大小）</p>
<p>统计每个词的频数/出现/tf-idf</p>
<p>然后计算<strong>cos相似性</strong> （Jaccard coefficient
也不是不能用</p>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231204213958228.png" alt="image-20231204213958228" style="zoom:50%;"></p>
<p>分别是document和term（词）</p>
<h2 id="注意">注意：</h2>
<p>名字和参数一般都不太确定，所以最好还是加点文献引用</p>
<p>重点：</p>
<ul>
<li>高维Lp的不好使</li>
<li>cos和阈值约束的距离</li>
<li>non-metrics可能对于高位的相似性计算更好（还没学吧）</li>
</ul>
<h1 id="lecture3-dimension-reductionpcasvd">Lecture3 Dimension
reduction(PCA,SVD)</h1>
<p>主成分分析（Principal component analysis (PCA)</p>
<p>这章的目的应该是对数据降维，手段包括特征<strong>eigen</strong> 和奇异
<strong>singular</strong></p>
<h2 id="动机">动机：</h2>
<ul>
<li>高维数据比较困难
<ul>
<li>难以聚类</li>
<li>对于特征强相关的 可以用少数特征来表示同样的信息</li>
</ul></li>
</ul>
<p>手段：缩减维度来减少冗余</p>
<ul>
<li>只知道成对的距离</li>
</ul>
<p>好处：</p>
<ul>
<li>降低开销</li>
<li>去除噪声</li>
<li>结果更好理解</li>
</ul>
<blockquote>
<p>早期做会丢失信息</p>
</blockquote>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20230920215126603.png" alt="image-20230920215126603" style="zoom:50%;"></p>
<h2 id="前置数学知识">前置数学知识</h2>
<p>需要的数学知识比较多，包括协方差矩阵(covariance matrix)
拉普拉斯矩阵，半正定矩阵，矩阵的奇异值向量等</p>
<p><strong>向量<span class="math inline">\(x\)</span>一般说的都是列向量</strong></p>
<p>下面先对协方差矩阵进行一些学习。</p>
<h3 id="协方差矩阵">协方差矩阵</h3>
<p>相关系数用来描述两个变量之间的<strong>线性</strong>相关关系，范围从[-1,1]
<span class="math display">\[
ρ = (Σ((X - μX)(Y - μY))) / (σX * σY)
\]</span> 相关系数可以认为是标准化了的协方差</p>
<p>协方差矩阵是这么算的</p>
<p>注意其中的<span class="math inline">\(X,Y\)</span>
可以理解为多元变量，即每一项是一个一维向量（一个变量）的二维变量 <span class="math display">\[
Cov(X, Y) = {Σ((X - μX)(Y - μY)) \over N-1}
\]</span>
<img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20230921175006273.png" alt="image-20230921175006273" style="zoom: 67%;"></p>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/Correlation66-169530774470329.png" alt="img"> <span class="math display">\[
A=\left\{\begin{matrix}X &amp; Y &amp; Z\end{matrix}\right\}=\left\{
\begin{matrix} x_1-x &amp; y_1-y &amp; z_1-z \\ ... &amp; ... &amp;
...\\ x_n-x &amp; y_n-y &amp; z_n-z \end{matrix} \right\}
\]</span> 其中<span class="math inline">\(X,Y,Z\)</span>都是随机变量，<span class="math inline">\(x_i\)</span>可以理解为一次观测值</p>
<p><span class="math display">\[
C(X,Y,Z)={A^TA \over n}
\]</span></p>
<ul>
<li><p>协方差矩阵的性质：</p>
<ul>
<li>对角线上是每个变量的方差</li>
<li>显然是一个对称矩阵，<span class="math inline">\(C=C^T\)</span>，
<ul>
<li>因为<span class="math inline">\(Cov(X,Y)=Cov(Y,X)\)</span></li>
</ul></li>
<li>他还是一个半正定矩阵</li>
</ul>
<blockquote>
<p>至于为什么正定 暂且还没有搞懂</p>
</blockquote></li>
</ul>
<p><strong>对于零均值（mean-centered)的数据，协方差矩阵的特征向量矩阵是正交矩阵</strong></p>
<p>update:
对称矩阵的特征向量都是正交的——正定矩阵的特征向量都是正交的</p>
<h3 id="正交矩阵">正交矩阵</h3>
<p>对于矩阵 Q，以下条件等价：</p>
<ol type="1">
<li>Q 是正交矩阵。</li>
<li>Q 的列向量是单位向量，并且两两正交（即内积为零）。</li>
<li>Q 的转置<span class="math inline">\(Q^T\)</span>是其逆矩阵（<span class="math inline">\(Q^TQ = QQ^T = I\)</span>，其中 <span class="math inline">\(I\)</span> 是单位矩阵）</li>
</ol>
<h3 id="半正定矩阵">半正定矩阵</h3>
<p>首先，在实数范畴内，矩阵的正定性都是在其为对称矩阵的前提下研究的，即：</p>
<center>
（半）正定矩阵一定是对称矩阵
</center>
<p>定义：</p>
<p>对对称矩阵A，若对任意非零向量<span class="math inline">\(x\)</span>，有<span class="math inline">\(x^TAx&gt;0\)</span>,则称<span class="math inline">\(A\)</span>为正定矩阵，若能取到0，则为半正定矩阵</p>
<p>性质：</p>
<ul>
<li>特征值大于（等于0） 半正定的时候能取到</li>
<li>对角线元素非负</li>
</ul>
<h3 id="特征值分解">特征值分解</h3>
<p>一般来说，求解特征值是为了实现矩阵的对角化： <span class="math display">\[
A = PDP⁻¹
\]</span> <span class="math inline">\(A\)</span>是方阵</p>
<p><span class="math inline">\(P\)</span>是可逆矩阵，列向量为<span class="math inline">\(A\)</span>的特征向量</p>
<p><span class="math inline">\(D\)</span>是对角阵，对角线是<span class="math inline">\(A\)</span>的特征值，顺序与<span class="math inline">\(P\)</span>对应</p>
<p>对角化的矩阵具有很多应用，如快速计算A的高次幂。</p>
<p><strong>对角化步骤：</strong></p>
<p>首先对特征值，特征向量的一般形式进行了解： <span class="math display">\[
对方阵A,有Ax=\lambda x ，称x是A的一个特征向量，\lambda 是x对应的特征值
\]</span></p>
<h4 id="特征值求解">特征值求解</h4>
<ol type="1">
<li><p>求解det(行列式) <span class="math inline">\(det(A-\lambda
I)=0\)</span>,求出若干特征值</p></li>
<li><blockquote>
<p>det(A)=0</p>
<p>A不可逆</p>
<p>A线性相关</p>
<p>A不满秩</p>
</blockquote></li>
<li><p>求解<span class="math inline">\(Ax=\lambda
x\)</span>,求出每个特征值对应的特征向量（非零）</p></li>
</ol>
<blockquote>
<p>特征值有很多良好的性质：</p>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20230920200335261-169530776031330.png" alt="image-20230920200335261" style="zoom:50%;"></p>
</blockquote>
<p>​ 此时，如果 1. <span class="math inline">\(A\)</span>满秩 2. <span class="math inline">\(X={x_1,...,x_n}\)</span>特征向量是一组线性无关的</p>
<p>​ 那么矩阵可以对角化为 <span class="math display">\[
A = PDP⁻¹
\]</span>
对于零均值（mean-centered)的数据，协方差矩阵的特征向量矩阵是正交矩阵,即
<span class="math display">\[
P^{-1}=P^T
\]</span> 所以也可以写成 <span class="math display">\[
C=PDP^T
\]</span></p>
<h3 id="奇异值分解">奇异值分解</h3>
<p>SVD能够适用于任何矩阵分解，</p>
<p>对于任意矩阵<span class="math inline">\(A\)</span>,</p>
<p>对<span class="math inline">\(AA^T\)</span>求特征值，用单位化特征向量构成<span class="math inline">\(U\)</span></p>
<p>对<span class="math inline">\(A^TA\)</span>求特征值，用单位化特征向量构成<span class="math inline">\(V\)</span></p>
<p>对<span class="math inline">\(AA^T或A^TA\)</span>求特征值的平方根，构成对角阵<span class="math inline">\(\Sigma\)</span> <span class="math display">\[
A=U\Sigma V^T
\]</span> U和V分别成为左右奇异值矩阵</p>
<h2 id="pca">PCA</h2>
<p>PCA for <strong>Principal component analysis</strong>,主成分分析</p>
<h3 id="假设">假设</h3>
<ul>
<li>高方差可以展示数据的结构
<ul>
<li>为什么？</li>
<li>如果在这一维度（i.e.旋转后的坐标轴投影）上，数据具有较大的方差，就可以认为这一个维度能够反映作为信号的信息，而其他轴可能就是它的噪声</li>
</ul></li>
<li>数据可以被正交基向量的线性组合表示</li>
</ul>
<p>希望可以用一个变形的矩阵<span class="math inline">\(d*r的P\)</span>，来将<span class="math inline">\(n*d的 D\)</span>表示为<span class="math inline">\(n*r\)</span>的<span class="math inline">\(D*P_r\)</span>(r&lt;d)</p>
<p><strong>即，希望把数据由d维降到r维</strong></p>
<blockquote>
<p>这r维是全新的正交特征，也成为主成分，希望可以有相对最大的数据方差，而舍弃的维度中，方差几乎为0</p>
</blockquote>
<h3 id="特征值分解-1">特征值分解</h3>
<p>怎么让方差最大呢？就需要用协方差矩阵，对应的特征值所对应的<span class="math inline">\(k\)</span>个特征向量所组成的矩阵</p>
<p>首先考虑零均值（mean-centered)的数据<span class="math inline">\(D\)</span>,</p>
<p>一般数据的零均值过程如下：</p>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20230920214837704.png" alt="image-20230920214837704" style="zoom:50%;"></p>
<ol type="1">
<li><p>mean-centered</p>
<p>每一列计算其均值，分别减掉</p></li>
<li><p>计算协方差矩阵<span class="math inline">\(C\)</span></p></li>
</ol>
<p><span class="math display">\[
C = {1\over n − 1} D^T D
\]</span> 将其对角化为 <span class="math display">\[
C=PΛP^T
\]</span> 3. 选取<span class="math inline">\(P\)</span>的前<span class="math inline">\(r\)</span>大的特征值对应的向量，组成新的特征矩阵</p>
<p><span class="math display">\[
P_r
\]</span> 4. 目标降维矩阵即为</p>
<p><span class="math display">\[
D^`=DP_r
\]</span></p>
<h3 id="pca-不好使的时候">PCA 不好使的时候：</h3>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231204224613675.png" alt="image-20231204224613675" style="zoom:50%;"></p>
<figure>
<img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231204224717461.png" alt="image-20231204224717461">
<figcaption aria-hidden="true">image-20231204224717461</figcaption>
</figure>
<h2 id="奇异值svd分解">奇异值SVD分解</h2>
<h3 id="pca和svd的区别">PCA和SVD的区别：</h3>
<ul>
<li>SVD不要求mean-centerized</li>
<li>PCA一般搞covariance matrix（所以会有半正定的好性质），
SVD都行（一般直接搞D）</li>
<li></li>
</ul>
<h3 id="svd过程">SVD过程</h3>
<p>步骤与上面类似，区别在于在对角化的一步使用奇异值分解，</p>
<p><span class="math display">\[
D=Q\Sigma P^T
\]</span>
<img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231204230014035.png" alt="image-20231204230014035" style="zoom:50%;"></p>
<p>对<span class="math inline">\(P\)</span>选择前r大的特征向量（对应特征值大小）构成<span class="math inline">\(P_r\)</span></p>
<p>目标降维矩阵即为 <span class="math display">\[
D^`=DP_r
\]</span></p>
<ul>
<li>如果 D` 是mean-centered，就和PCA基向量一样</li>
<li>一般来说会跳过mean-centered，特别是D稀疏且非负（如文档矩阵）</li>
</ul>
<blockquote>
<p>此处仅使用了右奇异矩阵，就是对样本的列进行了压缩（列数变少了）。而使用左奇异矩阵可以完成对行数的压缩</p>
<p>（来自参考）</p>
</blockquote>
<p>使用左奇异矩阵<span class="math inline">\(D^TQ_r\)</span> describes
items by r latent components</p>
<ul>
<li>可以大幅降维</li>
<li>LSA（潜在语义分析使用）</li>
<li>文档格式的矩阵</li>
<li>减少同义词噪声</li>
<li><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20230920220923499.png" alt="image-20230920220923499" style="zoom:50%;"></li>
</ul>
<p>??noise reduction: truncated SVD tends to correct
inconsistencies??</p>
<h2 id="怎么挑选r">怎么挑选r</h2>
<p>降维后的维度r（主成分个数)</p>
<p>有一个类似于置信度的东西，笔记就不说了</p>
<h2 id="总结">总结</h2>
<p>启发性， Work well only if the underlying assumptions are true.</p>
<h2 id="参考文献">参考文献</h2>
<p>这一篇讲得肯定比我一下午写得明白</p>
<p>https://medium.com/ai%E5%8F%8D%E6%96%97%E5%9F%8E/preprocessing-data-%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90-pca-%E5%8E%9F%E7%90%86%E8%A9%B3%E8%A7%A3-afe1fd044d4f</p>
<h1 id="簇趋势分析-clustering-tendency">簇趋势分析 Clustering
tendency</h1>
<p>（将cluster翻译为簇，将clustering翻译为聚类）</p>
<p>直觉上来说，就是希望可以将数据分为<span class="math inline">\(K\)</span>个簇，每一个簇内的点彼此相似，而不同簇之间的点区别很大：</p>
<h6 id="hard-clustering-每一个点都是一个簇">Hard clustering：
每一个点都是一个簇</h6>
<h6 id="soft-clustering不同的参数可以让一个点属于不同的簇">Soft
clustering：不同的参数可以让一个点属于不同的簇</h6>
<p>聚类的目的是什么：</p>
<ul>
<li>每个簇的形状是固定的还是任意的？</li>
<li>大小均匀还是不一样</li>
<li>密度？</li>
<li>不同簇之间是重叠的还是分得很开？</li>
<li>是否有异常值？</li>
</ul>
<hr>
<p>需要什么：</p>
<ul>
<li><p>距离</p></li>
<li><p>簇间距离（inter-cluster distances）（有时候需要）</p>
<p><strong>簇内距离 intra-cluster distance</strong></p></li>
<li><p>数据在向量空间的表示</p>
<ul>
<li>有时需要，但有时候相似图就够了</li>
</ul></li>
<li><p>评估聚类的评分函数</p></li>
<li><p>簇数<span class="math inline">\(K\)</span></p>
<ul>
<li>一般都要</li>
</ul></li>
</ul>
<hr>
<h2 id="目标函数">目标函数：</h2>
<p>通常结合两个目的：</p>
<p>最小化簇内方差（within-cluster-variation, wc）</p>
<p>最大化簇间方差（between-cluster variation bc)</p>
<ul>
<li><p>最小化簇内方差</p>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20230929011023686.png" alt="image-20230929011023686" style="zoom:50%;"></p>
<p>计算每个簇的中心和簇内个各点的距离总和（其实就是方差，如果<span class="math inline">\(C_i\)</span>是均值,希望簇是紧的-&gt;适用于超球面的簇（大概是圆的吧）</p>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231204231431987.png" alt="image-20231204231431987" style="zoom:50%;"></p>
<p>计算每个簇的最大最小近邻，应对 elongated
cluster（某个方向狭长的簇）</p></li>
<li><p>最大化簇间距离</p>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20230929011309612.png" alt="image-20230929011309612" style="zoom:50%;"></p>
<p>希望每个簇之间都离得很远</p>
<p><strong>用centroid来代表簇</strong></p></li>
<li><p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20230929011414990.png" alt="image-20230929011414990" style="zoom: 50%;"></p>
<p>K-means用的基本就是群内距离和</p></li>
</ul>
<h2 id="聚类趋势">聚类趋势</h2>
<p>不同特征的选择对于聚类结果很重要，所以在预处理的时候，包括特征提取，选择，降维都会影响。</p>
<p>是否要scale呢？</p>
<ul>
<li>一般来说，如果差的很大 最好搞一下</li>
<li>但有的时候会把分离的搞在一起去</li>
</ul>
<p>如何选择特征，了解聚类趋势呢？</p>
<p><strong>这里的聚类趋势指的是一种笼统的，用于聚类前的处理，初步聚类并验证，主要为特征选择服务</strong></p>
<p>方法：</p>
<ul>
<li>对pair-distance 可视化
<ul>
<li>只能当作提示</li>
</ul></li>
<li>滤波方法：
<ul>
<li>基于熵的手段</li>
<li>Hopkins statistic</li>
</ul></li>
<li>Wrapper models 和聚类验证指数
<ul>
<li>average silhouette 平均轮廓</li>
<li><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231204233025389.png" alt="image-20231204233025389" style="zoom:50%;"></li>
</ul></li>
</ul>
<h3 id="可视化检查">可视化检查</h3>
<p>对于距离的分布使用直方图可视化</p>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20230929012222927.png" alt="image-20230929012222927" style="zoom:50%;"></p>
<p>均匀分布的距离一般就是单峰</p>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20230929012243836.png" alt="image-20230929012243836" style="zoom:50%;"></p>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20230929012534464.png" alt="image-20230929012534464" style="zoom:50%;"></p>
<p>多峰就是有更多簇</p>
<p><strong>峰数和簇数没有一一对应关系！</strong></p>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231204233349799.png" alt="image-20231204233349799" style="zoom: 67%;"></p>
<p>这个其实很好推，不会推可以用下面这个当作提示来推</p>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20230929012511164.png" alt="image-20230929012511164" style="zoom:50%;"></p>
<h3 id="基于熵的方法">基于熵的方法</h3>
<p>在随机数据中，熵会很高，但对有簇的数据，熵就低了</p>
<h4 id="计算熵">计算熵</h4>
<p>给了两种计算熵的方法，</p>
<ul>
<li><p>一种是在多维数据之中把数据discretize 成多个 grid
regions，用每个方格中数据的比例计算熵</p>
<p>问题：</p>
<ul>
<li>大维度中p不好准确估计</li>
<li>m（划分的类）不好选，一般选择</li>
</ul></li>
<li><p>第二种是
计算每个数据点的成对距离，再把距离discretize成多个bin，同理计算熵</p></li>
</ul>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231205192116925.png" alt="image-20231205192116925" style="zoom:50%;"></p>
<p>有m个grid，每个维度上就有(<span class="math inline">\(m^{\frac{1}{k}}\)</span>)向上取整个bins</p>
<h4 id="选择特征">选择特征：</h4>
<p>分为前向选择和后向选择，分别是从零每次添加最好的/从全体每次删除最坏的</p>
<ol type="1">
<li><strong>描述的是feature的效果,而非此操作的效果</strong></li>
</ol>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231205192243605.png" alt="image-20231205192243605" style="zoom:50%;"></p>
<h3 id="hopkins-statistic">Hopkins statistic</h3>
<p><strong>用途：检测目标数据中是否有cluster(相比于随机数据）</strong></p>
<p>比较目标数据中子集的，最近邻的数据分布和随机数据产生的对比，计算出一个指标，如果显得目标数据中熵最近邻很小，就说明有簇</p>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231205192455204.png" alt="image-20231205192455204" style="zoom: 33%;"></p>
<p>问题：</p>
<ul>
<li>子集不好选：在数据的中心和边缘分布不一致
<ul>
<li>选择球面内的数据——均值附近能包括超过50%的点</li>
</ul></li>
<li>每次执行不一样
<ul>
<li>多次取平均</li>
</ul></li>
</ul>
<h3 id="wrapper-models-and-validation-indices">Wrapper models and
validation indices</h3>
<p>简单地说，就是先选一点，聚类看结果，然后再多比一比</p>
<h4 id="无监督式">无监督式：</h4>
<p>先cluster，再用一些 internal cluster validity index验证</p>
<ul>
<li>不可能所有都检查：使用greedy heuristic</li>
<li>取决于聚类方法和验证指标</li>
</ul>
<h4 id="有监督式">有监督式</h4>
<p>先对cluster聚类，然后用聚类结果当标签来用</p>
<p>使用 class label 来评估每个特征的好坏（用各种goodness
measures）（不是goodall）</p>
<p>但有点循环定义的问题，感觉没太说清楚</p>
<h1 id="k-representative-聚类">K-representative 聚类</h1>
<p>这章介绍了K-means，和其他几种派生的算法。分别适用于categories/混合数据。还有其他一些有不同的优点。</p>
<p>此外，一个重点是如何运用手段选择<span class="math inline">\(K\)</span>
这些手段中的不少会用在后面的聚类验证中。</p>
<h2 id="聚类介绍">聚类介绍</h2>
<h3 id="适用于-numerical的">适用于 numerical的</h3>
<h4 id="k-means">K-means</h4>
<ul>
<li>适用类型：
<ul>
<li>多维数类（numerical）</li>
<li>compact hyper-spherical形状的簇</li>
</ul></li>
<li>目标：最小化簇内距离平方和
<ul>
<li><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231205220332917.png" alt="image-20231205220332917" style="zoom:50%;"></li>
<li>左边是全体方差，右边分别是簇内方差和簇间方差</li>
<li>左边固定，最小化簇内方差即最大化簇的整体方差</li>
</ul></li>
<li>距离：<span class="math inline">\(L_2\)</span></li>
<li>代表：簇内平均数</li>
<li>优点：
<ul>
<li>如果紧凑，分离好，形状为超球面，那么效果很好</li>
<li>好做</li>
<li>比较高效</li>
</ul></li>
<li>缺点：
<ul>
<li>对初始化敏感，只能找到局部最优
<ul>
<li>一般多跑几次用indice筛</li>
</ul></li>
<li>对outliers敏感</li>
<li>收敛有时候很慢</li>
<li>需要K</li>
</ul></li>
<li>复杂度：
<ul>
<li><span class="math inline">\(O(nKq)\)</span>,q for iterations</li>
</ul></li>
</ul>
<h4 id="k-medians">K-medians</h4>
<ul>
<li><p>距离：<span class="math inline">\(L_1\)</span></p></li>
<li><p>代表：簇内每一维中位数</p></li>
<li><p>优点：</p>
<ul>
<li>more robust to outliers</li>
</ul></li>
<li><p>缺点：</p>
<ul>
<li>计算代价更高（中位数比平均数难找）</li>
</ul></li>
</ul>
<h4 id="k_medoids">K_medoids</h4>
<ul>
<li>距离：都可以</li>
<li>代表：
<ul>
<li>the center-most data point in a cluster，</li>
<li>到簇内其他点的距离之和最小的<strong>数据点</strong></li>
</ul></li>
<li>优点：
<ul>
<li>比较高效（比K-means慢）</li>
<li>more robust to outliers</li>
<li>适用于任意数据类型（能算数据就行）
<ul>
<li>因为不需要捏造一个中心点</li>
</ul></li>
<li>代表是真实的点</li>
</ul></li>
</ul>
<h3 id="适用于-categories的">适用于 categories的</h3>
<h4 id="k-modes">K-modes</h4>
<ul>
<li>目标： 最小化簇内距离
<ul>
<li><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231205221755345.png" alt="image-20231205221755345" style="zoom:50%;"></li>
</ul></li>
<li>距离：
<ul>
<li>如果两个在此特征不相等则为1，否则为0</li>
<li><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231205221836990.png" alt="image-20231205221836990" style="zoom:50%;"></li>
<li>不加权的简单重叠距离</li>
<li><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231205221847434.png" alt="image-20231205221847434" style="zoom:50%;"></li>
</ul></li>
<li>代表：每个特征众数捏造出来的一个mode</li>
<li>过程：
<ul>
<li>初始化随机选择K个初始点，</li>
<li>计算每个其他点的距离，形成cluster</li>
<li>以cluster内每个特征的众数，更新捏造出来一个新的mode</li>
<li>循环</li>
</ul></li>
</ul>
<h3 id="适用于混合数据类型的">适用于混合数据类型的：</h3>
<h4 id="k-prototypes">K-prototypes</h4>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231205222712446.png" alt="image-20231205222712446" style="zoom:50%;"></p>
<p>大概相当于重新给类属性定义了距离和权重的普通kmeans</p>
<h3 id="拓展">拓展：</h3>
<p>对于非凸聚类可以考虑映射到高维空间简单k-means</p>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231205222815583.png" alt="image-20231205222815583" style="zoom:50%;"></p>
<h2 id="选择k的技术">选择K的技术</h2>
<h3 id="sse-elbow">SSE elbow</h3>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231205222908851.png" alt="image-20231205222908851" style="zoom:50%;"></p>
<p>随着k上升，SSE肯定是下降的，但是K太高也失去了意义</p>
<p>大概目测这么一个降速变缓慢的肘部K，只介绍了目测的方法</p>
<h3 id="silhouette-peak">silhouette peak</h3>
<p>选择silhouette最高的K</p>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231205223022991.png" alt="image-20231205223022991" style="zoom:50%;"></p>
<p>单个点的计算过程如下，大概思想是，使用某个点簇内平均距离和最近他簇平均距离来计算，后面还会介绍。</p>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231205223307888.png" alt="image-20231205223307888" style="zoom:50%;"></p>
<p>？这个average silhouette width是啥</p>
<p><em><strong>可以看作是avg si</strong></em></p>
<h3 id="calinski-harabasz-index">Calinski-Harabasz index</h3>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231205223435985.png" alt="image-20231205223435985" style="zoom:50%;"></p>
<p>使用簇间方差和簇内方差倒一下</p>
<p>很适合K-means，因为K-means就是这么弄得</p>
<ul>
<li>值越大越好</li>
</ul>
<h3 id="gap-statistic">Gap statistic</h3>
<p>生成若干随机数据，评估数据的簇内平均对距离</p>
<p>最好的标准比较抽象，选择能满足要求的最小K</p>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231205223738112.png" alt="image-20231205223738112" style="zoom:50%;"></p>
<h1 id="hierarchical-clustering">hierarchical clustering</h1>
<p>这一章讲的脉络比较杂，简要归纳讲述重点大概可以分以下几个：</p>
<ul>
<li>不同的linkage区别</li>
<li>和图论的关系</li>
<li>Agglomerative凝聚和divisive分裂的区别</li>
<li>派生的Bisecting K-means，效果可以集众家之长</li>
</ul>
<p>可能考察点：</p>
<ul>
<li>不同linkage（特别是那两个）对于形状的影响</li>
<li>agglomerative情况下，不同linkage的演绎</li>
<li>可能会考和图论的关系</li>
</ul>
<h2 id="层次聚类">层次聚类</h2>
<p>以层次结构将数据组织成树状结构，逐渐合并/划分簇，最终形成层次树</p>
<ol type="1">
<li>每次选择linkage最近的两个cluster</li>
<li>合并cluster，更新距离矩阵</li>
</ol>
<p>？为什么说顺序会对结果有影响？</p>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231206042416906.png" alt="image-20231206042416906" style="zoom:50%;"></p>
<h2 id="聚类方向">聚类方向</h2>
<h3 id="agglomerative">Agglomerative</h3>
<ul>
<li>自底向上</li>
<li>初始每个cluster为每个数据点</li>
<li>优点：
<ul>
<li>容易实施</li>
<li>比较慢，至少<span class="math inline">\(O(N^2)\)</span>(需要计算所有距离)
<ul>
<li>但是比divisive要快</li>
</ul></li>
</ul></li>
<li>缺点：
<ul>
<li>早期决定由局部特征决定，晚点不能取消
<ul>
<li>局部最优</li>
</ul></li>
</ul></li>
</ul>
<h3 id="divisive">Divisive</h3>
<ul>
<li><p>自顶向下，逐步拆分</p></li>
<li><p>具体怎么实施？好像并不容易——不太好像agglomerative那样，最小生成树拆掉一条最大边的样子</p>
<ul>
<li><p>一种介绍了的可能实施，可以是<strong>Bisecting
K-means</strong></p>
<ol type="1">
<li>把SSE最高的cluster，拆分成两个cluster</li>
<li>运用2-means，迭代<span class="math inline">\(q\)</span>次，选最好的结果</li>
</ol>
<ul>
<li>优点：
<ul>
<li>高效</li>
<li>和层次比效果也不错</li>
</ul></li>
</ul></li>
</ul></li>
<li><p>优点：</p>
<ul>
<li>效果更好
<ul>
<li>因为更早创造大cluster，可以更好考虑全局特征</li>
</ul></li>
</ul></li>
<li><p>缺点：</p>
<ul>
<li>慢，最快<span class="math inline">\(O(N^2logn)\)</span></li>
</ul></li>
</ul>
<h2 id="linkage">linkage</h2>
<p>这个算是hierarchical最tricky的地方：如何衡量cluster之间的距离？</p>
<p>传统的K-representive，使用某个点代表cluster距离，然后继续正常算，但hierarchical中不是这么搞得</p>
<p>不同的linkage度量方法对于结果影响比较大</p>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231206041304570.png" alt="image-20231206041304570" style="zoom:50%;"><strong>最主要说得就是single（最近点距离）和complete（最远点距离）</strong></p>
<ul>
<li>single
<ul>
<li>形状：
<ul>
<li>elongated，拉长的</li>
<li>straggly，稀疏的</li>
<li>concentric cluster? 集中在一点的？</li>
</ul></li>
<li>选择最小距离</li>
</ul></li>
<li>Complete
<ul>
<li>形状：
<ul>
<li>small,小</li>
<li>compact 紧凑的</li>
<li>hyper-spherical 超球面的</li>
<li>equal-sized 大小差不多</li>
</ul></li>
<li>选择最大距离</li>
</ul></li>
</ul>
<p>Single 容易通过链式效应逐步拉长</p>
<p><em>图中的就通过链状噪声包括了一部分别的cluster的</em></p>
<p>但是complete会和旁边的cluster均分噪声</p>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231206041912590.png" alt="image-20231206041912590" style="zoom: 50%;"></p>
<ul>
<li>Average
<ul>
<li>非常紧凑</li>
<li>允许不同大小和密度</li>
</ul></li>
<li>Ward（最小方差）
<ul>
<li>选择最小化合并后簇的方差增量</li>
<li>紧凑，</li>
<li>well-separated 分离的好</li>
<li>超球面的 hyper-spherical</li>
<li>大小相近</li>
<li><strong>不容易拉长</strong></li>
</ul></li>
<li>质心距离
<ul>
<li>相对球球形</li>
<li>大小相近</li>
<li>不拉长</li>
</ul></li>
</ul>
<h2 id="和图论的关系">和图论的关系</h2>
<p>Single linkage类似于联通量，比较像最小生成树</p>
<p>Complete linkage
类似于clique，连出了clique才用最长的代价来合并cluster</p>
<ul>
<li>Single linkage：
<ul>
<li>每次添加一条最短边</li>
<li>联通的分量就合并成cluster</li>
<li>直到形成最小生成树</li>
</ul></li>
<li>Complete linkage：
<ul>
<li>每次添加一条最短边</li>
<li>联通的clique才形成一个cluster</li>
<li>联通clique的最后一条边是最远的代价（没有比它更短的了</li>
<li>直到形成完全连通图</li>
</ul></li>
</ul>
<h2 id="不同的展示方法">不同的展示方法：</h2>
<p>不同的树状图dendrograms来展示：</p>
<ul>
<li>Threshold dendrogram：
<ul>
<li>主要展示顺序</li>
</ul></li>
<li>Proximity dendrogram：
<ul>
<li>增加刻度，显示添加每一个点的代价</li>
</ul></li>
</ul>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231206042738597.png" alt="image-20231206042738597" style="zoom:50%;"></p>
<h1 id="谱聚类-spectral-clustering">谱聚类 Spectral clustering</h1>
<p><em>这个玩意应该不怎么会考。</em></p>
<p>主要讲解的点：</p>
<ol type="1">
<li>如何进行spectral clustering
<ol type="1">
<li>流程是什么</li>
<li>中间有什么选择</li>
<li>相比于其他方法有哪些优点</li>
</ol></li>
<li>如何构建相似图
<ol type="1">
<li>不同的建图方法。</li>
</ol></li>
</ol>
<h4 id="优点">优点：</h4>
<ol type="1">
<li>可以检查到任意形状的簇
<ul>
<li>因为要降维 当前维度形状不影响</li>
</ul></li>
<li>就算不同簇的密度不一样也不怕
<ul>
<li>如果用了KNN来建图</li>
</ul></li>
<li>只要有成对的相似度/距离，任意数据类型都能搞</li>
</ol>
<h4 id="缺点">缺点：</h4>
<ol type="1">
<li>计算代价高</li>
<li>要选择的参数多 不好调</li>
</ol>
<h2 id="spectral-clustering">Spectral clustering</h2>
<h3 id="主旨">主旨</h3>
<ol type="1">
<li>创造一个相似图
<ul>
<li>相似图怎么创建并不容易，一般来说，首先要有所有的点对距离</li>
<li>然后利用距离进行相似度建图，这一步比较有技术含量，不同的建图手段对结果影响很大</li>
</ul></li>
<li>在相似图中，利用多个矩阵将数据呈现在低维空间中
<ul>
<li>这一步是本节主要讲解的知识点</li>
</ul></li>
<li>对新的数据进行聚类（比如说用K-means）</li>
</ol>
<p>这里主要讲解第二步。</p>
<h4 id="名词解释">名词解释：</h4>
<ul>
<li>weight matrix W 记录了每个点到别的店的相似度</li>
<li>diagonal degree matrix Λ
对角阵，记录了每个点所有相似度的求和（加权度）</li>
<li>Laplacian matrix L = Λ −
W，如题，对他求解通过特征向量得到数据的低位表示</li>
<li>normalized Laplacian matrices <span class="math inline">\(L_{rw}\)</span>, <span class="math inline">\(L_{sym }\)</span>if desired)
其他手段得到的拉普拉斯矩阵</li>
</ul>
<h4 id="步骤">步骤：</h4>
<ol type="1">
<li>通过W得到Λ ，再计算L = Λ − W</li>
</ol>
<p><em>对于另外两种拉普拉斯矩阵另有计算方式：</em></p>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231206235240164.png" alt="image-20231206235240164" style="zoom:50%;"></p>
<ol start="2" type="1">
<li><p>我们希望找到<span class="math inline">\(y\)</span>这个向量组作为数据的低纬度嵌入，可以让目标函数最小：</p>
<ul>
<li><p>希望非平凡解：<span class="math inline">\(y_i!=0\)</span></p></li>
<li><p>希望标准化：<span class="math inline">\(y^Ty=1\)</span></p>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231206235527465.png" alt="image-20231206235527465" style="zoom:50%;"></p></li>
</ul></li>
<li><p>通过数学证明可以知道，L的最小的对应非平凡特征向量组成的向量组，满足条件</p></li>
<li><p>对L计算他的特征值，选择它前<span class="math inline">\(k\)</span>个最小特征值对应的特征向量，组成新的矩阵<span class="math inline">\(Y\)</span></p>
<ul>
<li><span class="math inline">\(k\)</span>，降维后的维度</li>
<li>前<span class="math inline">\(k\)</span>个不算0，一般连通图都能算出来一个0.
<ul>
<li>python算出来特别小的也可以认为是浮点数误差导致的</li>
</ul></li>
</ul></li>
<li><p>对新的矩阵Y用K-means聚类</p></li>
</ol>
<h4 id="怎么挑选k">怎么挑选k？</h4>
<p>一般来说<span class="math inline">\(k=K\)</span>(降得维度等于簇数)，或者<span class="math inline">\(k&lt;K\)</span></p>
<p>可以通过这样的Eigengap来选择：k再大就要跃一个台阶了</p>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231207000050697.png" alt="image-20231207000050697" style="zoom:50%;"></p>
<h4 id="三种拉普拉斯矩阵怎么选">三种拉普拉斯矩阵，怎么选？</h4>
<p>一般来说就用unnormalized的，要试试就先试试random-walk</p>
<ul>
<li>其实这里有一个小问题，其实其他的相似度建图方法并不能保证unnormalized
建出来的<span class="math inline">\(L\)</span>对称，但由于他给的应该是mutual-KNN，所以<span class="math inline">\(W\)</span>就是对称的，所以没问题</li>
<li>一般来说其实后两种标准化的可以保证不是对称的W也能建出对称拉普拉斯</li>
</ul>
<h2 id="similarity-graph">Similarity graph</h2>
<h4 id="要求">要求：</h4>
<ul>
<li>应该能反应局部特征（近邻）</li>
<li>对结果影响大</li>
<li>希望相似矩阵<span class="math inline">\(W\)</span>稀疏，但能保证是连通图
<ul>
<li>或者联通分量远小于<span class="math inline">\(K\)</span></li>
</ul></li>
</ul>
<h4 id="步骤-1">步骤：</h4>
<ol type="1">
<li><p>首先计算两个向量的相似度，</p>
<ul>
<li><p>对于numeric，一般用Gaussian
similarity，<img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231207001035688.png" alt="image-20231207001035688" style="zoom:33%;"></p></li>
<li><p>上面那一坨是两个向量距离的平方</p></li>
</ul></li>
<li><p>有了相似度之后，考虑如何得到<span class="math inline">\(w_{ij}\)</span>，这里介绍了四种方法。</p></li>
<li><p>痛点：多个参数，并不好选<img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231207001620663.png" alt="image-20231207001620663" style="zoom:50%;"></p></li>
</ol>
<h4 id="ǫ-neighbourhood">ǫ-neighbourhood</h4>
<p>阈值法，只保留相似度够高的</p>
<ul>
<li>对于密度不同的cluster很难搞</li>
<li>不怎么用</li>
</ul>
<h4 id="k-nearest-neighbour">k-nearest neighbour</h4>
<p>对于每个点，保留他的k个最近邻</p>
<ul>
<li>通常是第一选择</li>
<li>可以把图变成若干联通分量
<ul>
<li>也就是不对称了</li>
</ul></li>
</ul>
<h4 id="mutual-k-nearest-neighbour">mutual k-nearest neighbour</h4>
<p>只有互为k近邻才保留</p>
<h4 id="fully-connected-graph">fully connected graph</h4>
<p>直接放</p>
<p>问题：</p>
<ul>
<li>矩阵并不稀疏——计算代价大！</li>
</ul>
<h3 id="以前的草稿但有例子">以前的草稿，但有例子</h3>
<ul>
<li></li>
</ul>
<blockquote>
<p>它的主要思想是把所有的数据看做空间中的点，这些点之间可以用边连接起来。距离较远的两个点之间的边权重值较低，而距离较近的两个点之间的边权重值较高，通过对所有数据点组成的图进行切图，让切图后不同的子图间边权重和尽可能的低，而子图内的边权重和尽可能的高，从而达到聚类的目的。</p>
</blockquote>
<p>主要关注点之间的距离</p>
<p>所需要的东西：</p>
<ul>
<li>相似图<span class="math inline">\(G\)</span>
<ul>
<li>数据点</li>
<li>边权<span class="math inline">\(w_{ij}\)</span> 两点间的相似性</li>
</ul></li>
</ul>
<p>参考：</p>
<p>https://www.cnblogs.com/pinard/p/6221564.html</p>
<h2 id="图">图</h2>
<p>需要四个矩阵</p>
<ol type="1">
<li>权重矩阵<span class="math inline">\(W\)</span></li>
<li>点度对角阵Λ</li>
<li>拉普拉斯矩阵 <span class="math inline">\(L = Λ − W\)</span></li>
<li>可能需要正则化的拉着普拉斯矩阵</li>
</ol>
<p>做法：</p>
<p>​
<img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20230919165658041.png" alt="image-20230919165658041" style="zoom:50%;"></p>
<p>首先有这么一个相似度（边权）的邻接矩阵<span class="math inline">\(W\)</span></p>
<ul>
<li>无向图</li>
<li>没有权重就0，1</li>
</ul>
<p>然后得到点度对角阵 显示每个点的度（边权的和）</p>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20230919165835601.png" alt="image-20230919165835601" style="zoom:50%;"></p>
<p>然后得到拉普拉斯矩阵 <span class="math inline">\(L = Λ −
W\)</span></p>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20230919170046657.png" alt="image-20230919170046657" style="zoom:50%;"></p>
<p><span class="math inline">\(L\)</span> 的好性质：</p>
<ul>
<li>对称矩阵
<ul>
<li>特征值都是实数（为啥）</li>
</ul></li>
<li>半正定的，最小的特征值等于0</li>
</ul>
<p>最后算出来就是这样的：</p>
<p>可以认为是<span class="math inline">\(k=1\)</span>，结果就非常明显</p>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231207000354898.png" alt="image-20231207000354898" style="zoom:50%;"></p>
<h1 id="聚类验证">聚类验证</h1>
<p>这一章讲的内容主要涵盖了以下几点：</p>
<ol type="1">
<li>不同验证系数的介绍
<ul>
<li>内部系数</li>
<li>外部系数</li>
<li>使用统计中的假设检验</li>
</ul></li>
<li>使用聚类验证手段的目的</li>
<li>渗透一般的验证思想</li>
</ol>
<h3 id="聚类验证的目的">聚类验证的目的</h3>
<ol type="1">
<li>检测选取的特征好不好——看看他们聚类的效果怎么用</li>
<li>检测簇数（或者其他超参数）是否合适</li>
<li>单纯看看聚类效果好不好（goodness of clustering)
<ul>
<li>可以拿来对比聚类方法</li>
<li>和分类相比</li>
</ul></li>
</ol>
<p>这些都取决于聚类的目的</p>
<ul>
<li>包括对于数据的假设，形状等</li>
</ul>
<h2 id="三种聚类评估手段">三种聚类评估手段</h2>
<h4 id="internal-criteria">Internal criteria</h4>
<ul>
<li>验证系数，不需要标签</li>
<li>类似于聚类时的目标函数
<ul>
<li>适合于比较相同的算法来调参</li>
<li>好的聚类在不同的目标（函数）时，也可能评分很差</li>
</ul></li>
<li><strong>很多</strong>更偏好/适用于convex |hyperspherical的形状</li>
<li>对于不同目的的聚类没用（不能评估）</li>
</ul>
<h4 id="external-criteria">External criteria</h4>
<ul>
<li>和预定义的标签比较</li>
<li><em>但是标签不一定总能反应真实的cluster</em></li>
</ul>
<h4 id="statistical-hypothesis-testing">Statistical hypothesis
testing</h4>
<ul>
<li>最可靠，但代价最高</li>
</ul>
<h3 id="internal-criteria-1">Internal criteria</h3>
<h4 id="silhouette-index">Silhouette index</h4>
<p>评估每个点的簇内距离和最近簇平均距离，使用平均Silhouette
index来评估整体聚类效果如何</p>
<ul>
<li>范围：[-1,1]
<ul>
<li>越高越好</li>
</ul></li>
<li>公式：
<ul>
<li><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231206211115821.png" alt="image-20231206211115821" style="zoom:50%;"></li>
</ul></li>
<li>距离：适用于任意距离</li>
<li>例题：
<ul>
<li><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231206211157153.png" alt="image-20231206211157153" style="zoom:50%;"></li>
<li>对于negative
values，意味着一些点的本簇内平均其他点对距离小于最近簇的
<ul>
<li>很可能是分错了</li>
</ul></li>
</ul></li>
</ul>
<h4 id="calinski-harabasz-index-1">Calinski-Harabasz index</h4>
<p>评估簇内方差和簇间方差</p>
<ul>
<li>范围：[0,+∞]
<ul>
<li>越大越好</li>
</ul></li>
<li>公式：
<ul>
<li><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231206211433741.png" alt="image-20231206211433741" style="zoom:50%;"></li>
</ul></li>
<li>需要K&gt;2</li>
<li>偏好：
<ul>
<li>K-means 因为和Kmeans的目标函数一直</li>
</ul></li>
<li>距离：
<ul>
<li>需要用<span class="math inline">\(L_{2}\)</span>距离</li>
</ul></li>
</ul>
<h4 id="davies-bouldin-index">Davies-Bouldin index</h4>
<p>衡量簇内点到质心的距离和簇间距离的比值</p>
<ul>
<li>范围：[0,+∞]
<ul>
<li>越小越好</li>
</ul></li>
<li>公式：
<ul>
<li><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231206211951371.png" alt="image-20231206211951371" style="zoom:50%;"></li>
<li>在这种情况下，对于每个簇，检查和他比较最坏的簇（max)</li>
<li>也可以换成avg</li>
</ul></li>
<li>距离：
<ul>
<li>需要和聚类用一样的<span class="math inline">\(L_p\)</span></li>
</ul></li>
<li>什么时候能等于0？
<ul>
<li>全都是singleton的时候</li>
<li>一种惩罚措施是，对于singleton，给一个惩罚性高代价</li>
</ul></li>
<li>可以用它去检查K</li>
</ul>
<h3 id="external-validation">External Validation</h3>
<p>比较聚类结果<span class="math inline">\(C_i\)</span>和预定义标签<span class="math inline">\(D_i\)</span></p>
<p>主要介绍了标准化的互信息NMI</p>
<p>摆了一个Rand index，没说</p>
<p>捎带介绍了一下purity</p>
<h4 id="purity">purity</h4>
<p>比较弱，检查了对于每个聚类，和它交集最多的class能交多少的和</p>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231206212459521.png" alt="image-20231206212459521" style="zoom:50%;"></p>
<p>肯定会随着K增大而增大</p>
<h4 id="nminormalized-mutual-information">NMI(Normalized mutual
information)</h4>
<p>计算了每个cluster和class的互信息，并用每一个的熵进行了标准化</p>
<ul>
<li>优点：
<ul>
<li>不依赖cluster和标签的数量</li>
</ul></li>
<li>缺点：
<ul>
<li>对于singleton处理的不好</li>
</ul></li>
</ul>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231206220934681.png" alt="image-20231206220934681" style="zoom: 33%;"></p>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231206221119730.png" alt="image-20231206221119730" style="zoom: 50%;"></p>
<p>其中的<span class="math inline">\(P\)</span>可以看作频数，是每个大小的标准化</p>
<h3 id="statistical-hypothesis-testing-1">Statistical hypothesis
testing</h3>
<ul>
<li>过程：
<ul>
<li>选择需要检验的零假设<span class="math inline">\(H_0\)</span>（没有观察到显著的不同）</li>
<li>选择需要检验的统计量<span class="math inline">\(T\)</span>,比如SI</li>
<li>看看当<span class="math inline">\(T=t\)</span>（能够得到我们算出来的统计量）
p值如何？能否拒绝零假设？</li>
</ul></li>
<li>问题在于不知道T的分布
<ul>
<li>一般采用蒙特卡洛实验，多次模拟分布</li>
<li>P值就是随机试验中能够观测到<span class="math inline">\(T=t\)</span>的频率</li>
</ul></li>
</ul>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231206221530091.png" alt="image-20231206221530091" style="zoom:50%;"></p>
<ul>
<li>计算代价大</li>
<li>替代方法多</li>
</ul>
<h2 id="其他方法">其他方法</h2>
<ul>
<li>看看簇的大小：一个很大，一个就几个点——那几个可能是outliers</li>
<li>不同的cluster可能区别于用了不同的特征（大小，或者其他标准）</li>
</ul>
<h3 id="总结-1">总结</h3>
<ul>
<li>验证很重要！
<ul>
<li>就算随机数据也能聚类出来，但难以通过验证</li>
<li>这些指标也可能有偏差，无法反映潜在的聚类</li>
<li>使用多种验证手段</li>
</ul></li>
<li>目标，距离和聚类手段应该吻合</li>
</ul>
<h1 id="association-discovery">Association discovery</h1>
<p>例子：</p>
<ul>
<li>子图</li>
<li>集合</li>
<li>关联规则</li>
<li>序列</li>
<li>片段</li>
</ul>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231010162740226.png" alt="image-20231010162740226" style="zoom:50%;"></p>
<h3 id="好处">好处：</h3>
<ul>
<li>大数据集上可能很好用
<ul>
<li>有时候在np难问题上会有全局优化的解法</li>
<li>可以在短时间内处理大量特征丰富的数据</li>
</ul></li>
</ul>
<p><strong>binarized data</strong></p>
<ul>
<li>应用多</li>
<li>依赖分析通常是数据建模的第一步：帮助选择特征</li>
<li>可以是别的办法的一部分</li>
</ul>
<h2 id="数据">数据</h2>
<p><strong>处理的数据通常是二元的或者是类的（Binarization and
discretization）</strong>**</p>
<p>对于数类的进行类似的处理通常会丢失信息但是也减少了噪声</p>
<p>如何处理数类数据？四种途径：</p>
<figure>
<img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231010163741912.png" alt="image-20231010163741912">
<figcaption aria-hidden="true">image-20231010163741912</figcaption>
</figure>
<p>Association通常的形式如下：</p>
<ul>
<li>set 经常有几个元素会取共同的值</li>
<li>rule 若干元素取若干值 可以推断出其他元素取若干值</li>
</ul>
<p>考虑到可能的取值范围只有0或1，所以就可以更加简化：</p>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231010164733337.png" alt="image-20231010164733337" style="zoom:33%;"></p>
<p>下来先从统计的角度看看</p>
<p>这里给了两个判定标准</p>
<ul>
<li><p><span class="math inline">\(\delta\)</span> leverage</p></li>
<li><p><span class="math inline">\(\gamma\)</span> lift</p></li>
</ul>
<blockquote>
<p>统计小课堂：</p>
<p>事件是某种特定的结果或观察，是离散的，明确定义的，可以计数的，比如
某某干了某某这种</p>
<p>而变量是可以取不同值的属性，如离散变量或连续变量</p>
<p>这里还有互相独立变量 mutually independent 和独立集合 independence
set</p>
</blockquote>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231010170404037.png" alt="image-20231010170404037" style="zoom: 50%;"></p>
<p>这里给出了两个事件独立和变量独立的判定：</p>
<ul>
<li>变量独立要求两个变量能发生的所有事件都独立</li>
</ul>
<p>对于二元变量来说，这两个可以说等价</p>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231012193235494.png" alt="image-20231012193235494" style="zoom: 33%;"></p>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231012193615256.png" alt="image-20231012193615256" style="zoom:33%;"></p>
<p>在这个里面使用了极大似然估计的<span class="math inline">\(P\)</span>（频数除以总数）简化代替了真实的概率P</p>
<p>在这个基础上，我们定义了事件之间相关性的计算</p>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231012172002438.png" alt="image-20231012172002438" style="zoom: 33%;"></p>
<p>以及还有衡量规则强度，即相关性强度的指标</p>
<p>信心程度（confidence，cf，<span class="math inline">\(\phi\)</span>的计算)：条件概率</p>
<p>认为X能推出C，就需要在发生X的情况下发生C的概率很高，大于一个指定阈值</p>
<p>但是高的cf不一定就能保证统计的依赖</p>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231012173013495.png" alt="image-20231012173013495" style="zoom:33%;"></p>
<p>??? 这里很多东西都没看懂</p>
<h1 id="part-2">Part 2</h1>
<p><strong>先找frequent set 再后处理很慢!</strong></p>
<p>在判断找到的关联/规则好不好的时候，我们是应该用leverage <span class="math inline">\(\delta\)</span> 还是用 lift <span class="math inline">\(\gamma\)</span> 呢？</p>
<p>取决于我们想要数据发掘的目的：</p>
<p>我们是想要关于两个变量，<span class="math inline">\(X,
C\)</span>之间的关系，还是说两个变量取得具体的值，<span class="math inline">\(X=1,C=c\)</span>的关系呢？</p>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231017224723456.png" alt="image-20231017224723456" style="zoom: 33%;"></p>
<ul>
<li><span class="math inline">\(\gamma\)</span>
用来衡量值之间的关联</li>
<li><span class="math inline">\(\delta\)</span>
用来衡量变量之间的关联</li>
</ul>
<p>第二个规则只对甜的好使，说明你更关心甜的——什么样能保证是甜的，即使有遗漏。——<span class="math inline">\(\gamma\)</span>更高</p>
<p>而第一个规则更关心变量之间的关系，<span class="math inline">\(\delta\)</span> 更高</p>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231017224947618.png" alt="image-20231017224947618" style="zoom: 33%;"></p>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231017180234281.png" alt="image-20231017180234281" style="zoom:50%;"></p>
<ul>
<li>$$对<span class="math inline">\(1\)</span>这种非常敏感，因为case都非常罕见，就能得到<span class="math inline">\(\gamma\)</span>的最大可能值——n
<ul>
<li>不要单独用<span class="math inline">\(\gamma\)</span></li>
<li>对2来说就会更好一点</li>
<li><span class="math inline">\(\gamma\)</span>更偏好罕见的规则</li>
<li><figure>
<img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231018191804300.png" alt="image-20231018191804300">
<figcaption aria-hidden="true">image-20231018191804300</figcaption>
</figure></li>
</ul></li>
<li><span class="math inline">\(\delta\)</span> 更偏好接近一半的概率：
<ul>
<li><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231018192055624.png" alt="image-20231018192055624" style="zoom:67%;"></li>
<li>看起来 4比3更强，但是算下来的<span class="math inline">\(\delta\)</span> 看着还比3低一点</li>
</ul></li>
</ul>
<p>下面要使用统计的显著性来讲，你已经找到了一个规则
想看看它是不是显著。</p>
<p>比方说使用p值计算，小的就可以认为假设成立</p>
<h2 id="显著性检验">显著性检验</h2>
<h4 id="fishers-exact-p-value">Fisher’s exact p-value</h4>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231018192755723.png" alt="image-20231018192755723" style="zoom: 80%;"></p>
<ul>
<li><strong>越小越好</strong></li>
<li>一般来说，数据集很大的时候<span class="math inline">\(pF\)</span>会很小，所以会算他的对数值来报告，看着更明白</li>
<li>可以看出来，这个就能把4比3强分得很明显了</li>
<li>有时候算这个太麻烦 就用互信息来说了</li>
</ul>
<h4 id="互信息-卡方">互信息&amp; 卡方</h4>
<ul>
<li><ul>
<li>互信息算出来的值和p不一样，但是顺序应该是一样的</li>
<li>越大越好</li>
<li>某种程度上可以用它得到p值</li>
</ul></li>
</ul>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231018194324369.png" alt="image-20231018194324369" style="zoom:80%;"></p>
<p>第三种长得像卡方，但是念 squared measure</p>
<ul>
<li>大的更好</li>
<li>可以从这个分布里找出p值？</li>
<li>不太推荐，因为对于假设是真的非常敏感，如果是半真不假的算出来很差</li>
<li>好处是好算 快</li>
</ul>
<p>还有一个问题是，由于要检验的假说非常多，会让很多假的偶尔通过了检验：</p>
<blockquote>
<p>这种问题被称为 Multiple hypothesis testing
problem，目前仍然有阉阄</p>
</blockquote>
<p>解决方法：</p>
<ul>
<li>放低阈值</li>
<li>别的办法</li>
</ul>
<p>？？虽然这里的阈值放在0.05就有5%凑巧能通过不是很理解</p>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231018201438252.png" alt="image-20231018201438252" style="zoom:80%;"></p>
<h2 id="避免过拟合或冗余的规则">避免过拟合或冗余的规则</h2>
<h3 id="冗余规则">冗余规则</h3>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231018202054468.png" alt="image-20231018202054468" style="zoom:50%;"></p>
<p>如果v-&gt;h强，再加一个无关的S也会强，这是一种继承的联系</p>
<p>如果只知道6
会觉得可能一起吃才坏，但其实都知道了以后会发现V,S可能是条件独立的，甚至S还有点反作用</p>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231018202927258.png" alt="image-20231018202927258" style="zoom: 67%;"></p>
<blockquote>
<p>统计小课堂：条件独立</p>
<p>条件独立是一个概率论和统计学中的概念，用于描述两个随机变量在给定另一个随机变量的条件下是否相互独立。具体来说，随机变量A和B在给定随机变量C的条件下是条件独立的，如果满足以下条件：</p>
<p>P(A, B | C) = P(A | C) * P(B | C)</p>
<p>这意味着在已知C的情况下，事件A和事件B的联合概率等于它们的条件概率的乘积。如果上述等式成立，那么A和B就被称为在条件C下是独立的。</p>
</blockquote>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231018204747685.png" alt="image-20231018204747685" style="zoom:33%;"></p>
<h3 id="虚假规则">虚假规则</h3>
<p>R3 就是</p>
<p>40%吃蛋糕的人也喝了酒，所有人里面有50%的人考挂了，但是吃了蛋糕的考挂有54%——多出来的这些其实是吃了蛋糕的人会喝酒，而喝酒的人更容易挂导致的</p>
<p>这张图里，就是在假定，在A的前提下，C和F是独立的，算了一堆值，最后算出来的fr(CF)是和真的一样的——说明确实独立</p>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231018205516930.png" alt="image-20231018205516930" style="zoom:67%;"></p>
<p>这个以后还会讲，下来说overfitted rule</p>
<h3 id="过拟合规则">过拟合规则</h3>
<p>首先说了
这取决于想要value-based还是variable-based的，前者会比较容易</p>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231018210053244.png" alt="image-20231018210053244" style="zoom:67%;"></p>
<p>当条件更多时，只有频率更高才称为有改善</p>
<ul>
<li>如果一致，说明X,Q独立</li>
<li>如果更低，应该就是负相关（给定X，C和Q负相关？）</li>
<li>但是如果多加了一个条件，只高了一点呢？</li>
<li>就需要显著性检验了（fisher`s P,etc.)</li>
<li>图里的<span class="math inline">\(M_C\)</span>是一个统称</li>
</ul>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231019173633545.png" alt="image-20231019173633545" style="zoom:50%;"></p>
<p>在我们关心值的时候，也就是希望能得到更多正例，可以通过R,B-&gt;S来提高S的概率，相对于R-&gt;S,但是问题在于反例
~（R,B)-&gt;~S 不红不大到甜的概率降低了</p>
<ul>
<li>所以说两个方向都要评估
才知道多加的一个条件，也就是更苛刻的规则是否有改善</li>
</ul>
<hr>
<p>这里学到的就是两个东西：</p>
<ul>
<li>统计的显著性
<ul>
<li>揭示了目前发现的这个规则到底有多强</li>
<li>揭示了再繁复的变量会不会改善规则，即是不是冗余或虚假的</li>
<li>同样对未来的数据好事</li>
</ul></li>
<li>Multiple hypothesis testing problem
<ul>
<li>规则太多，都要验证</li>
</ul></li>
</ul>
<p>在做关联挖掘的时候，首先要决定是干变量的还是值的——决定了验证的手段</p>
<p>考虑筛掉过拟合和虚假关联</p>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231019180942143.png" alt="image-20231019180942143" style="zoom:50%;"></p>
<h2 id="搜索空间剪枝">搜索空间剪枝</h2>
<p>问题：统计关联和显著性不是单调属性</p>
<p><strong>这个measure M 是用来衡量关联的强度或者显著性的</strong></p>
<p>这个measure<span class="math inline">\(M\)</span> 由四部分决定：</p>
<ul>
<li>数据集大小<span class="math inline">\(n\)</span></li>
<li>pattern 的 frequency</li>
<li>条件和结果的frequency</li>
</ul>
<p><span class="math inline">\(M\)</span>可以分为两种：</p>
<ul>
<li>ibg，越大越好，
<ul>
<li><span class="math inline">\(\delta,\gamma,卡方，MI\)</span></li>
</ul></li>
<li>dbg，越小越好：
<ul>
<li><span class="math inline">\(p_F,ln p_f\)</span></li>
</ul></li>
</ul>
<p>就是利用了这些 Goodness measure
进行了筛选和剪枝，只有条件更多结果更好才继续搜下去</p>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231019181154631.png" alt="image-20231019181154631" style="zoom:50%;"></p>
<p><strong>strength 就是δ and γ</strong></p>
<p>剪枝：</p>
<p>在算频繁项集的时候就开始了</p>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231019191023412.png" alt="image-20231019191023412" style="zoom: 67%;"><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231019191040756.png" alt="image-20231019191040756" style="zoom:50%;"></p>
<p>算到AB的时候，就有可能可以把AB以后的全部剪掉：</p>
<p>例如，如果我们的M是越大越好，那么通过某项的概率来简单估算他的上限：</p>
<p>以<span class="math inline">\(\delta\)</span>作为M为例：</p>
<ul>
<li>可以用<span class="math inline">\(P(C)\)</span>,结果的频率来估算上界</li>
<li>可用条件的频率估算上界</li>
<li>可用条件结果的频率估算上界</li>
</ul>
<p>上界不够阈值都可以直接剪枝</p>
<p><strong>如果想要最好的多少多少条，可以随着搜索而更新阈值</strong></p>
<h5 id="冗余剪枝">冗余剪枝:</h5>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231210181333890.png" alt="image-20231210181333890" style="zoom:50%;"></p>
<p>大概是说，有两类评估/搜索方法，</p>
<p>一种是基于值的，<strong>Magnum Opus</strong></p>
<ul>
<li>只评估一个方向</li>
<li>只用leverage和lift</li>
</ul>
<p>一种是基于变量的<strong>Kingfisher</strong></p>
<ul>
<li>要用剩下的统计显著性</li>
</ul>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231019193244353.png" alt="image-20231019193244353" style="zoom:50%;"></p>
<h3 id="kingfisher-algorithm">Kingfisher algorithm</h3>
<ul>
<li>剪枝思想都一样</li>
</ul>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231019193650207.png" alt="image-20231019193650207" style="zoom:50%;"></p>
<p>这边</p>
<ul>
<li>使用p值 做measurement</li>
<li>假设规则都是非冗余的：图里有定义</li>
<li>思路：加速的 branch&amp;bound</li>
</ul>
<blockquote>
<p>branch &amp;bound</p>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231019194440045.png" alt="image-20231019194440045" style="zoom: 50%;"></p>
</blockquote>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231019194349838.png" alt="image-20231019194349838" style="zoom:50%;"></p>
<p>？？ 没看懂这个剪枝在干嘛</p>
<h1 id="graph-mining-web-and-recommender-systems">Graph mining: Web and
recommender systems</h1>
<p>一般来说，把需要mining的Web看成两部分：</p>
<ul>
<li>Web的内容：
<ul>
<li>包括网络结构和网站本身作为文档得内容</li>
</ul></li>
<li>Web附加的内容：
<ul>
<li>浏览记录，行为，排序，日志</li>
</ul></li>
</ul>
<p>对应的就有两类应用：</p>
<ul>
<li>以内容为中心：文档聚类，网页搜索 连接发现</li>
<li>以使用为中心：推荐系统或日志分析</li>
</ul>
<p>非常简单的介绍了Web Search</p>
<h3 id="web-search">Web Search</h3>
<figure>
<img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231121212705032-17005948256241.png" alt="image-20231121212705032">
<figcaption aria-hidden="true">image-20231121212705032</figcaption>
</figure>
<p>经典预处理手段：</p>
<h3 id="inverted-indices">Inverted indices</h3>
<p>反向索引——原来是通过文档能查到词，现在通过这个 就能用词查到文档了</p>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231121213421288.png" alt="image-20231121213421288" style="zoom: 67%;"></p>
<hr>
<p>重点在于排序算法：</p>
<p>排序算法有两种主流的思路：</p>
<ul>
<li>基于内容的评分：
<ul>
<li>使用关键词的出现频率，包括他的加权（是否是标题/字体，相对位置等）</li>
<li>容易被伪造攻击，计算开销应该也大</li>
</ul></li>
<li>基于自然投票的名声评分：
<ul>
<li>厉害的网页应该由很多（厉害的）网页指向</li>
<li>用户反馈/相似的选择</li>
</ul></li>
</ul>
<p>这里介绍的HITS和PageRank都是使用后一种，即在网络层面研究的。</p>
<hr>
<h2 id="排序算法">排序算法</h2>
<h3 id="overview">Overview</h3>
<ul>
<li>HITS
<ul>
<li>Hyperlink-Induced Topic Search（超链接导出主题搜索）</li>
<li>和查询相关</li>
<li>通过包含给定的查询词的网页作为基础，添加他们指向和被指的网页，就此两方面给出评分。</li>
<li>分别给出Authority（谁是好的源头）和Hubs（谁指向好的）</li>
</ul></li>
<li>Pagerank
<ul>
<li>查询无关的</li>
<li>好的网页更可能被其他好网页引用，结构相对简单</li>
</ul></li>
</ul>
<h3 id="hits">HITS</h3>
<ul>
<li>Hub：指向评分</li>
<li>Authority：被指评分</li>
</ul>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231121214418313.png" alt="image-20231121214418313" style="zoom:67%;"></p>
<h4 id="构造方法">构造方法：</h4>
<ul>
<li>因为是查询相关的，首先构造查询涉及的点导出子图
<ol type="1">
<li>首先选出和查询最相关的top-K个网页，作为 root set <span class="math inline">\(R\)</span></li>
<li>添加<span class="math inline">\(R\)</span>中指向的网页，和一部分指向<span class="math inline">\(R\)</span>的网页（比方说每个节点最多50个） 构造出
base set <span class="math inline">\(V\)</span></li>
<li>构造出<span class="math inline">\(G_{sub}=(V,E)\)</span>的点导出子图</li>
</ol></li>
<li>子图中，对每一个计算Hub和Authority值，多轮迭代直至收敛
<ul>
<li><span class="math inline">\(\forall v \in V, h(v) = a(v)=
C\)</span>,初始值满足<span class="math inline">\(\Sigma_i h(v_i)^2 =
\Sigma_i a(v_i)^2 =
1\)</span>,一个常见的初始取值是<img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231121215213683.png" alt="image-20231121215213683" style="zoom:50%;"></li>
<li>迭代以下步骤直至收敛：
<ul>
<li><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231121215258382.png" alt="image-20231121215258382" style="zoom:50%;"></li>
<li>Hub，指向评分= 指向的点的被指评分的和</li>
<li>Authority， 被指评分 = 被指点的指向评分和</li>
<li>对评分进行算数初始化</li>
</ul></li>
<li>一般来说，会关注<span class="math inline">\(a(v_i)\)</span>最高的点</li>
</ul></li>
</ul>
<h3 id="pagerank">PageRank</h3>
<ul>
<li><p>入点越多，入店的访问频率越高，这个点的PR就越高</p></li>
<li><p>评分取决于surfer（半随机游走者）的访问频率，也就是平稳状态概率<span class="math inline">\(\pi\)</span></p></li>
<li><p>最后算的是这么一个东西：<img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231121215813658.png" alt="image-20231121215813658" style="zoom:50%;"></p>
<p>因为其实是马尔科夫链，也有快一点算的办法（换句话说就是不用模拟）</p></li>
</ul>
<h4 id="随机游走模型">随机游走模型：</h4>
<ol type="1">
<li>随机放一个walker在图上某节点</li>
<li>这个walker下一步要移动到另一个点：
<ul>
<li>以<span class="math inline">\(\alpha\)</span>概率乱跳到全图随机一个点上（每个点公平平分）</li>
<li>剩下的<span class="math inline">\(1-\alpha\)</span>中，随机在这个点的出点中找一个转移
<ul>
<li>通常<span class="math inline">\(\alpha=0.1\)</span>,称为
<strong>smoothing or damping probability</strong></li>
</ul></li>
</ul></li>
<li>记录每个点被访问的频数，最后的频率就是PR评分</li>
</ol>
<p>一般来说，这种分量里面的PR会高，关门打狗</p>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231121220224610.png" alt="image-20231121220224610" style="zoom:50%;"></p>
<h4 id="矩阵模型">矩阵模型</h4>
<p>在以上的基础上，把概率矩阵多次迭代，可以避免模拟：</p>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231121222308351.png" alt="image-20231121222308351" style="zoom:50%;"></p>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231121222330328.png" alt="image-20231121222330328" style="zoom:50%;"></p>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231121222416987.png" alt="image-20231121222416987" style="zoom:50%;"></p>
<p>使用收敛阈值：当下降速度够慢，认为已经收敛，返回</p>
<h4 id="另外的pagerank">另外的PageRank：</h4>
<ul>
<li>内容敏感的PageRank：
<ul>
<li>转移的时候仅在相关内容节点之间转移</li>
</ul></li>
<li>SimRank：
<ul>
<li>朴素思想：如果两个点的入点集合相似，那么这两个点相似</li>
<li>好处：纯粹的图论算法，内容无关；可以比较任意两个节点间的相似度，而非PR每个结点的重要性。</li>
<li><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231121223129142.png" alt="image-20231121223129142" style="zoom:50%;"></li>
<li>也可以用随机游走模拟，两个walker分别从点a,b出发，相遇时间的期望函数再对阻尼系数算一下。</li>
<li><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231121223225109.png" alt="image-20231121223225109" style="zoom:50%;"></li>
</ul></li>
<li></li>
</ul>
<h1 id="recommender-systems">Recommender systems</h1>
<p>这一章的主题是根据user，item和user-item
的信息来进行推荐系统，算是一种和文档挖掘，图挖掘相关的，应用层面的知识。</p>
<p>推荐主要可以分为两类：</p>
<ul>
<li>Collaborative Filtering:
<ul>
<li>主旨是通过user/item相似性，进行的预测来进行推荐</li>
<li>相对复杂，高级，抽象</li>
</ul></li>
<li>Content-Based Filtering
<ul>
<li>主旨是分析每个节点的性质来配对</li>
<li>比较Naive</li>
</ul></li>
</ul>
<p>重点放在第一种上。</p>
<h2 id="手里的数据">手里的数据</h2>
<p>主要分为三类：</p>
<ul>
<li><p>user profile</p>
<ul>
<li>包括购买历史，隐式或显式的兴趣</li>
</ul></li>
<li><p>Item profile：</p>
<ul>
<li>文字描述，关键词</li>
</ul></li>
<li><p>Utility matrix</p>
<ul>
<li>描述了每个用户（row）对每个产品（column）的兴趣</li>
<li>分为两类
<ul>
<li>一种是只有positive/null的，表明看过/买过</li>
<li>一种是有评分的，喜不喜欢</li>
<li>用的时候差别不大</li>
<li><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231122222929872.png" alt="image-20231122222929872" style="zoom:50%;"></li>
</ul></li>
</ul></li>
</ul>
<h2 id="content-based-filtering">Content-Based Filtering</h2>
<p>这个比较简单：</p>
<ul>
<li>如果手里没有矩阵：
<ul>
<li>直接变成了文本问题，从item中找到K个最合适user的,
例如使用tf-idf+cos</li>
<li>感觉word embedding也能干</li>
</ul></li>
<li>如果有矩阵：
<ul>
<li>就变成了一个预测问题</li>
<li>根据矩阵的形式（二元或numerical）来训练一个分类器或逻辑回归</li>
<li>问题在于一般来说训练集都很小，容易做出同质化推荐</li>
</ul></li>
</ul>
<h2 id="collaborative-filtering">Collaborative filtering</h2>
<p>主流思想是考虑其他的样本关系，甚至整个数据集来做出预测，不太考虑预测样本本身的特质。</p>
<p>主要分为四类，但详细讲了前两类。</p>
<ul>
<li><ol type="i">
<li>Neighbourhood-based</li>
</ol>
<ul>
<li>找到相似的用户/项目，根据别的来预测这个的</li>
</ul></li>
<li><ol start="2" type="i">
<li>Graph-based</li>
</ol></li>
<li>根据评分关系建图，利用图上计算的指标来推荐</li>
</ul>
<p>后两种没太看懂</p>
<ol start="3" type="i">
<li><p>Clustering-based</p></li>
<li><p>Latent factor -based</p></li>
</ol>
<h3 id="neighbourhood-based">Neighbourhood-based</h3>
<p>分为两种，item-based和user-based</p>
<h4 id="user-based">user-based</h4>
<ol type="1">
<li>计算用户的相似性
<ul>
<li>每个用户被表示为评分的向量</li>
<li>可以通过类似Pearson correlation coefficient 来评分相似性</li>
<li>只考虑都有评分的项目</li>
</ul></li>
<li>找到可用辅助预测的邻居
<ul>
<li>找到K个最近邻</li>
<li>移除相似度小于阈值的</li>
<li>对某邻居，每一个评分标准化（减去均值）</li>
<li>最终目标用户的项目预测得分，是相似性加权平均后，该项得分，再加上目标用户其余评分的均值。</li>
<li><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231122224656345.png" alt="image-20231122224656345" style="zoom:67%;"></li>
</ul></li>
</ol>
<p>例子：</p>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231122224713844.png" alt="image-20231122224713844" style="zoom: 67%;"><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231122224722045.png" alt="image-20231122224722045" style="zoom: 67%;"></p>
<h4 id="item-based">item-based：</h4>
<p>类似的，相当于矩阵经过了一次转置——找到若干个相似的item，根据该用户对他们的评分预测此item的分数</p>
<hr>
<p>有哪些相似性度量可以用在这里？</p>
<ul>
<li>Pearson Correlation， 当然可以用，顺便也能用mean centreing</li>
<li>调整过的 cosine similarity:
<ul>
<li>用不用mean-centering 是个问题，因为0向量不好处理</li>
<li>只对所有值都是正的才好用</li>
</ul>
<h3 id="graph-based">Graph-based</h3></li>
</ul>
<p>建图：如果user_i对item_j有评价，就连一条边，最后的图，<span class="math inline">\(V={U,I}\)</span></p>
<ul>
<li>边(可以是有权的，也可以对其进行标准化，不过如果标准出了负权图就很麻烦）</li>
</ul>
<p>课上只介绍了简单情况:只考虑无权边</p>
<h4 id="只用g来找最近邻">只用G来找最近邻</h4>
<p>用G来计算Pagerank/SimRank，以此作为相似性衡量标准</p>
<p>有了相似性之后就和前面的neighbor-based没区别了</p>
<h4 id="考虑pr-值的推荐">考虑PR 值的推荐</h4>
<p>当PR的随机跳转到user_i时，给他最大PR的item</p>
<p>当PR的随机跳转到item_j时，把他推给最大的user</p>
<p><span class="math inline">\(\alpha\)</span> 的影响：</p>
<ul>
<li>小的话，就总是推荐流行的item
<ul>
<li>都是靠随机游走走过去的</li>
</ul></li>
<li>大的话，每个用户都会比较specific
<ul>
<li>因为随机性强？</li>
</ul></li>
</ul>
<p>？这里没太搞懂</p>
<h3 id="clustering-based">Clustering-based</h3>
<p>目标：希望能够通过聚类提前知道相似的user/item community</p>
<p>问题：数据过于稀疏</p>
<p>解决：改进K-means，对都有评价的子集做</p>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231122230614477.png" alt="image-20231122230614477" style="zoom:67%;"></p>
<h3 id="latent-factor--based">Latent factor -based</h3>
<p>只简单提了一下，但越来越流行</p>
<p>目标：通过两个低维向量乘积来大概表示utility matrix</p>
<p>手段：
将每个user/item表示为一个向量，想要知道他们的评分乘一下就知道了</p>
<p>​ 可以用调整的SVD做这个事</p>
<p><strong>会丢失信息</strong></p>
<h1 id="section"><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231122231032224.png" alt="image-20231122231032224" style="zoom:50%;"></h1>
<h1 id="mining-database-of-multiple-graphs">Mining database of multiple
graphs</h1>
<p>这章主要讲了如何评估图之间的距离，在不同距离基础上进行对图的聚类。</p>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231122235648191.png" alt="image-20231122235648191" style="zoom:80%;"></p>
<p>重要的子知识点包括</p>
<ul>
<li>图的同构，最大相似子图</li>
<li>频繁子图</li>
<li>图的表示</li>
</ul>
<h2 id="图的同构">图的同构：</h2>
<p>如果两个图是同构的（isomorphic）
那么他们对应的点应该有对应的边，匹配关系被表示为：</p>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231122231802113.png" alt="image-20231122231802113"> <span class="math display">\[
M = { ( v, u ) | v ∈ V, u ∈ U, u = f ( v ) }
\]</span></p>
<ul>
<li>当有标签时，对应点的标签相同</li>
</ul>
<p>在此基础上有了<strong>子图同构(Subgraph
isomorphism)</strong>,要求一个图经过映射可以称为另一个的子图，讨论范围只考虑联通的情况：</p>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231122231934349.png" alt="image-20231122231934349" style="zoom:50%;"></p>
<p>？暂时不知道最下面的条件为什么更弱</p>
<p>在此基础上有了<strong>最大相似子图Maximum common subgraph
(MCG)</strong>：</p>
<ul>
<li><span class="math inline">\(G_0\)</span>是<span class="math inline">\(G_1,G_2\)</span>的同构子图</li>
<li>点数最多</li>
</ul>
<p>特点：</p>
<ul>
<li>可以用来判断距离/频繁子图挖掘</li>
<li>只有NP-HARD的解法，基本就是穷举：</li>
</ul>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231122232256067.png" alt="image-20231122232256067" style="zoom:50%;"></p>
<ul>
<li>应该能看懂吧?核心就是那一句递归 类似于DFS的动规了</li>
</ul>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231122232313041.png" alt="image-20231122232313041" style="zoom:50%;"></p>
<ul>
<li>判断新加入的是否合理,就是看是否对于已经加入映射关系的<span class="math inline">\(v2,u2\)</span>,新的映射关系能够拓展的一条边在另一个图中不存在</li>
</ul>
<h2 id="图的距离">图的距离：</h2>
<p>两个思路，一种是通过图的匹配来看（MCS,最小编辑距离），另一种是将图变形（更像是嵌入）之后来算。</p>
<h3 id="图的匹配">图的匹配</h3>
<h4 id="mcs-based">MCS-based：</h4>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231122232539597.png" alt="image-20231122232539597" style="zoom:50%;"><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231122232555248.png" alt="image-20231122232555248" style="zoom:50%;"></p>
<p>基本就是考虑他们没有在MCS中的点数和，以及可能进行标准化</p>
<p><strong>只对以下条件适用：</strong></p>
<ul>
<li>小图，不然效率低</li>
<li>大小差距不大，要不然MCS影响不大</li>
</ul>
<h4 id="最小编辑距离">最小编辑距离：</h4>
<p>不同操作代价不同，根据应用定义</p>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231122232727124.png" alt="image-20231122232727124" style="zoom:50%;"></p>
<h3 id="transformation-based">Transformation-based</h3>
<p>Idea：在新的空间中表示</p>
<h4 id="频繁子图表示法">频繁子图表示法</h4>
<p>找到图里的若干频繁子图，通过这些频繁子图的频数形成向量<span class="math inline">\(F={f_1,...,f_d}\)</span>来表示图</p>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231122232913846.png" alt="image-20231122232913846" style="zoom: 67%;"></p>
<p>？为什么是text-similarity</p>
<p>？ 为什么dont overlap too much</p>
<h4 id="拓扑标识符">拓扑标识符</h4>
<p>从图中计算若干指数作为新的特征，从此计算距离：</p>
<p>例子：</p>
<p>计算图中所有最短路的和（感觉很烂）</p>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231122233235261.png" alt="image-20231122233235261" style="zoom:67%;"></p>
<ul>
<li>丢失了结构信息</li>
<li>需要领域知识</li>
</ul>
<h4 id="核方法相似性">核方法相似性：</h4>
<p>将图投射到 Hilbert
空间（欧几里得空间是他的子集），两个图的点积就是他们的相似性</p>
<p>（应该知道就行了）</p>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231122233542565.png" alt="image-20231122233542565" style="zoom:50%;"></p>
<h2 id="频繁子图发掘">频繁子图发掘</h2>
<p>在距离和聚类都有应用</p>
<p>基本类似于前面学过的那个，用了个GraphApriori 找频繁集</p>
<p>？后面学会了在这补上</p>
<h2 id="图聚类">图聚类</h2>
<h4 id="基于距离的">基于距离的：</h4>
<ol type="1">
<li>K-medoids（几乎就是Kmeans）</li>
<li>Spectral and other graph-based
methods（层次聚类，或者别的图算法？）</li>
</ol>
<p>（用距离的都能用）</p>
<p><strong>图距离计算开销大，一般小图才这么干</strong></p>
<h4 id="基于频繁子图的">基于频繁子图的：</h4>
<ul>
<li><p>使用频繁子图，将图表示为向量，跟上面算距离的思路一样</p></li>
<li><p>第二种方法：</p>
<p>？没看懂（录像这块没录到）</p>
<p>大概也就是一种检测共同子图频数来做的分类，不断迭代</p>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231122235224832.png" alt="image-20231122235224832" style="zoom:50%;"></p></li>
</ul>
<h1 id="overview-of-social-network-analysis">Overview of social network
analysis</h1>
<p>如题所示，这节课确实就是个overview</p>
<p>提出了以下四个任务，主要还是在图聚类的基础上做Community detection</p>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231122235758023.png" alt="image-20231122235758023" style="zoom:50%;"></p>
<h2 id="social-influence-analysis">Social influence analysis</h2>
<p>这个研究的有两个问题，</p>
<ul>
<li>如何衡量每个点的影响力：
<ul>
<li>无向图中叫centrality</li>
<li>有向图叫prestige</li>
</ul></li>
<li>影响传播或扩散模型
<ul>
<li>在边权的基础上，考虑一些点的影响力</li>
<li>找到能最大化影响的seed set</li>
<li>提了一下</li>
</ul></li>
</ul>
<p>关于点的影响力，给了以下若干指标：</p>
<ul>
<li>Degree centrality
<ul>
<li>基本就是度的标准化</li>
</ul></li>
<li>Closeness centrality:
<ul>
<li>衡量某节点到其他所有节点最短距离的逆（倒数）</li>
</ul></li>
<li>Betweenness centrality：
<ul>
<li>衡量某节点在多少最短路上</li>
</ul></li>
</ul>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231123200822701.png" alt="image-20231123200822701" style="zoom:50%;"></p>
<h2 id="community-detection">Community detection</h2>
<p>目标：找到若干切边，能够形成分组，使得切边代价和最小</p>
<p>一般情况是NP-HARD</p>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231123200945775.png" alt="image-20231123200945775" style="zoom:50%;"></p>
<p>方法：</p>
<ul>
<li><p>层次聚类：(Spectral clustering)</p></li>
<li><p>Kerninghan-Lin</p>
<ul>
<li>用于平衡的二分图（两组点数尽可能接近）</li>
<li>每次迭代，交换一对点，将能给出最大改进的作为目标</li>
</ul></li>
<li><p>Girwan-Newman algorithm</p>
<ul>
<li>移除 bridge edges</li>
<li>通常考虑betwenness最高的边</li>
<li>有点像迪杰斯特拉？</li>
</ul></li>
<li><p>METIS algorithm：</p>
<ul>
<li>通过将紧密相连的点合并，在更简单的图上再划分</li>
<li>划分完再恢复回去</li>
<li><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231123201500373.png" alt="image-20231123201500373" style="zoom: 67%;"></li>
</ul></li>
</ul>
<h2 id="链接预测与点的相似性">链接预测与点的相似性</h2>
<p>主要有若干思路：</p>
<ul>
<li>分析点的相似性来做：
<ul>
<li>计算代价低</li>
</ul></li>
<li>训练分类器来看这个边应该有否：
<ul>
<li>代价高，但更准</li>
</ul></li>
<li>使用缺失值预估法，类似于矩阵分解为向量相乘</li>
</ul>
<p>下面介绍了两种点相似性的算法：</p>
<ol type="1">
<li><p>使用公共邻居来衡量：</p>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231123202029666.png" alt="image-20231123202029666" style="zoom:50%;"></p></li>
<li><p>使用随机游走-基础的方法：</p></li>
</ol>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231123202122036.png" alt="image-20231123202122036" style="zoom:50%;"></p>
<p>？ 第一个没看懂</p>
<ul>
<li>SimRank</li>
<li>Katz-measure：
<ul>
<li>两个点之间通路越多，越相似</li>
<li>更长的通路衰减的更狠</li>
</ul></li>
</ul>
<h1 id="text-mining">Text mining</h1>
<p>讲的东西主要分以下几部分：</p>
<ul>
<li>预处理</li>
<li>词的表示
<ul>
<li>tf-idf</li>
</ul></li>
<li>文本聚类和应用
<ul>
<li>算是重点</li>
</ul></li>
</ul>
<p>还顺便提了一下 word -embedding</p>
<p>基本概念：</p>
<ul>
<li>Corpus，语料，可以认为是文档集合</li>
<li>Lexicon，词典，词的集合</li>
</ul>
<p>文档本来是有序的词序列，但</p>
<ul>
<li>经过bags of words 模型处理，就变成了词和对应频数
<ul>
<li>, e.g., Our cat likes the neighbour’s cat. → {our: 1, cat: 2, likes:
1, the: 1, neighbour’s: 1 }</li>
</ul></li>
<li>再下改良空间中，用一个向量表示一个文档
<ul>
<li>非常稀疏</li>
<li>出现的词会更重要一点</li>
</ul></li>
</ul>
<h2 id="预处理-1">预处理：</h2>
<p>一般来收，这些预处理步骤非常naive</p>
<p>通常的步骤：</p>
<ul>
<li>Tokenization
<ul>
<li>通常就是word=toekn</li>
</ul></li>
<li>清洗，合并：
<ul>
<li>Lower-casing</li>
<li>移除 stopwords，（考虑转小写的顺序）</li>
<li>合并为本身的形式：
<ul>
<li>stemming——移除词缀，去掉ing等</li>
<li>Lemmatizaton——通过词典移除派生词，对付was这种</li>
</ul></li>
<li>去除标点
<ul>
<li>考虑数字怎么办</li>
<li>复合词中的破折号 dash</li>
</ul></li>
</ul></li>
<li>Stopwords：
<ul>
<li>缺乏信息，多次出现的词</li>
<li>考虑上下文，可能需要调整</li>
</ul></li>
<li>stemming
<ul>
<li>可能把多义词搞坏</li>
<li>也可能没检测到(e.g., alumnus → alumnu vs. alumni → alumni)</li>
<li>rule-based，可能会查表</li>
</ul></li>
<li>Lemmatization：
<ul>
<li>考虑上下文，词性标注和查表</li>
<li>更慢，更准</li>
</ul></li>
</ul>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231124205116146.png" alt="image-20231124205116146" style="zoom: 67%;"></p>
<center>
一个经过不同处理的例子
</center>
<h2 id="表示">表示</h2>
<p>这里研究的都是文档级别的表示</p>
<ol type="1">
<li>频数模型：向量的每个分量就是词在对应文档中的频数，可能还正则化了</li>
<li>二元；只考虑出现与否</li>
<li>TF-IDF表示</li>
</ol>
<p>这里主要介绍TF-IDF</p>
<h3 id="tf-idf">Tf-idf</h3>
<p>TF: term-frequency</p>
<p>IDF：Inverse document frequency</p>
<p>某个词在文档中的词频乘以含有其文档的频率的倒数</p>
<p>最常见的计算方法：</p>
<figure>
<img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231124210008896.png" alt="image-20231124210008896">
<figcaption aria-hidden="true">image-20231124210008896</figcaption>
</figure>
<p>文档词频越高，出现的文档越少，值就越高</p>
<p>具体计算方法取决于实现</p>
<p>常见的其他实现：</p>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231124210112067.png" alt="image-20231124210112067" style="zoom: 67%;"></p>
<h3 id="词是很好的特征吗">词是很好的特征吗？</h3>
<p>问题：</p>
<ul>
<li>单个单词可能只是有用信息的一部分：
<ul>
<li>n-gram来看</li>
</ul></li>
<li>频率过高的词区分能力不足
<ul>
<li>停用词来帮忙</li>
</ul></li>
<li>独特或罕见的词看不出来相似性
<ul>
<li>多义词/同义词</li>
<li>用Wordnet/embedding LSA（潜在语义分析来搞）</li>
<li>拼写错误或英美区别</li>
</ul></li>
<li>多义词或同义词</li>
</ul>
<p>可以考虑用N-gram搞：</p>
<p>规定gram长度？</p>
<p>会导致很多特征，还需要过滤一下：</p>
<pre><code>* gram 频率
* 卡方，互信息，LSA</code></pre>
<p>来筛出需要的gram</p>
<blockquote>
<p>LSA是什么？ 就是text上的SVD，用低位向量表示高维矩阵</p>
<p>对于同义词有用，多义词有一点用（不太显著吧）</p>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231124212907723.png" alt="image-20231124212907723" style="zoom:67%;"></p>
</blockquote>
<h2 id="聚类">聚类:</h2>
<p>在得到了文档的向量空间表示后，其实一般的聚类方法都能试试。</p>
<ul>
<li>距离一般用cosien.如果两个向量都是标准化过的（范数为1），那么他们的<span class="math inline">\(L_2范数\)</span>和cos距离有一下关系：
<ul>
<li><figure>
<img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231124214911673.png" alt="image-20231124214911673">
<figcaption aria-hidden="true">image-20231124214911673</figcaption>
</figure></li>
</ul></li>
<li>PCA/LSA可以试着搞一下</li>
<li>K-means可以作为baseline</li>
</ul>
<p>介绍了许多莫名其妙的技巧：</p>
<ul>
<li>使用cluster digests-最高频词
来代替representative，更好地显示cluster的属性</li>
<li>聚类时，对每个cluster额外维护一个高频词的属性</li>
</ul>
<p>下来还讲了一个技巧用于给text找更好的种子（然后K-MEANS）</p>
<h3 id="scattergather">Scatter/Gather</h3>
<p>找种子：</p>
<ol type="1">
<li><p>Buckshot：</p>
<p>不在整个上找种子，而是先对部分样本聚类成K个，K个质心作为种子</p>
<ul>
<li>快</li>
</ul></li>
<li><p>Fractionation</p>
<p>更好地把握局部特征：</p>
<p>先将数据分为若干个桶，桶内若干文档分为若干cluster，每个cluster合并成为一个更大的文档，以此重复直到只剩K个cluster，他们的centroid作为seed</p>
<ul>
<li>质量更好</li>
<li>为什么还要再跑一边Kmeans？这样考虑全局，避免引入随机分桶的噪声</li>
</ul></li>
</ol>
<p>下来跑一遍K-MEANS</p>
<p>精炼：</p>
<ol type="1">
<li><p>检测：</p>
<p>如果平均相似分太低（和质心的或者两两之间的）
就掰开，重新聚类</p></li>
<li><p>合并：</p>
<p>主题词显著重叠，就合并</p></li>
</ol>
<h3 id="其他方法-1">其他方法：</h3>
<p>可以这么简单重排，</p>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231124235509749.png" alt="image-20231124235509749" style="zoom:50%;"></p>
<p>或者像二分图求最小割一样</p>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231124235554911.png" alt="image-20231124235554911" style="zoom:50%;"><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231124235605069.png" alt="image-20231124235605069" style="zoom:50%;"></p>
<h3 id="应用">应用：</h3>
<ol type="1">
<li>分类</li>
</ol>
<p>对于新文档来说，找到他的K个最近邻digests（每个clusterd都有若干digests），将其主导的标签作为其标签就行了</p>
<p>？为什么说这比KNN要快</p>
<p>？以及可以处理同义词和多义词</p>
<ol start="2" type="1">
<li><p>创新检测：</p>
<p>维持若干文档clusters
的时间戳，对新文档计算所有的（cosine）相似度（和每个cluster digest）</p>
<p>如果能加入某个cluster（相似度大于阈值），那么更新cluster及其时间戳</p>
<p>如果不能，就把他当作最新的一个cluster并移除一个最老的cluster</p>
<p>感觉可以被特定的序列攻击？</p></li>
</ol>
<h2 id="额外知识">额外知识：</h2>
<ul>
<li><p>Word embedding：</p>
<p>词向量的距离可以反映语义相似度</p>
<ul>
<li><p>常用方法：</p>
<ul>
<li><p>矩阵分解：</p>
<p>类似SVD,分解word co-occurrence matrix</p></li>
<li><p>Word2vec:</p>
<p>使用隐式神经网络的权重</p>
<ul>
<li>CBOW： 用上下文预测词</li>
<li>Skip-gram：用词预测上下文</li>
</ul></li>
<li><p>Word2vec:</p>
<ul>
<li>需要大语料，所以像tf-idf从零开始不太能</li>
<li>但是好处是可以迁移</li>
</ul></li>
<li><p>Glove：</p>
<ul>
<li>需要的预料更少，更快</li>
<li><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231128152317013.png" alt="image-20231128152317013" style="zoom:50%;"></li>
</ul></li>
</ul></li>
</ul></li>
</ul>
<h2 id="summary">Summary：</h2>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231128152346894.png" alt="image-20231128152346894" style="zoom:50%;"></p>
<h1 id="data-randomization-for-assessing-the-results">Data randomization
for assessing the results</h1>
<ul>
<li>零假设（null hypothesis）</li>
</ul>
<h1 id="专题">专题</h1>
<h2 id="名词解释类问题">名词解释类问题：</h2>
<ul>
<li>Data cleaning:</li>
</ul>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231201210715587.png" alt="image-20231201210715587" style="zoom:67%;"></p>
<p>covariance： 协方差</p>
<p>matrices：矩阵（复数）</p>
<p>singular vectors 奇异向量</p>
<p>eigenvector 特征向量</p>
<p>orthogonal basis vectors 正交基向量</p>
<p>Entropy 熵</p>
<p>singleton 单独的，单点成簇</p>
<h3 id="数据类型">数据类型：</h3>
<ul>
<li>numerical</li>
<li>categorical</li>
<li>bianry</li>
<li>graph</li>
</ul>
<h2 id="计算复杂度">计算复杂度</h2>
<ul>
<li>如果写的很傻逼 多项式算法也可能很烦：（例如每次都重复算距离）</li>
</ul>
<p>？ 这里再重复统计一下常见算法的复杂度</p>
<ul>
<li>hierarchical(On2) spectral(On3,Kmeans(OnKq))</li>
<li>memory可能也是问题</li>
<li>需要对计算量有预估</li>
</ul>
<h2 id="generic-apriori">Generic Apriori</h2>
<p><strong>这可能是最有名的/常见的DM算法</strong></p>
<h2 id="不同的指标和使用情景">不同的指标和使用情景</h2>
<ul>
<li>leverage</li>
<li>lift</li>
<li>MI</li>
</ul>
<h1 id="卷子的知识点">卷子的知识点</h1>
<ul>
<li>选择相似图/cluster的距离阈值</li>
</ul>
<h3 id="association-mining相关内容">Association mining相关内容</h3>
<ul>
<li><p>one-item set</p>
<ul>
<li>相关概念</li>
<li>展现为trasaction形式</li>
</ul></li>
<li><p>Apriori</p>
<ul>
<li>会模拟</li>
<li>直到最小频率</li>
<li>展示candidate</li>
<li>频繁集</li>
<li>enumeration tree</li>
</ul></li>
<li><p>为什么有的时候剪枝可以不用数</p></li>
<li><p>不同的集合</p>
<ul>
<li>maximal</li>
<li>closed</li>
<li>0-free</li>
</ul></li>
<li><p>对于给定集合的指标计算</p>
<ul>
<li>confidence</li>
<li>leverage</li>
<li>lift</li>
<li>nMI给出算式要会算</li>
</ul></li>
<li><p>怎么通过上面那些值找候选规则之间的正向统计依赖</p></li>
<li><p>怎么看一个规则是significant的</p>
<ul>
<li>怎么通过指标看看一个是不是要被剪枝掉</li>
</ul></li>
<li><p>知道一般筛选规则的步骤</p>
<ul>
<li>先用什么在用什么</li>
</ul></li>
</ul>
<h1 id="questions">questions</h1>
<p>w为什么说最下面的这是个更弱的条件</p>
<p><img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231122201721586.png" alt="image-20231122201721586" style="zoom: 50%;"></p>
<p>这里的图apriori暂时看不太懂 回头看看apriori再继续搞</p>
<figure>
<img src="/2023/09/16/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%20CSE4650-Methods-of-Data-Mining/image-20231122212220148.png" alt="image-20231122212220148">
<figcaption aria-hidden="true">image-20231122212220148</figcaption>
</figure>
]]></content>
      <categories>
        <category>Substance</category>
      </categories>
      <tags>
        <tag>Note</tag>
        <tag>Course</tag>
      </tags>
  </entry>
  <entry>
    <title>Usage of OCL</title>
    <url>/2022/07/20/Usage-of-OCL/</url>
    <content><![CDATA[<p>硕士论文</p>
<p>Usage Analysis of the Object Constraint Language in Model Driven
Engineering</p>
<p>的阅读笔记</p>
<p>和OCL总结性质的教程</p>
<span id="more"></span>
<h1 id="论文笔记">论文笔记</h1>
<p>本篇是某硕士论文</p>
<p>Usage Analysis of the Object Constraint Language in Model Driven
Engineering</p>
<p>的阅读笔记 ，主要目的在于总结OCL的特点及用途</p>
<p>后面还有一点OCL总结性质的教程</p>
<h2 id="abs">ABS</h2>
<p>在MDE中，元模型和模型转换很重要</p>
<p>OCL用来给元模型表达操作和约束</p>
<p>目前的OCL经验研究都在小数据集上，我们收集了100k的数据集，研究人们的用途和代码的不同。研究手段包括：表达式的不同分布，表达式的复杂度与常用的结构</p>
<p>还研究了可维护性</p>
<h2 id="section">1</h2>
<h3 id="intro">INTRO</h3>
<p>OMG利用OCL在许多其他语言的规格中进行标准化</p>
<h3 id="mde">MDE</h3>
<p>MDE将系统的抽象程度从代码提升到模型</p>
<p>OMG提出 MOF ，Meta Object Facility，元对象机制作为建模语言的标准</p>
<h3 id="ocl">OCL</h3>
<p>对模型有限制：比如某个属性只能是大写的</p>
<p>这就是对OCL的需求</p>
<p>是文本语言可以用来写宣告性表达</p>
<p>约束模型的属性或者关系</p>
<p>作为UML的标准，最开始的目标是克服系统设计中的指定细节的不足</p>
<p>也被和model transformation languages 一起使用</p>
<p><img src="/2022/07/20/Usage-of-OCL/image-20220720222814111.png" alt="image-20220720222814111" style="zoom: 33%;"></p>
<p><img src="/2022/07/20/Usage-of-OCL/image-20220720222107056.png" alt="image-20220720222107056" style="zoom:33%;"></p>
<p>context：self指的实例，也就是说哪一个类</p>
<p>Inv：这一段是个不变量，必须为真，要不然模型的状态就是无效</p>
<p>=用于检查而非赋值</p>
<p><code>self</code> 是否可以省略？</p>
<p>可以</p>
<figure>
<img src="/2022/07/20/Usage-of-OCL/image-20220720222756050.png" alt="image-20220720222756050">
<figcaption aria-hidden="true">image-20220720222756050</figcaption>
</figure>
<figure>
<img src="/2022/07/20/Usage-of-OCL/image-20220720223004101.png" alt="image-20220720223004101">
<figcaption aria-hidden="true">image-20220720223004101</figcaption>
</figure>
<figure>
<img src="/2022/07/20/Usage-of-OCL/image-20220720223210930.png" alt="image-20220720223210930">
<figcaption aria-hidden="true">image-20220720223210930</figcaption>
</figure>
<p>这三个等效</p>
<p>可以省略<code>self</code>?</p>
<p><code>allInstances（）</code>是什么？</p>
<h3 id="问题描述">问题描述</h3>
<p>OCL约束可能很长很复杂，降低了维护性和阅读</p>
<p>同样的意思可以有多个OCL写</p>
<p>规范性和复杂性也与元模型的设置油管</p>
<p>想提供一个guideline 让人们会写好OCL</p>
<p>给出一些常用和少用的模式</p>
<p>比较不同来源的OCL</p>
<h3 id="rq">RQ</h3>
<p>RQ1: How are developers using OCL?</p>
<p>复杂度 ，目的和结构/模式</p>
<p>RQ2: What maintainability measures are there for OCL code?</p>
<p>RQ3: What are the differences in OCL usage among different
sources?</p>
<h2 id="related-work">Related Work</h2>
<h3 id="refactoring-ocl-to-improve-maintainablity">refactoring OCL to
improve maintainablity</h3>
<p>关于 Code Smell的：</p>
<p>magic literal, and chain, long journey, rules exposure, duplicated
code.</p>
<p>implies chain, redundancy, non-atomic rule, verbose expression,
forAll chain, downcasting, type-related conditionals.</p>
<figure>
<img src="/2022/07/20/Usage-of-OCL/image-20220720230016457.png" alt="image-20220720230016457">
<figcaption aria-hidden="true">image-20220720230016457</figcaption>
</figure>
<p>讲了一些重构的研究，方法，还按是否改变context来把方法分为两类</p>
<h3 id="co-evolution">co-evolution</h3>
<p>元模型改变（UML改变） 而对应的OCL需要协同改变</p>
<p>两种办法：</p>
<ul>
<li><p>使用模板来写OCL 这样 工具就知道怎么改变了</p>
<p>全自动，但开发者就受到了限制</p></li>
<li><p>让开发者从侯选中挑选</p>
<p>半自动</p></li>
</ul>
<h3 id="ocl-用途">OCL 用途</h3>
<p>讲了一些相关的文献</p>
<h2 id="methodology">Methodology</h2>
<h3 id="how-use">1 How use</h3>
<h4 id="emf">EMF</h4>
<p>EMF Eclipse Modeling Framework ,核心是Ecore meta-model</p>
<p>是MOF的一个实现</p>
<p>Acceleo</p>
<p>z主要适用于model-text转换 算是一个技术</p>
<p>后面的result分析懒得读了 干脆把网站上学OCL的笔记也写在这里</p>
<h1 id="ocl-tutorial">OCL tutorial</h1>
<ul>
<li><p>所有的表达式都没有副作用</p></li>
<li><p>没有表达式的终结符</p></li>
<li><p>- - 单行注释</p></li>
<li><p>:: 表示某个聚合中的元素</p>
<figure>
<img src="/2022/07/20/Usage-of-OCL/image-20220723165155863.png" alt="image-20220723165155863">
<figcaption aria-hidden="true">image-20220723165155863</figcaption>
</figure></li>
<li><p>context指代模型中的任意元或子元素</p></li>
<li><p>self 指代当前对象</p></li>
<li><p>Invariant:</p>
<figure>
<img src="/2022/07/20/Usage-of-OCL/image-20220723152141704.png" alt="image-20220723152141704">
<figcaption aria-hidden="true">image-20220723152141704</figcaption>
</figure>
<p><code>inv</code>是必要的 inv名字是可选的，为了要多写几条可以use
<code>and</code> to conjunct</p>
<figure>
<img src="/2022/07/20/Usage-of-OCL/image-20220723164714032.png" alt="image-20220723164714032">
<figcaption aria-hidden="true">image-20220723164714032</figcaption>
</figure></li>
<li><p>Pre and Post</p>
<figure>
<img src="/2022/07/20/Usage-of-OCL/image-20220723152356097.png" alt="image-20220723152356097">
<figcaption aria-hidden="true">image-20220723152356097</figcaption>
</figure>
<p><span class="citation" data-cites="pre">@pre</span> refers to
before</p>
<p><code>result</code> is a <strong>resevered</strong> word for
operation</p>
<p>? really the resevered word?</p>
<ul>
<li>yes</li>
</ul>
<figure>
<img src="/2022/07/20/Usage-of-OCL/image-20220723171347778.png" alt="image-20220723171347778">
<figcaption aria-hidden="true">image-20220723171347778</figcaption>
</figure>
<p><code>result</code> corresponding to <code>self</code></p>
<figure>
<img src="/2022/07/20/Usage-of-OCL/image-20220723173014485.png" alt="image-20220723173014485">
<figcaption aria-hidden="true">image-20220723173014485</figcaption>
</figure>
<p>name is optionnaly</p>
<p>what is used to describe a <strong>operation</strong> ?</p>
<ul>
<li>other language, not ocl</li>
</ul></li>
</ul>
<p>​
<img src="/2022/07/20/Usage-of-OCL/image-20220723164808230.png" alt="image-20220723164808230" style="zoom:50%;"></p>
<p>​
<img src="/2022/07/20/Usage-of-OCL/image-20220723164831094.png" alt="image-20220723164831094" style="zoom:50%;"></p>
<p>why <code>income</code> use <code>::</code>?</p>
<p>类的方法就可以？</p>
<ul>
<li><p>Query</p>
<figure>
<img src="/2022/07/20/Usage-of-OCL/image-20220723153432607.png" alt="image-20220723153432607">
<figcaption aria-hidden="true">image-20220723153432607</figcaption>
</figure>
<p><code>body</code> is the reserved word</p></li>
<li><p>Definition</p>
<figure>
<img src="/2022/07/20/Usage-of-OCL/image-20220723153609388.png" alt="image-20220723153609388">
<figcaption aria-hidden="true">image-20220723153609388</figcaption>
</figure></li>
</ul>
<h3 id="collection">Collection</h3>
<figure>
<img src="/2022/07/20/Usage-of-OCL/image-20220723153659946.png" alt="image-20220723153659946">
<figcaption aria-hidden="true">image-20220723153659946</figcaption>
</figure>
<p>able to convert with the built -in method</p>
<h4 id="navigation">navigation</h4>
<p>dot <code>.</code> can refer to its property or the relationship
partner. for the relationship , the type of return varies from the
number of nested relationship <img src="/2022/07/20/Usage-of-OCL/image-20220723154553404.png" alt="image-20220723154553404"></p>
<h4 id="expressions">Expressions</h4>
<figure>
<img src="/2022/07/20/Usage-of-OCL/image-20220723155057962.png" alt="image-20220723155057962">
<figcaption aria-hidden="true">image-20220723155057962</figcaption>
</figure>
<p>nested set will be flattened</p>
<figure>
<img src="/2022/07/20/Usage-of-OCL/image-20220723165414150.png" alt="image-20220723165414150">
<figcaption aria-hidden="true">image-20220723165414150</figcaption>
</figure>
<p><code>-&gt;</code> is used to call the operation of the
collections</p>
<p><strong>also the loop</strong></p>
<p>index started from <code>1</code></p>
<h3 id="loop">Loop</h3>
<figure>
<img src="/2022/07/20/Usage-of-OCL/image-20220723165513125.png" alt="image-20220723165513125">
<figcaption aria-hidden="true">image-20220723165513125</figcaption>
</figure>
<p><code>collect(expr)</code> return a bag containing the item match the
expr</p>
<figure>
<img src="/2022/07/20/Usage-of-OCL/image-20220723161136173.png" alt="image-20220723161136173">
<figcaption aria-hidden="true">image-20220723161136173</figcaption>
</figure>
<p><code>forAll</code>has a <code>iterator</code> and a
<code>acc</code></p>
<p>Equal：</p>
<figure>
<img src="/2022/07/20/Usage-of-OCL/image-20220723162024009.png" alt="image-20220723162024009">
<figcaption aria-hidden="true">image-20220723162024009</figcaption>
</figure>
<h3 id="let">Let：</h3>
<figure>
<img src="/2022/07/20/Usage-of-OCL/image-20220723162059681.png" alt="image-20220723162059681">
<figcaption aria-hidden="true">image-20220723162059681</figcaption>
</figure>
<p>let xx=yy <code>in</code></p>
<p><code>in</code> is used to point out the scope(作用域)?</p>
<p>How many reserved words?</p>
<p>Key words are reserverd words</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">and, attr, body, context, def, else, endif, endpackage, if, implies, in, inv, let, not, oper, or, package,</span><br><span class="line">post, pre, then, xor</span><br></pre></td></tr></table></figure>
<h2 id="问题">问题：</h2>
<p>.allInstances（）是什么？不是集合的内置函数？会返回这个类的所有实例？</p>
<ul>
<li>不是 是用于类的，会返回所有实例，应该算OCL预定义方法</li>
</ul>
<p>Let 的 in 是否规定了作用域？</p>
<ul>
<li>是</li>
</ul>
<p>self 真的可以省略吗，任何情况？</p>
<p><img src="/2022/07/20/Usage-of-OCL/image-20220720223004101.png" alt="image-20220720223004101" style="zoom:50%;"></p>
<p>怎么理解这种forall的两个迭代器？</p>
<ul>
<li>不太好用迭代器来说，大概理解是任意两个对象</li>
</ul>
<figure>
<img src="/2022/07/20/Usage-of-OCL/image-20220723171101885.png" alt="image-20220723171101885">
<figcaption aria-hidden="true">image-20220723171101885</figcaption>
</figure>
<p>what is the <code>stereotype</code> here?</p>
<figure>
<img src="/2022/07/20/Usage-of-OCL/image-20220723173623088.png" alt="image-20220723173623088">
<figcaption aria-hidden="true">image-20220723173623088</figcaption>
</figure>
<p>是这样吗？？ 默认可以不用collect？</p>
<p>——应该是 想要一个属性可以直接用dot 的导航来代替collect</p>
]]></content>
      <categories>
        <category>Substance</category>
      </categories>
      <tags>
        <tag>OCL</tag>
        <tag>Tutorial</tag>
      </tags>
  </entry>
  <entry>
    <title>Note for Class CS-E4710 - Machine Learning: Supervised Methods D</title>
    <url>/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/</url>
    <content><![CDATA[<p>这是关于课程
机器学习-监督方法的笔记。这门课作业不算多，但是内容普遍比较理论化
希望可以学的好一点。 <span id="more"></span></p>
<h1 id="introduction">Introduction</h1>
<p>典型任务：分类，回归，排序</p>
<ul>
<li>分类：
<ul>
<li>将数据通过 决定面或边界 来分到预定义的类中</li>
<li>Multi-label Classification： 某个样本可以同时属于多个类</li>
<li>Extreme classification：类非常非常多</li>
</ul></li>
<li>回归
<ul>
<li>期望的输出是数字变量</li>
</ul></li>
<li>排序
<ul>
<li>不需要具体的值但是希望有一个排序列表</li>
<li>输入通常是一个偏序对列表：x&gt;y</li>
</ul></li>
</ul>
<p>定义了输入空间，输出空间，损失函数等内容。</p>
<p>将模型记作<span class="math inline">\(h\)</span></p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230927192311142-16958321007251.png" alt="image-20230927192311142" style="zoom:50%;"></p>
<p>值得注意的是有一个没见过的东西：</p>
<h4 id="empirical-risk">empirical risk ：</h4>
<p>​ 通过计算训练集的平均loss 来衡量模型的错误近似水平</p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230927191941385.png" alt="image-20230927191941385" style="zoom: 67%;"></p>
<p>在训练集上使用，评估模型对于训练集的拟合能力</p>
<p><span class="math inline">\(R\)</span> 有一个帽子，是训练集上的</p>
<p>####generalization error</p>
<p>另一个东西是 泛化误差 （generalization error) 或者称之为（真实）风险
(<strong>risk</strong>)</p>
<figure>
<img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230927192755056-16958321038722.png" alt="image-20230927192755056">
<figcaption aria-hidden="true">image-20230927192755056</figcaption>
</figure>
<p>大概意思是，这个数值是 损失函数在真实数据集的期望，其中<span class="math inline">\(D [ L(h(x), y)
]\)</span>真实数据下损失函数的期望。</p>
<p>在我们不知道整体数据分布<span class="math inline">\(D\)</span>的前提下，我们怎么通过训练集和模型类<span class="math inline">\(H\)</span>来说<span class="math inline">\(R(h)\)</span>呢？</p>
<p>有两种方法：</p>
<ul>
<li>通过测试集的经验风险评估</li>
<li>统计学习理论（下面要学的）</li>
</ul>
<p>这给俩名词解释：</p>
<h4 id="hypothesis-classes-or-model-families">hypothesis classes or
model families</h4>
<p>指的就是大类模型：</p>
<ul>
<li>线性模型
<ul>
<li>逻辑回归</li>
</ul></li>
<li>神经网络</li>
<li>核方法
<ul>
<li>SVM</li>
</ul></li>
<li>组装方法
<ul>
<li>Random Forests</li>
</ul></li>
</ul>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231208033807955.png" alt="image-20231208033807955" style="zoom: 50%;">可以认为是一个超参数未定的模型，而<span class="math inline">\(h\)</span>就是一个我们给出了的确定参数的模型</p>
<h2 id="线性回归">线性回归</h2>
<figure>
<img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230927194640083.png" alt="image-20230927194640083">
<figcaption aria-hidden="true">image-20230927194640083</figcaption>
</figure>
<p>优化问题：</p>
<figure>
<img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230927195059252.png" alt="image-20230927195059252">
<figcaption aria-hidden="true">image-20230927195059252</figcaption>
</figure>
<p>最小值有定值，只要关于x的一个矩阵可逆</p>
<figure>
<img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230927195124896.png" alt="image-20230927195124896">
<figcaption aria-hidden="true">image-20230927195124896</figcaption>
</figure>
<h2 id="二元分类">二元分类</h2>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230927200137127.png" alt="image-20230927200137127" style="zoom:67%;"></p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230927200148870.png" alt="image-20230927200148870" style="zoom:67%;"></p>
<h4 id="consistent-hypothesis">consistent hypothesis</h4>
<p>下面探讨关于学到的<span class="math inline">\(h\)</span></p>
<p><strong>当然需要训练集上（数据）没问题</strong></p>
<p>如果一个<span class="math inline">\(h\)</span>能够在训练集上全部分对，那么我们称其为一致假设(<strong>consistent
hypothesis</strong>) 并在此基础上有两个极端例子：</p>
<ul>
<li>最兼容假设(Most general hypothesis)<span class="math inline">\(G\)</span></li>
<li>最特定假设(Most specific hypothesis)<span class="math inline">\(S\)</span></li>
</ul>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230927200435685.png" alt="image-20230927200435685" style="zoom:67%;"></p>
<p>直观上来说，选一个中间的会最安全：</p>
<ul>
<li>Margin：
<ul>
<li>the minimum distance between the decision boundary and a training
point</li>
<li>在决定边界核训练点之间的最小距离</li>
</ul></li>
</ul>
<p><strong>最小化到两个极端例子的距离</strong></p>
<p>这种方式在SVM支持向量机中使用</p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230927200655041.png" alt="image-20230927200655041" style="zoom:67%;"></p>
<p>这里还有点问题，关于边距Margin的</p>
<h2 id="评估模型">评估模型</h2>
<p>对于分类问题最常用的LOSS是01损失，但是他有俩个问题：</p>
<ul>
<li>难以应对数据不平衡</li>
<li>不同的预测错误可能代价不一致：致命疾病的诊断</li>
</ul>
<p>引入假阴，假阳的概念：</p>
<ul>
<li>这里的假 说的是预测错误</li>
<li>这里的阴和阳说的是预测结果</li>
</ul>
<p>一般来说，更加specific的模型会倾向于假阴更多，假阳更少（更趋向于预测阴？）</p>
<p><strong>长方形面积更小</strong></p>
<p>如果更general就相反</p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230927201411977.png" alt="image-20230927201411977" style="zoom:67%;"></p>
<p><strong>Confusion matrix</strong>（混淆矩阵/误差矩阵）</p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230927201544950.png" alt="image-20230927201544950" style="zoom:67%;"></p>
<p>在此基础上，引入了四个评价模型的指标</p>
<ul>
<li>Empirical risk
<ul>
<li>错误预测的比例</li>
</ul></li>
<li>Precision/Positive Predictive Value
<ul>
<li>预测为阳中，正确的比例</li>
</ul></li>
<li>Recall/Sensitivity
<ul>
<li>真阳中对了多少</li>
</ul></li>
<li>F1
<ul>
<li>Recall 核 Precision搞一起的一个东西</li>
<li></li>
</ul></li>
</ul>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230927201913451.png" alt="image-20230927201913451" style="zoom:67%;"></p>
<p>在这个基础上，引入了一个称为<strong>ROC</strong>的概念。引入某种分类时候模型给出的可信度阈值<span class="math inline">\(θ\)</span>，大于他算成阳反之则阴，通过调节这个东西可以调节模型的假阴假阳占比</p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230927202255998.png" alt="image-20230927202255998" style="zoom:67%;"></p>
<p>ROC曲线可以衡量一个模型的指标，包括在假阴假阳上的权衡/是否有全局最优的<span class="math inline">\(h\)</span></p>
<ul>
<li>底下的面积称为AUC/AUROC</li>
<li>一般来说，一个分类器越倾向于预测为阳，其中真/假的比例就越高</li>
</ul>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230927202511785.png" alt="image-20230927202511785" style="zoom:67%;"></p>
<p>怎么估计模型在更广阔数据中的表现呢？</p>
<ul>
<li>可以使用独立测试集的 empirical risk来估计，期望就是
<ul>
<li>测试集上的Rh 肯定更低</li>
<li>模型越复杂，empirical risk越低，可能把模型太复杂了</li>
<li>可能搞的是fitting而非learning</li>
</ul></li>
</ul>
<h1 id="统计学习方法">统计学习方法</h1>
<p>我们希望可以最小化这个东西 true risk/generalization error：</p>
<figure>
<img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230927192755056-16958321038722.png" alt="image-20230927192755056">
<figcaption aria-hidden="true">image-20230927192755056</figcaption>
</figure>
<p>但是不知道真实的数据分布： 这就是本节课要解决的问题</p>
<p>假设真实数据和训练集独立同分布（i.i.d.)</p>
<h2 id="概率近似正确学习框架-pac-framework">概率近似正确学习框架 PAC
framework</h2>
<p>Probably Approximate Correct (PAC) Learning
framework，形式化了机器学习的泛化概念</p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230929164001463.png" alt="image-20230929164001463" style="zoom:50%;"></p>
<p>值得一提的是其中的概念类<span class="math inline">\(C\)</span>,这是我们希望找到$h $ ∈ <span class="math inline">\(H\)</span>,来尽可能地接近他。</p>
<p>这样的话，最底下的公式也就好理解了</p>
<p>下面定义了对于这种概念类（concept class) <span class="math inline">\(C\)</span> 是否是PAC-learnable的。</p>
<p>如果他是可学习的，那么：</p>
<ul>
<li>存在一个算法<span class="math inline">\(A\)</span></li>
<li>给定数据集<span class="math inline">\(S\)</span></li>
<li>能够学到一个目标假设类<span class="math inline">\(h_s\)</span>∈<span class="math inline">\(H\)</span></li>
<li>能使得泛化误差小于<span class="math inline">\(\epsilon\)</span>的概率大于1-δ</li>
</ul>
<p><strong>对于任何分布和随机<span class="math inline">\(\epsilon\)</span>，sample size <span class="math inline">\(m\)</span>对1/<span class="math inline">\(\epsilon\)</span>,1/δ多项式增长，只要两个</strong></p>
<p>在此基础上，增加了一个 efficiently
PAC-learnable的概念，<strong>更强，对数据集大小<span class="math inline">\(m\)</span>也有要求</strong>，如果训练的算法<span class="math inline">\(A\)</span>能够以某种多项式复杂度时间对这三个变量增长的话</p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230929165522922.png" alt="image-20230929165522922" style="zoom:50%;"></p>
<p>具体解释一下这个式子：</p>
<ul>
<li>$<span class="math inline">\(, Generalization error
bound，泛化误差，我们有信心这个学到的概念类\)</span>h<span class="math inline">\(在全局数据上的错误率 通常采用\)</span>$</li>
<li><span class="math inline">\(1-\delta\)</span>, Confidence
level，信心程度，我们认为前面那个错误率小于多少多少的失败率，通常采用<span class="math inline">\(\delta =0.05\)</span> <strong><span class="math inline">\(\delta\)</span>可以说是错误率</strong></li>
<li>所需的样本大小和运行时间，不应该随着误差降低和信心增强，而爆炸增长——需要多项式复杂度</li>
<li><span class="math inline">\(\{R(h_S ) ≤ \epsilon\}\)</span>
被看成了一个随机变量，因为我们确实不知道哪个<span class="math inline">\(H中的h\)</span>会被挑出来，以及他在真实数据上的表现（这一句和最后一句不一样，有点问题）</li>
</ul>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230929170658370.png" alt="image-20230929170658370" style="zoom:50%;"></p>
<p>上面概念的可视化：<strong>非常好图片</strong></p>
<ul>
<li>最高的就是泛化误差的期望</li>
<li>信心越强，<span class="math inline">\(\delta\)</span>越小,<span class="math inline">\(\epsilon\)</span>就越大（能接受的泛化误差）
gap就越大</li>
</ul>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230929173533871.png" alt="image-20230929173533871" style="zoom:50%;"></p>
<p>下面讨论怎么样得出样本量，错误率和置信程度的一个公式（这破玩意看了快一下午）</p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230929190656757.png" alt="image-20230929190656757" style="zoom:50%;"></p>
<p>首先我们看这样一个东西，<span class="math inline">\(R\)</span>是想学到的概念类<span class="math inline">\(C\)</span>, <span class="math inline">\(R`\)</span>是我们在目前样本上，能学到的最紧的<span class="math inline">\(h\)</span>，可以把目前的样本都预测对，也是一个最紧的预测，没有留下什么余地。那么</p>
<ul>
<li>错误只会来自于假阴，即，<span class="math inline">\(R`\)</span>把真实的应该是蓝色的错误给排除出去了</li>
<li>泛化误差即是两个预测的差，即<span class="math inline">\(R(h)&lt;=P(R-R`)\)</span></li>
<li>我们能得到的理想<span class="math inline">\(h\)</span>肯定要大于<span class="math inline">\(R`\)</span></li>
<li>计算出的泛化误差<span class="math inline">\(\epsilon
&#39;=Pr_D(R-h)\)</span></li>
</ul>
<p>下面我们引入一个用于辅助证明的基础预设， <span class="math display">\[
Pr_D(R)&gt;\epsilon
\]</span> 这是几乎显然的，不然你用最紧的<span class="math inline">\(R&#39;\)</span> 都能得到 <span class="math inline">\(R(R&#39;)&lt;\epsilon\)</span>
,也就没有继续研究的意义了(因为<span class="math inline">\(R&#39;\)</span>是<span class="math inline">\(R\)</span>的子集，<span class="math inline">\(Pr_D(R-h)&lt;=Pr_D(R-R&#39;)&lt;Pr_D(R)\)</span>)</p>
<p>下面在误差区域中，构造四个小长方形辅助证明</p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230929192028402.png" alt="image-20230929192028402" style="zoom:50%;"></p>
<p><span class="math inline">\(r_i\)</span>之间彼此有重叠，且 <span class="math display">\[
Pr_D(r_i)={\epsilon \over 4}
\]</span> 可以看出来，他们的并集的概率密度几乎是显然小等于<span class="math inline">\(\epsilon\)</span>
的（从图上来看，等于去不到，但不妨严谨一点）</p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230929192254785.png" alt="image-20230929192254785" style="zoom:50%;"></p>
<p>错误的预测仅会出现在浅蓝色外框和橘色内框中。</p>
<p>如果我们得到的预测类<span class="math inline">\(h\)</span>能够交到四个小矩形<span class="math inline">\(r_i\)</span>,那么剩下的面积显然小于<span class="math inline">\(\epsilon\)</span>,即 <span class="math display">\[
R(h)&lt;\epsilon
\]</span>
换句话说，即上一句的逆否命题：如果算出来的泛化误差大于阈值<span class="math inline">\(\epsilon\)</span>
,那么至少有一个小矩形没被交到。</p>
<p>对于目前的训练数据，蓝色点点来说，不在某个小矩形<span class="math inline">\(r_i\)</span>的概率是<span class="math inline">\(P=1-{\epsilon \over 4}\)</span></p>
<p><strong>这里没有对数据的分布进行假定，因为取这个矩形的时候就假定了某个小矩形所占的概率密度，不需要数据是均匀分布的</strong></p>
<p>那么，现在的数据集容量<span class="math inline">\(m=card(D_{train})\)</span>,这些点都不在某个小矩形的概率就是
<span class="math display">\[
(1-{\epsilon \over 4})^m
\]</span>
考虑每个点不在每个小矩形之间彼此独立（为什么？），那么’至少有一个小矩形没被分布到‘
就是四个上述事件的和事件， <span class="math display">\[
P(R(h)&gt;\epsilon)&lt;=\Sigma_1^4(1-{\epsilon \over 4})^m=4(1-{\epsilon
\over 4})^m
\]</span> 我们希望这种事情（误差大于阈值）的概率小于一个信心程度<span class="math inline">\(\delta\)</span> ,那么就有了 <span class="math display">\[
P(R(h)&gt;\epsilon)&lt;\delta
\]</span> 同时也是： <span class="math display">\[
P(R(h)&lt;=\epsilon)&gt;=1-\delta
\]</span></p>
<p>=&gt; <span class="math display">\[
4(1-{\epsilon \over 4})^m&lt;\delta
\]</span> 继续放缩</p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230929194054908.png" alt="image-20230929194054908" style="zoom:50%;"></p>
<p><span class="math display">\[
4(1−{\epsilon \over 4})^m ≤ 4 exp({−m \epsilon \over 4})&lt;\delta
\]</span> 解得： <span class="math display">\[
m&gt;= {4 \over \epsilon}*ln({4\over\delta})
\]</span> 揭示了训练样本量和泛化误差<span class="math inline">\(\epsilon\)</span>,信心指数<span class="math inline">\(\delta\)</span>的关系</p>
<p>同时有了要达到希望的错误率，使用$m,$的关系</p>
<p>（为什么放缩这么一家伙？）</p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230929194534646.png" alt="image-20230929194534646" style="zoom:50%;"></p>
<p>这里展示了一些三者之间的函数关系图，可以看到：</p>
<ul>
<li>数据量越大，泛化误差越低</li>
<li>数据量的增加有边际递减效应（law of diminishing returns)</li>
<li>信心指数<span class="math inline">\(\delta\)</span>越高，就需要越多的数据来降低同样的误差</li>
</ul>
<p>这样的一个泛化误差边界有这样的好处：</p>
<ul>
<li>对于任意的目标概念类<span class="math inline">\(C\)</span>(包括难学的)，任意的数据分布<span class="math inline">\(D\)</span>(包括对抗性生成的让学习更难的)都适用，因为没有依赖他们进行假设推导</li>
<li>我们此时研究的是
误差分布的最大，也就是图像的尾巴，而没有研究这个量收敛到尾部的情况）</li>
</ul>
<p>所以：</p>
<ul>
<li>经验预估检测错误率（empirically estimated test
errors）应该比这个低很多
（训练集上算的？，<strong>也是从真分布上估计出来的</strong>）</li>
<li>这个也应该比实际的上界松很多，是一个非常general的上界</li>
</ul>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230929195059752.png" alt="image-20230929195059752" style="zoom:50%;"></p>
<h2 id="pac可学习理论对有限假设集">PAC可学习理论对有限假设集</h2>
<p>对于有限假设集<span class="math inline">\(H\)</span>，它满足以下条件：</p>
<ul>
<li>输入元素满足有限域（例如离散化的数值或布尔变量，即，可列举的）</li>
<li>假设的表示也是有有限空间的（变量重复有限次数）</li>
<li>用布尔代数运算符对付布尔公式的自己</li>
</ul>
<p>这个东西已经被统计学习理论研究清楚了</p>
<p>可以知道：</p>
<ul>
<li>样本复杂边界（Sample complexity bound），就是样本量。
即给定泛化误差阈值<span class="math inline">\(\epsilon\)</span>
和置信度<span class="math inline">\(\delta\)</span>，需要多大的样本量才行</li>
<li>泛化误差（ generalization error bound）。
给定样本量和置信度，可以达到多大的误差。</li>
</ul>
<p>当然这俩都得要你算出来有限假设集的大小才行</p>
<p>成立的前提是，<strong>存在</strong>一个一致假设…<strong>consistent
hypothesis, one with zero empirical risk</strong>，</p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230929205824865.png" alt="image-20230929205824865" style="zoom:67%;"></p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230929205745657.png" alt="image-20230929205745657" style="zoom:67%;"></p>
<p><strong>后面对于这个公式（<span class="math inline">\(m&gt;=\)</span>)给了证明，不过没太看懂</strong></p>
<p>？为什么这边还单独对有限假设类给了另一个公式</p>
<p>下面给出例子进行详细说明</p>
<p>对于一种相对简单的情况：</p>
<h3 id="boolean-conjunctions">Boolean conjunctions</h3>
<p>（包括了 and 和 not的布尔公式，不能用or）</p>
<p>每一个变量是布尔变量</p>
<p>比如这样的输入：</p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230929205419126.png" alt="image-20230929205419126" style="zoom:50%;"></p>
<p>假设集大小为 <span class="math display">\[
|H|=3^d
\]</span> 每一个变量要么正，要么否，要么不出现——承认无关变量的可能</p>
<h3 id="任意布尔公式">任意布尔公式</h3>
<p>？怎么区分到底是3为底还是2</p>
<p>若有<span class="math inline">\(d\)</span>个变量，每一个都可以为正或负，那么输入空间<span class="math inline">\(X\)</span> <span class="math display">\[
|X| = 2^d
\]</span> 考虑到并的存在，一个假设<span class="math inline">\(h\)</span>就是一个子集<span class="math inline">\(S\)</span>的每一项并集</p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230929210542203.png" alt="image-20230929210542203" style="zoom:50%;"></p>
<p>对于<span class="math inline">\(X\)</span>中的每一个向量，都有挑选与否的可能：
<span class="math display">\[
|H|=2^{2^d}
\]</span> 计算其他东西用上面的公式就行了</p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230929210728950.png" alt="image-20230929210728950" style="zoom:50%;"></p>
<p>这个可以看出来，对<span class="math inline">\(d\)</span>是指数增长的，所以不认为是PAC-learnable</p>
<h2 id="证明大纲">证明大纲</h2>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20230929211935506.png" alt="image-20230929211935506" style="zoom:50%;"></p>
<p>这页没看懂</p>
<p>后面基本都能看懂 先搁着把</p>
<p>下面还讨论了一点有限假设集不一致的情况，从公式来看是多了一点项</p>
<h2 id="参考">参考：</h2>
<p>https://theigrams.github.io/zjblog/2021/07/22/pac.html</p>
<h1 id="l3-pac-learning的拓展">L3 PAC learning的拓展</h1>
<p>上面学的，一个是假设 假设类是矩形，一个是假设 假设类是有限的</p>
<p>但是在现实中，有更多不依赖这种前提的问题（SVM的hyperplane，神经网络的连续输入）</p>
<p>所以我们希望有更好的工具来分析这些案例</p>
<h2 id="vc-dimension">VC dimension</h2>
<p>可以说是一个用来衡量假说类容量的概念，用来适用于不同的理论</p>
<h1 id="lecture-4-model-selection">Lecture 4 Model selection</h1>
<ul>
<li>给定数据集 怎么算<span class="math inline">\(R(h),R(h*),R*\)</span>?</li>
</ul>
<p>开始一个大型的名词解释来整清楚上面这个过程：</p>
<h4 id="bayes-error">Bayes error</h4>
<p>给定数据（<span class="math inline">\(X,Y\)</span>）的分布，能够实现的最小误差</p>
<p><span class="math inline">\(R^*\)</span></p>
<ul>
<li><p>算不出来，但可以用下面介绍的方式分解</p></li>
<li><p>generalization error<span class="math inline">\(R(h)\)</span>也算不出来</p></li>
<li><p>是一个用来衡量最好表现的理论工具</p></li>
</ul>
<h5 id="bayes-classifier">Bayes classifier</h5>
<p>一个假设<span class="math inline">\(h\)</span>，能实现<span class="math inline">\(R(h) = R^∗\)</span>泛化误差等于Bayes error的称之为
<strong>Bayes classfier</strong></p>
<h5 id="noise">noise</h5>
<p>Bayes classifier 实现的平均误差是<strong>noise</strong></p>
<p>noise的期望 就是Bayes error</p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231208204056083.png" alt="image-20231208204056083" style="zoom:50%;"></p>
<p>不过这张图里，noise是对于每一个<span class="math inline">\(x\)</span>
预测概率较小的那一项的概率？</p>
<h4 id="excess-error">excess error</h4>
<p>衡量一个假设<span class="math inline">\(h\)</span>相对于最优误差的
Bayes classifier，会达到多少泛化误差</p>
<p>通常会这么分解：</p>
<figure>
<img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231208204814056.png" alt="image-20231208204814056">
<figcaption aria-hidden="true">image-20231208204814056</figcaption>
</figure>
<ul>
<li>泛化误差的差就是我们感兴趣的东西</li>
</ul>
<h5 id="estimation-error">estimation error</h5>
<p><span class="math inline">\(\epsilon_{est}= R(h) −
R(h∗)\)</span>这一部分称为估计误差，是目前<span class="math inline">\(h\)</span>距离<span class="math inline">\(h`∈H\)</span> ，假设类中最优的一个，的距离</p>
<ul>
<li>这一部分也被称为variance，好像能达到</li>
</ul>
<h5 id="approximation-error">approximation error</h5>
<ul>
<li><span class="math inline">\(\epsilon_{approxiamation}== R(h ∗ ) −
R∗\)</span> 这一部分成为估计误差，描述假设类和最佳假设类的距离
<ul>
<li>这部分称为bias，难以消除</li>
</ul></li>
</ul>
<hr>
<p>下面给出一个理想情况下计算的例子：</p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231208204450075.png" alt="image-20231208204450075" style="zoom:50%;"></p>
<ul>
<li>X均匀分布</li>
<li>分类器会将概率最大的作为预测输出</li>
<li><span class="math inline">\(R^*\)</span> 计算了对于每一个X的概率
乘以他的预测小标签的概率的和</li>
</ul>
<p>为了拟合这个分布，我们提出了一个假设类<span class="math inline">\(H\)</span>：</p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231208205315166.png" alt="image-20231208205315166" style="zoom:50%;"></p>
<p><span class="math inline">\(H\)</span>中最好的<span class="math inline">\(h^`\)</span>,将3以下的数字标为0，3标为1</p>
<p>他的泛化误差<span class="math inline">\(R(h^`)\)</span>就是这么算的：，通过没有达到设计的那一部分数据的影响</p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231208205523218.png" alt="image-20231208205523218" style="zoom:50%;"></p>
<p>然后目前，我们在一个13个数据的训练集上，训练出了1个h，能够将1标为正，其他为0</p>
<p>他能达到最好的empirical
error<img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231208205709230.png" alt="image-20231208205709230" style="zoom: 50%;">=0.384（虽然没什么用），</p>
<p>利用上面的真数据分布计算出的generalization
error（在这个例子中都算错的项
对应的概率）是0.433（有点奇怪，因为干的事情就不一样）</p>
<p>最后以R(h)的形状展示出来</p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231208210055643.png" alt="image-20231208210055643" style="zoom:67%;"></p>
<hr>
<p>总结：</p>
<ul>
<li><span class="math inline">\(\epsilon\)</span>estimation = R(h) − R(h
`) 这一部分可以通过PAC理论估计出来
<ul>
<li>？PAC估计的是什么来着</li>
</ul></li>
</ul>
<p>可以看出来，<span class="math inline">\(\epsilon_{app}=approximation
= R(h ∗ ) − R ∗\)</span>
这一部分搞不出来，因为i不知道概念类和目标的差异——无法评估概念类的好坏！</p>
<p>在我们使<span class="math inline">\(H\)</span>
更复杂的时候，我们就有更大机会包括离 Bayes classifer 更近的<span class="math inline">\(h\)</span>，降低 approximation error</p>
<p><strong>但是会让<span class="math inline">\(h\)</span>更难学！——增大Estimation
error，generalization bounds 更松，VC维更高</strong></p>
<p>下来要干的事情： 尽可能好的训练模型，使得它既有加好的 empirical
error，又有较好的复杂性（在当前数据上表现好，但代价不太高）</p>
<h2 id="regularization-based-algorithms">Regularization-based
algorithms</h2>
<p>我们将假设类按照复杂度分层划分，<img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231208220455948.png" alt="image-20231208220455948" style="zoom: 67%;"><span class="math inline">\(\gamma\)</span>就是他的复杂度，可以是布尔的变量数也可以是神经网络的大小</p>
<p>需要挑选合适的<span class="math inline">\(\gamma\)</span></p>
<p>希望对模型中的大权重进行惩罚，下面的讨论将给予线性函数：<span class="math inline">\(x → w^T x\)</span></p>
<p>这里的<span class="math inline">\(\gamma\)</span>就是<span class="math inline">\(w\)</span>的范数<img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231208220643143.png" alt="image-20231208220643143" style="zoom:67%;"></p>
<h4 id="范数">范数</h4>
<p>范数常用有两种算法：</p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231208220715969.png" alt="image-20231208220715969" style="zoom:50%;"></p>
<hr>
<p>对于L2来说，算他的Rademacher complexity是有捷径的</p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231208220759112.png" alt="image-20231208220759112" style="zoom: 67%;"></p>
<p>一般来说，可以用<span class="math inline">\(w\)</span>的范数来当作这个假设类的Rm上界</p>
<hr>
<p>回到最开始的想法，学习的目标应该是最小化一个函数，考虑到经验误差和模型的复杂性，这里给出形式化定义：</p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231208220932936.png" alt="image-20231208220932936" style="zoom:67%;"></p>
<h2 id="model-selection-by-using-a-validation-set">Model selection by
using a validation set</h2>
<p>这里给出了一般的训练集/验证集和测试集划分手段</p>
<ul>
<li>使用网格搜，来寻找在验证集上表现最好的超参数
<ul>
<li>代价大！</li>
</ul></li>
<li>有意思的是，找到了最好的超参数后，<strong>在训练集和验证集上共同</strong>重新训练最后的模型，
<ul>
<li>再去测试集验证</li>
</ul></li>
<li>验证集：
<ul>
<li>防止过拟合</li>
<li>如果测试集拿来找超参，可能过分乐观
<ul>
<li>测试集不能参与贡献！</li>
</ul></li>
</ul></li>
<li>怎么分？
<ul>
<li>一个分法是分层分，每一个类中都挑出等比例的，最后合并</li>
<li>防止有的小类没被见过</li>
</ul></li>
</ul>
<h3 id="交叉验证-cross-validation">交叉验证 Cross-validation</h3>
<p>这一部分的内容目的是考虑普通的验证集和测试集划分，仍然可能存在一定问题：</p>
<ul>
<li>训练集 测试集可能虽然小但还是有噪声/outliers</li>
<li>训练过程仍然有随机性（初始化）</li>
</ul>
<p>————我们希望通过多搞几次（averaging multiple
splits）来克服这些问题！</p>
<ul>
<li>最好的超参数由N个验证集上平均最好的给出</li>
</ul>
<h4 id="搞验证集">搞验证集</h4>
<p>这一步的前提是你已经留好了一个不变的测试集</p>
<ol type="1">
<li>当然，你可以重复随机划分</li>
<li>但是我们这里用更科学的方法——</li>
</ol>
<h5 id="n-fold">n-Fold</h5>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209034508916.png" alt="image-20231209034508916" style="zoom:50%;"></p>
<h4 id="leave-one-out-cross-validation-loo">Leave-one-out
cross-validation (LOO)</h4>
<p>每次只把一个当作验证集，其他全部当成训练集</p>
<p>——很强，但计算代价大</p>
<h4 id="测试集一起搞">测试集一起搞</h4>
<p>n-fold的问题是，单个选择的测试集仍然肯恩恶搞有偏差</p>
<p>使用</p>
<h5 id="nested-cross-validation">Nested cross-validation</h5>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209034657651.png" alt="image-20231209034657651" style="zoom:50%;"></p>
<p>两层循环，先留测试集，剩下的内容上再N-fold搞训练集/验证集划分</p>
<h1 id="线性模型linear-models">线性模型，Linear models</h1>
<p>这一章开始学习具体的模型，内容包括</p>
<ul>
<li>模型的任务</li>
<li>模型是如何构建的
<ul>
<li>包括几何意义</li>
</ul></li>
<li>如何优化（损失函数）
<ul>
<li>错误是什么形式</li>
<li>错误又应该如何更新</li>
<li>对损失函数的证明/优化</li>
</ul></li>
<li>收敛条件
<ul>
<li>能对付什么样子的数据？
<ul>
<li>（是否收敛） 收敛速度</li>
</ul></li>
<li>数据大小，要求 分布</li>
</ul></li>
</ul>
<hr>
<p>这一章全部都是在做分类任务，甚至集中在线性分类</p>
<ul>
<li>先介绍任务</li>
<li>在介绍线性分类器
<ul>
<li>在其基础上介绍感知机</li>
</ul></li>
<li>再介绍逻辑回归（确实是在做分类的）</li>
</ul>
<h2 id="线性分类器">线性分类器：</h2>
<p>输入是多维实数，分类为正负例<img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209035700649.png" alt="image-20231209035700649" style="zoom:50%;"></p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209035755025.png" alt="image-20231209035755025" style="zoom:50%;"></p>
<p>假设类长这个样子，向量乘一下，再加上偏置项 然后再激活</p>
<h4 id="优点">优点：</h4>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209035919977.png" alt="image-20231209035919977" style="zoom:50%;"></p>
<ul>
<li>好理解</li>
<li>空间小</li>
<li>简单（好学习）</li>
</ul>
<p><strong>应该是第一个尝试的算法！</strong></p>
<h4 id="空间解释">空间解释：</h4>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209040225218.png" alt="image-20231209040225218" style="zoom:50%;"></p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209040015503.png" alt="image-20231209040015503" style="zoom:50%;"></p>
<ul>
<li>线是<span class="math inline">\(wx\)</span></li>
<li>法向量是<span class="math inline">\(w\)</span></li>
<li>其他分别标注出了
<ul>
<li>它到原点的距离</li>
<li>偏置项会导致法向量方向和超平面的关系</li>
<li>点和平面距离</li>
</ul></li>
</ul>
<h2 id="线性分类器的学习过程">线性分类器的学习过程</h2>
<p>可以看出来
原来的式子<img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209040329651.png" alt="image-20231209040329651" style="zoom:50%;">可以写的更简单</p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209040341303.png" alt="image-20231209040341303" style="zoom:50%;"></p>
<p>分类后的点的函数值对应向量和法向量所成的角度与正负例有关</p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209040430231.png" alt="image-20231209040430231" style="zoom:50%;"></p>
<p>这里学习到了一个重要的概念：</p>
<h4 id="margin">Margin</h4>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209040541458.png" alt="image-20231209040541458" style="zoom:50%;"></p>
<p>大概就是标准化过的点到超平面的距离向量，再和判断是否正确乘一下,&lt;0就是错了</p>
<p><strong>我们会希望这个margin大于0</strong></p>
<h2 id="感知机-perceptron">感知机 Perceptron</h2>
<p>第一个机器学习算法，1956，用来做二分类问题</p>
<p>形式是这样子的：</p>
<figure>
<img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209040800184.png" alt="image-20231209040800184">
<figcaption aria-hidden="true">image-20231209040800184</figcaption>
</figure>
<p>训练过程是这样的：</p>
<figure>
<img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209040814241.png" alt="image-20231209040814241">
<figcaption aria-hidden="true">image-20231209040814241</figcaption>
</figure>
<p>直到全部都分类正确</p>
<h4 id="具体训练过程">具体训练过程</h4>
<p><strong>主要研究对象是训练集</strong></p>
<p>在干什么呢？</p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209040904307.png" alt="image-20231209040904307" style="zoom:50%;"></p>
<ul>
<li><span class="math inline">\(yixi\)</span>被当作错误代价用来更新权重向量</li>
<li>我们来看一看对于一个（错误分类的点），它的margin会如何迭代：</li>
<li>可以看到它的下次margin就是这次再加上<span class="math inline">\(x^2\)</span>,一直在增大
<ul>
<li>当然不能保证一次更新就行</li>
</ul></li>
</ul>
<p>大概就是这样的一个过程：平面的法向量在转，就分好了</p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209041204053.png" alt="image-20231209041204053" style="zoom:50%;"></p>
<h4 id="若干好性质">若干好性质</h4>
<ul>
<li>只要两个类是线性可分的，就一定能超平面收敛</li>
</ul>
<h5 id="theorem-novikoff">Theorem （Novikoff)</h5>
<p>** 应用条件 应该是两个线性可分的类和感知机**</p>
<p>证明对于一个知道了</p>
<ul>
<li>大小</li>
<li>最大模(衡量数据集有多分散)</li>
</ul>
<p>的测试集</p>
<p>并且满足某条件</p>
<ul>
<li><span class="math inline">\(\gamma\)</span> 是最大可实现的margin
<ul>
<li>衡量数据集中两个类的分离程度</li>
</ul></li>
</ul>
<p><strong>可以证明训练的收敛步数上界</strong></p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209043016830.png" alt="image-20231209043016830" style="zoom:50%;"></p>
<p>?为什么这边<span class="math inline">\(w\)</span>都是标准化过的</p>
<p>但是，对于非线性可分数据来说，<strong>算法不会停止！</strong></p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209043207610.png" alt="image-20231209043207610" style="zoom:50%;"></p>
<h4 id="损失函数的形式化表达">损失函数的形式化表达</h4>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209043302587.png" alt="image-20231209043302587" style="zoom:50%;"></p>
<p>可以看出这个损失函数是凸的，所以有很好的性质：</p>
<ul>
<li>局部最小也是全局最小</li>
<li>比较好通过逐步更新减少loss
<ul>
<li>不是NP-hard</li>
</ul></li>
</ul>
<hr>
<p>但是现在，我们希望还有一个比感知机更好的算法，<strong>离最优比较近就终止</strong>就行了</p>
<h2 id="logistic-regression">Logistic regression</h2>
<p>它确实是分类器</p>
<ul>
<li><p>logistic函数</p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209043827299.png" alt="image-20231209043827299" style="zoom:50%;"></p></li>
<li><p>logit函数</p>
<ul>
<li>是他的反函数</li>
<li><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209043904187.png" alt="image-20231209043904187" style="zoom:50%;"></li>
</ul></li>
</ul>
<hr>
<p>logistic regression
认为<img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209044136286.png" alt="image-20231209044136286" style="zoom:50%;">可以用底下的式子写出来</p>
<ul>
<li>随之而来的它的margin</li>
</ul>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209044129577.png" alt="image-20231209044129577" style="zoom:50%;"></p>
<h4 id="loss">loss</h4>
<p>我们希望能够最大化训练集上的正确的条件概率分布，将其表示为<span class="math inline">\(w^*\)</span></p>
<p>两边取对数再稍作变形，得到它的相反数，<strong>loss</strong></p>
<p>最大化可能性，就是最小化loss</p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209044613396.png" alt="image-20231209044613396" style="zoom:50%;"></p>
<p>这是一个单调递减的可微凸函数，他还有好性质：</p>
<ul>
<li>离谱的时候值大，</li>
<li>正确的时候很慢——不会对已经分的比较好的继续努力</li>
</ul>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209044808627.png" alt="image-20231209044808627" style="zoom:50%;"></p>
<h4 id="训练">训练</h4>
<p>我们希望能够求出这个loss的最小值</p>
<p>因为它不是线性的，所以没办法直接给——使用随机梯度下降<strong>stochastic
gradient descent</strong></p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209045002927.png" alt="image-20231209045002927" style="zoom:50%;"></p>
<p>梯度就是<span class="math inline">\(J\)</span>对所有变量的偏导，组成的一个向量</p>
<p>算还是正经算，算出来发现仍然能扯回去</p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209045159599.png" alt="image-20231209045159599" style="zoom:50%;"></p>
<p>作为对比，原始的条件概率是这样的：</p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209045222049.png" alt="image-20231209045222049" style="zoom: 67%;"></p>
<p>对每个训练输入<span class="math inline">\(x\)</span>,计算出来他的梯度向量,最后发现直接乘对应向量就可以了</p>
<p><strong>这个梯度向量给出了最快下降的方向</strong></p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209045550298.png" alt="image-20231209045550298" style="zoom:50%;"></p>
<p>但是算这个东西其实很烦,因为对每个<span class="math inline">\(x\)</span>都要算这么一家伙(如果算全体梯度的话</p>
<h5 id="随机梯度下降">随机梯度下降</h5>
<p>每次从全体样本中抽一条,用它的样本更新<span class="math inline">\(w\)</span></p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209050651227.png" alt="image-20231209050651227" style="zoom:50%;"></p>
<p>显然,它的期望是和全梯度一样的(随机抽取的)</p>
<p>?更新了y不是不一样了</p>
<h5 id="过程">过程:</h5>
<p><strong>初始化<span class="math inline">\(w=0\)</span></strong></p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209050739626.png" alt="image-20231209050739626" style="zoom:50%;"></p>
<h5 id="stepsize">stepsize</h5>
<p>SGD 代表随机梯度下降（Stochastic Gradient Descent）</p>
<p>学习率被称为η, stepsize</p>
<ul>
<li>如果使用固定学习率
<ul>
<li>太大太小都不好</li>
</ul></li>
</ul>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209050923754.png" alt="image-20231209050923754" style="zoom:50%;"></p>
<ul>
<li>也可以使用关于训练轮数的动态学习率
<ul>
<li>仍然需要多次调整衰减系数<span class="math inline">\(\alpha\)</span></li>
</ul></li>
</ul>
<h5 id="停止条件">停止条件</h5>
<p>可能的选择:</p>
<ul>
<li>最大训练次数
<ul>
<li>需要为每个数据集单独设立</li>
</ul></li>
<li>梯度条件
<ul>
<li>梯度的维度小于某阈值</li>
</ul></li>
<li>一般来说,常用的是都能分类对就行了
<ul>
<li>loss收敛前就能达到</li>
</ul></li>
</ul>
<hr>
<p>这个东西还不是太理解</p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209051425440.png" alt="image-20231209051425440" style="zoom:50%;"></p>
<p>?? 这两个具体是啥什么区别?</p>
<ul>
<li>functional margin
<ul>
<li><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209190340424.png" alt="image-20231209190340424" style="zoom:50%;"></li>
<li>加了符号(判断是否正确的) 分类映射后的x</li>
<li>衡量离超平面有多远,可以认为是信心程度</li>
</ul></li>
<li>geometric margin
<ul>
<li><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209190504366.png" alt="image-20231209190504366" style="zoom:50%;"></li>
<li>上面那个东西的标准化,</li>
<li>带方向的,到超平面的距离:</li>
</ul></li>
<li><span class="math inline">\(|g(x)|/|w|\)</span>
<ul>
<li>不带方向的距离</li>
</ul></li>
</ul>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209191349201.png" alt="image-20231209191349201" style="zoom:50%;"></p>
<h1 id="svm">SVM</h1>
<p>这一章延续上一章logistic模型和感知机,在最大化functional
margin的基础上 提出了怎么找这样一个模型,这就是SVM</p>
<p>但是,对于支持向量的阐述其实并没有很明白</p>
<p>这一章的主题大概按照以下思路</p>
<ul>
<li>怎么找最大margin
<ul>
<li>functional margin没啥意义 要最大化 geometry margin</li>
<li><strong>这里需要明确<span class="math inline">\(\gamma\)</span></strong>是什么
做了一个巧妙变形,后面的优化求解问题建立在这个假设上</li>
</ul></li>
<li>首先讨论的是硬间隔
<ul>
<li>它有若干好性质</li>
<li>在其上讨论了它的VCD和 Rademacher</li>
</ul></li>
<li>其次考虑到线性不可分数据,我们希望模型能够对他仍然有处理能力
<ul>
<li>提出了软间隔模型,对每一个误分类加以代价的情况允许存在</li>
<li>在其上讨论了它的loss,称为<span class="math inline">\(L_{Hinge}\)</span>
<ul>
<li>通过不可导处定义使得梯度下降可行</li>
</ul></li>
<li>讨论了它的损失优化和梯度</li>
</ul></li>
<li>最后讨论了对偶软间隔SVM
<ul>
<li>认为超平面法向量可以写成支持向量的线性组合</li>
<li>然后将目标函数(也就是总错误)写成了含有<span class="math inline">\(a_i\)</span>的拉格朗日形式 求最小的优化问题
<ul>
<li>在此基础上讨论了一点核技巧</li>
</ul></li>
<li>下来把<span class="math inline">\(\alpha\)</span>像<span class="math inline">\(w\)</span>一样讨论了一下他的优化过程</li>
</ul></li>
</ul>
<hr>
<p>?是0-1损失的上界有什么好处?</p>
<p><img src="/2023/09/27/Note-for-Class-CS-E4710-Machine-Learning-Supervised-Methods-D/image-20231209204644767.png" alt="image-20231209204644767" style="zoom:50%;"></p>
<p><br>
</p>
<h1 id="section"></h1>
<p>为什么|w| 就是评估过拟合的指标?</p>
<h1 id="面向考试学习">面向考试学习</h1>
<h2 id="a1">A1</h2>
<h3 id="基于l1">基于L1</h3>
<ul>
<li>给任务 ，选择对应的模型
<ul>
<li>回归/排序/多标签分类/二元分类</li>
</ul></li>
<li>对于给定结果，不同损失函数哪个代价更高？
<ul>
<li>MSE</li>
<li>MAE</li>
</ul></li>
<li>基础概念，
<ul>
<li>假设，假设类，假设集</li>
<li>损失函数的定义，用途等</li>
</ul></li>
<li>不同指标的定义，计算
<ul>
<li>precision</li>
<li>recall</li>
<li>accuracy</li>
</ul></li>
</ul>
<h2 id="a2">A2</h2>
<h3 id="基于l2">基于L2</h3>
<ul>
<li>PAC learnable 理论
<ul>
<li>里面的符号定义</li>
<li>generalisation error 怎么算，含义</li>
</ul></li>
<li>boolean conjunctions classifier
<ul>
<li>是什么</li>
<li>要达到若干准确度需要多少组合</li>
</ul></li>
</ul>
<h3 id="基于l3">基于L3</h3>
<ul>
<li>VC维的概念</li>
<li>分类器VC维是x，意味什么？</li>
<li>估计需要多少训练样本，才能让test error 到达多少
<ul>
<li>才能让基于VC dimension complexity 降到多少一下</li>
<li>和<span class="math inline">\(\delta\)</span>的关系</li>
</ul></li>
<li>generalisation bound 是什么？</li>
</ul>
<h2 id="a3">A3</h2>
<h3 id="l45">L4，5</h3>
<ul>
<li>计算Bayes classifier的 Byes error
<ul>
<li>x和y的条件概率</li>
<li>h，x的分布</li>
</ul></li>
<li>Percetron Algorithm
<ul>
<li>不同类型标准化的影响</li>
<li>类的线性可分</li>
<li>Novikoff`s theorem
<ul>
<li>标准化对于权重矩阵的影响</li>
<li>迭代次数和标准化的影响</li>
</ul></li>
</ul></li>
<li>通过 cross-validation 选择最好的超参数
<ul>
<li>nested cross validation
<ul>
<li>5-fold</li>
</ul></li>
<li>Roc-Auc 分数</li>
<li>Stochastic gradient algorithm</li>
</ul></li>
<li>row-wise normalization</li>
</ul>
<h2 id="a4">A4</h2>
<h3 id="l467">L4,6,7</h3>
<ul>
<li>polynomial kernel
<ul>
<li>explicit feature space
<ul>
<li>degree</li>
</ul></li>
<li>定义polynomial 的向量空间维度</li>
</ul></li>
</ul>
<p>（L6)</p>
<ul>
<li>SVM,随机梯度下降，原始方法</li>
</ul>
<p>（L7)</p>
<ul>
<li><p>SVC，sklearn中的</p></li>
<li><p>在 compliexity model和 emperical error 取得平衡</p></li>
<li><p>Kernel method</p>
<ul>
<li>从已知的kernel中搞到一个新的</li>
<li>给出参数 看看是不是给了一个半正定的核</li>
</ul></li>
<li><p>Gaussian kernel （RBF）</p>
<ul>
<li>对应的表达式</li>
<li>gradient</li>
<li>结合 Gaussian kernel 和 概率密度函数的gradient</li>
<li>看看新的核是不是半正定</li>
</ul></li>
</ul>
]]></content>
      <categories>
        <category>Substance</category>
      </categories>
      <tags>
        <tag>Note</tag>
        <tag>Course</tag>
      </tags>
  </entry>
</search>
