{"meta":{"version":1,"warehouse":"4.0.1"},"models":{"Asset":[{"_id":"node_modules/hexo-theme-next/source/images/2.jpg","path":"images/2.jpg","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/images/apple-touch-icon-next.png","path":"images/apple-touch-icon-next.png","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/images/avatar.gif","path":"images/avatar.gif","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/css/main.styl","path":"css/main.styl","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/images/favicon-16x16-next.png","path":"images/favicon-16x16-next.png","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/images/favicon-32x32-next.png","path":"images/favicon-32x32-next.png","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/css/noscript.styl","path":"css/noscript.styl","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/images/logo-algolia-nebula-blue-full.svg","path":"images/logo-algolia-nebula-blue-full.svg","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/images/logo.svg","path":"images/logo.svg","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/images/mine-16x16.png","path":"images/mine-16x16.png","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/images/mine-32x32.png","path":"images/mine-32x32.png","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/bookmark.js","path":"js/bookmark.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/images/taffy.png","path":"images/taffy.png","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/comments-buttons.js","path":"js/comments-buttons.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/comments.js","path":"js/comments.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/config.js","path":"js/config.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/motion.js","path":"js/motion.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/next-boot.js","path":"js/next-boot.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/pjax.js","path":"js/pjax.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/schedule.js","path":"js/schedule.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/utils.js","path":"js/utils.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/schemes/muse.js","path":"js/schemes/muse.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/fancybox.js","path":"js/third-party/fancybox.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/pace.js","path":"js/third-party/pace.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/quicklink.js","path":"js/third-party/quicklink.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/rating.js","path":"js/third-party/rating.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/analytics/baidu-analytics.js","path":"js/third-party/analytics/baidu-analytics.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/analytics/google-analytics.js","path":"js/third-party/analytics/google-analytics.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/analytics/growingio.js","path":"js/third-party/analytics/growingio.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/chat/chatra.js","path":"js/third-party/chat/chatra.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/chat/gitter.js","path":"js/third-party/chat/gitter.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/chat/tidio.js","path":"js/third-party/chat/tidio.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/changyan.js","path":"js/third-party/comments/changyan.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/disqusjs.js","path":"js/third-party/comments/disqusjs.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/gitalk.js","path":"js/third-party/comments/gitalk.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/disqus.js","path":"js/third-party/comments/disqus.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/isso.js","path":"js/third-party/comments/isso.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/livere.js","path":"js/third-party/comments/livere.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/utterances.js","path":"js/third-party/comments/utterances.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/math/mathjax.js","path":"js/third-party/math/mathjax.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/math/katex.js","path":"js/third-party/math/katex.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/search/algolia-search.js","path":"js/third-party/search/algolia-search.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/search/local-search.js","path":"js/third-party/search/local-search.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/statistics/firestore.js","path":"js/third-party/statistics/firestore.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/statistics/lean-analytics.js","path":"js/third-party/statistics/lean-analytics.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/tags/mermaid.js","path":"js/third-party/tags/mermaid.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/tags/pdf.js","path":"js/third-party/tags/pdf.js","modified":1,"renderable":1}],"Cache":[{"_id":"source/_posts/Philip-Larkin.md","hash":"50d8765ed307461e79f02235f0f456614ecd3b9d","modified":1656416756876},{"_id":"source/_posts/About-Moby-Dick.md","hash":"389704bcc22556e0fab36b6ecba361c895408e56","modified":1655723769616},{"_id":"source/_posts/testpic.md","hash":"558b240a137a1ed7af8d894c8014fb603c503dd9","modified":1655723769626},{"_id":"source/about/index.md","hash":"52d246b19a283f0a9f8e9005bdd53115444b21ed","modified":1655723769631},{"_id":"source/tags/index.md","hash":"1c3722c4b4af5a2ea8970da1bfec9b8c68159367","modified":1655723769632},{"_id":"source/_posts/论文笔记-Automated-Generation-of-Constraints-from-Use-Case-Specifications-to-Support-System-Testing.md","hash":"21074f453069bc87a6ad51df0165aa84dc2a37cc","modified":1656416720079},{"_id":"source/_posts/火柴.md","hash":"c9636e5142f1d010b7b900855e2a975280f845fd","modified":1655723769630},{"_id":"source/_posts/SPT-Code.md","hash":"ca256b0d0476d2b3b8e9cc05ab172aa136403cbb","modified":1655730887373},{"_id":"source/categories/index.md","hash":"3123f55189718134e31eb51d72ddd9e68ecd4a18","modified":1655723769631},{"_id":"source/_posts/SPT-Code/image-20220527233143119.png","hash":"d089096a54c50ed0ab12ae4dacbe4a85b2bcb6bc","modified":1655723769625},{"_id":"source/_posts/SPT-Code/image-20220527231806850.png","hash":"bdd3bb79709080c4bb8493ae66cd290e7b8b8237","modified":1655723769621},{"_id":"source/_posts/论文笔记-Automated-Generation-of-Constraints-from-Use-Case-Specifications-to-Support-System-Testing/image-20220625212958549.png","hash":"8d95c96837046ecb47d5cf99db283be61e67d59a","modified":1656163798553},{"_id":"source/_posts/SPT-Code/image-20220527233752649.png","hash":"add232d275639723c148095061934b6857daf254","modified":1655723769626},{"_id":"source/_posts/testpic/image-20220523211852621.png","hash":"f443eb7116b28a70a135256d0d4521e3710e45ff","modified":1655723769627},{"_id":"source/_posts/论文笔记-Automated-Generation-of-Constraints-from-Use-Case-Specifications-to-Support-System-Testing/image-20220625181049650.png","hash":"8db94f3137b6f818297c81d3f2609df821327e2b","modified":1656151849678},{"_id":"source/_posts/论文笔记-Automated-Generation-of-Constraints-from-Use-Case-Specifications-to-Support-System-Testing/image-20220625173337030.png","hash":"9d4e40d0f44f88d561df76535dccb95bdc58931f","modified":1656149617047},{"_id":"source/_posts/论文笔记-Automated-Generation-of-Constraints-from-Use-Case-Specifications-to-Support-System-Testing/image-20220626184416896.png","hash":"c3c77c4c9784e8575192d727ee35e4764f73a9b4","modified":1656240256920},{"_id":"source/_posts/论文笔记-Automated-Generation-of-Constraints-from-Use-Case-Specifications-to-Support-System-Testing/image-20220626184653142.png","hash":"93b29c9dd359d85f1f5f9c8a3a010d1d84ee7fc2","modified":1656240413168},{"_id":"source/_posts/SPT-Code/image-20220527230530594.png","hash":"ca5795fe19159888c5a97acb4f35f489db7ab7ed","modified":1655723769620},{"_id":"source/_posts/论文笔记-Automated-Generation-of-Constraints-from-Use-Case-Specifications-to-Support-System-Testing/image-20220626184748233.png","hash":"b2aa19c215213b86de9410831101e13b3e6bf246","modified":1656240468250},{"_id":"source/_posts/论文笔记-Automated-Generation-of-Constraints-from-Use-Case-Specifications-to-Support-System-Testing/image-20220626184413924.png","hash":"c3c77c4c9784e8575192d727ee35e4764f73a9b4","modified":1656240253947},{"_id":"source/_posts/SPT-Code/image-20220527232629535.png","hash":"22e81124540e148c61cb00bc3e3f986c71713087","modified":1655723769624},{"_id":"source/_posts/论文笔记-Automated-Generation-of-Constraints-from-Use-Case-Specifications-to-Support-System-Testing/image-20220628194141482.png","hash":"da786f68967b2649b93e0542390542246b16a1f5","modified":1656416501560},{"_id":"source/_posts/testpic/image-20220524021029166.png","hash":"a4f4e62846dc739c9d970f5eea1b4414740f2184","modified":1655723769630},{"_id":"source/_posts/论文笔记-Automated-Generation-of-Constraints-from-Use-Case-Specifications-to-Support-System-Testing/image-20220628194155803.png","hash":"d9265012432b33c71d848c67b0a85228acdcbcb3","modified":1656416515874},{"_id":"node_modules/hexo-theme-next/_vendors.yml","hash":"8c2886a6af624f04fc6a376daf48c0698ea71bf7","modified":1655723668674},{"_id":"node_modules/hexo-theme-next/package.json","hash":"b32be32269dd2e241790a766a60357bfe13f2d45","modified":1655723668096},{"_id":"node_modules/hexo-theme-next/languages/README.md","hash":"b2567e32805dda79601157351a07e5ca9fe01315","modified":1655723668105},{"_id":"node_modules/hexo-theme-next/_config.yml","hash":"916bd29a95250cb1fb778d88f6b1864f5db26c3e","modified":1655730246258},{"_id":"node_modules/hexo-theme-next/README.md","hash":"56638e4978154a2f2a3f03ba84047b77b4a499cc","modified":1655723668106},{"_id":"node_modules/hexo-theme-next/LICENSE.md","hash":"68fc9a03d50fd4b5ea97092b05967d1819dea2c4","modified":1655723668102},{"_id":"node_modules/hexo-theme-next/languages/ar.yml","hash":"bca66db21c015dbd32970d8708b898518a773e1e","modified":1655723668675},{"_id":"node_modules/hexo-theme-next/languages/bn.yml","hash":"fccbf2855392186e11daa8590121073594037b7b","modified":1655723668676},{"_id":"node_modules/hexo-theme-next/languages/de.yml","hash":"4be7b8b76c81bf1853eb36d2e874b17546a0e792","modified":1655723668677},{"_id":"node_modules/hexo-theme-next/languages/es.yml","hash":"b813da5aed9d73b809133db4dfb08f90ec56afd9","modified":1655723668680},{"_id":"node_modules/hexo-theme-next/languages/en.yml","hash":"814d81c27fed736055ee300e0a6505b26ff4313c","modified":1655723668679},{"_id":"node_modules/hexo-theme-next/languages/fa.yml","hash":"6456d40dd42f44101d9d6e7054e9884e9163f948","modified":1655723668681},{"_id":"node_modules/hexo-theme-next/languages/fr.yml","hash":"b15dc05afdc94de02e5d3fee4f8d3dc5594dd37e","modified":1655723668682},{"_id":"node_modules/hexo-theme-next/languages/it.yml","hash":"c1eeab4992c76bfd436bb205ce58b1cfeef55ee6","modified":1655723668684},{"_id":"node_modules/hexo-theme-next/languages/id.yml","hash":"14e794db4eca36b257994d81eb513e61d1edcbd6","modified":1655723668683},{"_id":"node_modules/hexo-theme-next/languages/ko.yml","hash":"819c19eb9d142e5411f77cf3821d90f740ee114a","modified":1655723668685},{"_id":"node_modules/hexo-theme-next/languages/ja.yml","hash":"d48c4157e0e02e847aac7b513580d3364c81948c","modified":1655723668685},{"_id":"node_modules/hexo-theme-next/languages/pt-BR.yml","hash":"a1f27b3a592fc58f17d247f5563ff4a90a3da5f2","modified":1655723668687},{"_id":"node_modules/hexo-theme-next/languages/nl.yml","hash":"ecb8e39c6225f3c068a5fdd569ee7dafd5c41a1f","modified":1655723668686},{"_id":"node_modules/hexo-theme-next/languages/pt.yml","hash":"63a3e1e728ba5e6e22150de7331bb8a654f34960","modified":1655723668688},{"_id":"node_modules/hexo-theme-next/languages/ru.yml","hash":"8c2b6361f2de17561c1a3eede2bf47b4e2ba6ce5","modified":1655723668688},{"_id":"node_modules/hexo-theme-next/languages/si.yml","hash":"615d18d044f44df476d6bfbf73f7b0edc2632168","modified":1655723668689},{"_id":"node_modules/hexo-theme-next/languages/tk.yml","hash":"519239e35c3bda7b62b00ff5d34644f45b16fe6a","modified":1655723668692},{"_id":"node_modules/hexo-theme-next/languages/tr.yml","hash":"0bebba73d6f06c7dad61f80c0d7ad5f6f1791a01","modified":1655723668693},{"_id":"node_modules/hexo-theme-next/languages/uk.yml","hash":"7dd24580c0865c5a7bc4d391855045366a598936","modified":1655723668693},{"_id":"node_modules/hexo-theme-next/languages/vi.yml","hash":"c669c34da544a563ceae3e196addc9df6a78e024","modified":1655723668695},{"_id":"node_modules/hexo-theme-next/languages/zh-CN.yml","hash":"5a3ab21210304efef736e96bad254f789f42c567","modified":1655723668695},{"_id":"node_modules/hexo-theme-next/languages/zh-HK.yml","hash":"f195bb0502ffe66e850077a1af1033455ea65f93","modified":1655723668696},{"_id":"node_modules/hexo-theme-next/languages/zh-TW.yml","hash":"92256b90028de9a1e79c6bc0e5885b93e7fb4b17","modified":1655723668697},{"_id":"node_modules/hexo-theme-next/layout/_layout.njk","hash":"20e4160cd0deb4fa272cc3aed0f43520b3cf4a9c","modified":1655723668107},{"_id":"node_modules/hexo-theme-next/docs/AUTHORS.md","hash":"a648823121563c34a177ae91f5a774b5e29f01a0","modified":1655723668098},{"_id":"node_modules/hexo-theme-next/docs/AGPL3.md","hash":"0d2b8c5fa8a614723be0767cc3bca39c49578036","modified":1655723668097},{"_id":"node_modules/hexo-theme-next/layout/archive.njk","hash":"d759f4d2cf5ddc6875ea250113a00662c1caf6d1","modified":1655723668110},{"_id":"node_modules/hexo-theme-next/layout/category.njk","hash":"c68b7343d0f8145010f93351908cc36ef6212ec1","modified":1655723668120},{"_id":"node_modules/hexo-theme-next/docs/LICENSE.txt","hash":"f5b14f791b7cfa1d16da981d929152e088a5d1b8","modified":1655723668671},{"_id":"node_modules/hexo-theme-next/layout/post.njk","hash":"6abeb85fb3e4c382ed4bb6049b12a807e6226e67","modified":1655723668493},{"_id":"node_modules/hexo-theme-next/layout/page.njk","hash":"6c40aa438c658eb7f0cd0f6a759f18b43e7e8f93","modified":1655723668485},{"_id":"node_modules/hexo-theme-next/layout/index.njk","hash":"dd63e488ae8cc144335a5958acedf6a16edd7a92","modified":1655723668476},{"_id":"node_modules/hexo-theme-next/layout/tag.njk","hash":"9e16ba20c28a7f2c6bc75aa427f48122301a30aa","modified":1655723668497},{"_id":"node_modules/hexo-theme-next/layout/_macro/post-collapse.njk","hash":"1a30d751871dabfa80940042ddb1f77d07d830b9","modified":1655723668487},{"_id":"node_modules/hexo-theme-next/layout/_macro/post.njk","hash":"434b3e76a040a816169e1929657e4176e7b8164c","modified":1655723668492},{"_id":"node_modules/hexo-theme-next/docs/ru/README.md","hash":"6c82bfd2ec8248c248da701f091b548a7a133580","modified":1655723668103},{"_id":"node_modules/hexo-theme-next/docs/zh-CN/CODE_OF_CONDUCT.md","hash":"7a06d443f374bd1e84294067a0ac796afd9fbe60","modified":1655723668101},{"_id":"node_modules/hexo-theme-next/layout/_macro/sidebar.njk","hash":"eb786e8b35e354287cda345c524cd35ec955f692","modified":1655723668495},{"_id":"node_modules/hexo-theme-next/docs/zh-CN/CONTRIBUTING.md","hash":"a089f7a8368ab0b7d7b9b7ec0ac3767a453435df","modified":1655723668101},{"_id":"node_modules/hexo-theme-next/docs/zh-CN/README.md","hash":"ccf27b9249524b9fec1c15497b4353c8d1748c6c","modified":1655723668104},{"_id":"node_modules/hexo-theme-next/layout/_partials/comments.njk","hash":"d0c470b0f6690aa217e9ada848c5e2e73fb27c6f","modified":1655723668124},{"_id":"node_modules/hexo-theme-next/layout/_partials/languages.njk","hash":"e43f22198cccb5f6e306b1ce0d28d12a4fb891f8","modified":1655723668478},{"_id":"node_modules/hexo-theme-next/layout/_partials/pagination.njk","hash":"9876dbfc15713c7a47d4bcaa301f4757bd978269","modified":1655723668486},{"_id":"node_modules/hexo-theme-next/layout/_partials/footer.njk","hash":"cfb7001c7766a50eba23c990179343cdc42b23ae","modified":1655726418385},{"_id":"node_modules/hexo-theme-next/layout/_scripts/index.njk","hash":"6668878a0f9a1166c6a879755f54a08d942da870","modified":1655723668472},{"_id":"node_modules/hexo-theme-next/layout/_third-party/fancybox.njk","hash":"844559f46e2ff1c8be234d5763703106e2072a7b","modified":1655723668127},{"_id":"node_modules/hexo-theme-next/layout/_partials/widgets.njk","hash":"852a750524decf1efa587cd52b09e387ed8315de","modified":1655723668502},{"_id":"node_modules/hexo-theme-next/layout/_third-party/index.njk","hash":"d41eeb262978e34de4679d8971a9e7ac5d90ecbc","modified":1655723668474},{"_id":"node_modules/hexo-theme-next/layout/_third-party/pace.njk","hash":"d7ad5714079f7f65446f880baf14722435ca9061","modified":1655723668483},{"_id":"node_modules/hexo-theme-next/scripts/events/index.js","hash":"3ce10d4cce94e3d4c482c2e18bb6f0f0ca380d3d","modified":1655723667992},{"_id":"node_modules/hexo-theme-next/layout/_third-party/quicklink.njk","hash":"0efed71ed530447718c4ea5bbd5fc8695b0b0d5f","modified":1655723668493},{"_id":"node_modules/hexo-theme-next/scripts/filters/default-injects.js","hash":"872f01cb10e422a648ea505436532e776e92926b","modified":1655723667977},{"_id":"node_modules/hexo-theme-next/scripts/filters/locals.js","hash":"9eb5310664759931287dd28ea39165dfb67f12ed","modified":1655723668000},{"_id":"node_modules/hexo-theme-next/scripts/filters/minify.js","hash":"f160e39943e39d7276da86adb47c3f08e5f22c7a","modified":1655723668004},{"_id":"node_modules/hexo-theme-next/layout/_scripts/vendors.njk","hash":"be80b9fe415a9a09d74c28e230995fd292dfc123","modified":1655723668501},{"_id":"node_modules/hexo-theme-next/scripts/helpers/engine.js","hash":"d292b78485e8e8055712b0ed6de7cf559c5fbdcd","modified":1655723667982},{"_id":"node_modules/hexo-theme-next/scripts/filters/post.js","hash":"30e03a1d4828259f82d46e64cbfe2955b6cff9a9","modified":1655723668014},{"_id":"node_modules/hexo-theme-next/scripts/helpers/font.js","hash":"3394185a7f0393c16ce52c8028f90da3e9239c55","modified":1655723667986},{"_id":"node_modules/hexo-theme-next/scripts/helpers/next-config.js","hash":"226fccbe9c93265e65a300e3cb4bf6f9065cfdd7","modified":1655723668008},{"_id":"node_modules/hexo-theme-next/scripts/helpers/navigation.js","hash":"78107021101553c3d23e89290f7530b60cf4aa86","modified":1655723668007},{"_id":"node_modules/hexo-theme-next/scripts/helpers/next-url.js","hash":"a11b71ba0c5012e2cdcab31c15439156b215563e","modified":1655723668009},{"_id":"node_modules/hexo-theme-next/scripts/tags/button.js","hash":"c6ad2ed544fbb25ecb5d820c36e76302504271b7","modified":1655723667957},{"_id":"node_modules/hexo-theme-next/scripts/helpers/next-vendors.js","hash":"afdd6a188a74c188f0dd154fac70efd4080ca262","modified":1655723668010},{"_id":"node_modules/hexo-theme-next/scripts/tags/caniuse.js","hash":"935a311142a409c1896b3ae3f01fe7a9e2db1134","modified":1655723667958},{"_id":"node_modules/hexo-theme-next/scripts/tags/center-quote.js","hash":"92c19d796bdb3320df9caea59bf52df7a95d9da9","modified":1655723667958},{"_id":"node_modules/hexo-theme-next/scripts/tags/group-pictures.js","hash":"9ed799c329abf830f623689d7e136991256a24ca","modified":1655723667989},{"_id":"node_modules/hexo-theme-next/scripts/tags/label.js","hash":"8a73348186113bae0a51ea2f891c1bb882fab05a","modified":1655723667996},{"_id":"node_modules/hexo-theme-next/scripts/tags/link-grid.js","hash":"18a483c2d5afd701f6080ffdddf2d1321370336c","modified":1655723667998},{"_id":"node_modules/hexo-theme-next/scripts/tags/index.js","hash":"17f9451ce1f10f78437f52218757d38d4e1591b0","modified":1655723667993},{"_id":"node_modules/hexo-theme-next/scripts/tags/mermaid.js","hash":"4fb01ca650fa8b256b8d48f50dc1b18350bd3d6d","modified":1655723668002},{"_id":"node_modules/hexo-theme-next/scripts/tags/pdf.js","hash":"344636b6fd7e27e8831c1e194039afc0d61931cd","modified":1655723668012},{"_id":"node_modules/hexo-theme-next/scripts/tags/note.js","hash":"7b94ddb46b7d4b0fe815f2fbe4bd375f07f55363","modified":1655723668010},{"_id":"node_modules/hexo-theme-next/scripts/tags/tabs.js","hash":"0eabe51da40b4b13e16419c8fe02452d9a4fef73","modified":1655723668017},{"_id":"node_modules/hexo-theme-next/scripts/tags/video.js","hash":"2ee926448583be8f95af1f2884ae2c9c4830151d","modified":1655723668095},{"_id":"node_modules/hexo-theme-next/layout/_third-party/rating.njk","hash":"1bcdbc7fde26d6d9ef4e7fa43ffcff5a9506b20e","modified":1655723668494},{"_id":"node_modules/hexo-theme-next/source/css/_colors.styl","hash":"3c6798c10cc220d83481cb3f3782e78558cee789","modified":1655723668505},{"_id":"node_modules/hexo-theme-next/source/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1655723668502},{"_id":"node_modules/hexo-theme-next/source/images/avatar.gif","hash":"2dbc3e2f2d624b2ca1afe6edc2ca17307f1950c8","modified":1655723667951},{"_id":"node_modules/hexo-theme-next/source/css/_mixins.styl","hash":"32d31cb5a155681c19f5ad0bb56dcb08429f93ef","modified":1655723668514},{"_id":"node_modules/hexo-theme-next/source/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1655723668503},{"_id":"node_modules/hexo-theme-next/source/css/main.styl","hash":"78ce791cc4ac95386cf6839ca72f5f7b51f86ee9","modified":1655723668639},{"_id":"node_modules/hexo-theme-next/source/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1655723668504},{"_id":"node_modules/hexo-theme-next/source/css/noscript.styl","hash":"263eddabfae40e54c0591e7baa8403ade8cdd56d","modified":1655723668645},{"_id":"node_modules/hexo-theme-next/source/images/logo.svg","hash":"2cb74fd3ea2635e015eabc58a8d488aed6cf6417","modified":1655723668670},{"_id":"node_modules/hexo-theme-next/source/images/logo-algolia-nebula-blue-full.svg","hash":"b85e274207b1392782476a0430feac98db1e7da0","modified":1655723668670},{"_id":"node_modules/hexo-theme-next/source/images/mine-16x16.png","hash":"46a56b8ee580a508fedfceff822c9dc8be0fa20d","modified":1653934034099},{"_id":"node_modules/hexo-theme-next/source/images/mine-32x32.png","hash":"13e8c0230b242bc68c3b54267ef0024d7fb11667","modified":1653934034102},{"_id":"node_modules/hexo-theme-next/source/js/bookmark.js","hash":"0f563ffbf05fad30e854e413ab17ff7164ab5a53","modified":1655723667956},{"_id":"node_modules/hexo-theme-next/source/js/comments-buttons.js","hash":"1a7344440321713426a0b2ab17e276b5bdf85ade","modified":1655723667965},{"_id":"node_modules/hexo-theme-next/source/js/comments.js","hash":"66ae2e26ea36a41b72c638ea8b220296638ae952","modified":1655723667966},{"_id":"node_modules/hexo-theme-next/source/js/config.js","hash":"4c4ebbe3b3f3841a26f9d5af6d0ba8bc6da01c54","modified":1655723667970},{"_id":"node_modules/hexo-theme-next/source/js/pjax.js","hash":"919f5281c4a04d11cfd94573ecf57b3dbabd3cc8","modified":1655723668014},{"_id":"node_modules/hexo-theme-next/source/js/motion.js","hash":"f7c825cbff11885fa0dffa64824fd00e505d6a8d","modified":1655723668004},{"_id":"node_modules/hexo-theme-next/source/js/next-boot.js","hash":"da0f07f9eaaa83de70128b0feaea3fdadb90457a","modified":1655723668007},{"_id":"node_modules/hexo-theme-next/layout/_partials/head/head.njk","hash":"0ba2bf0266f1fcb8edbd961869f8521b29685c56","modified":1655723668469},{"_id":"node_modules/hexo-theme-next/layout/_partials/head/head-unique.njk","hash":"8da52a144060db1a0a088ccb2e6cc8376d1fce70","modified":1655723668468},{"_id":"node_modules/hexo-theme-next/source/js/schedule.js","hash":"a1333258726caf84f368a8f8454639c7dc1626bb","modified":1655723668016},{"_id":"node_modules/hexo-theme-next/layout/_partials/header/brand.njk","hash":"aff4613756456be26415febc668860fdab8d33c5","modified":1655723668114},{"_id":"node_modules/hexo-theme-next/source/js/utils.js","hash":"200088bfd042f5304b2a04befab0829148845e0e","modified":1655723668092},{"_id":"node_modules/hexo-theme-next/layout/_partials/header/index.njk","hash":"650de421a8ce4cf685428ffbe0087ff84cbd1356","modified":1655723668470},{"_id":"node_modules/hexo-theme-next/layout/_partials/header/menu-item.njk","hash":"41a8b0cc16f60fa085cb719d07216d86b6bc4bf8","modified":1655723668481},{"_id":"node_modules/hexo-theme-next/layout/_partials/header/menu.njk","hash":"ee6fc2f111572d3eeab0a2fecbb2d6b3e37ab26b","modified":1655723668481},{"_id":"node_modules/hexo-theme-next/layout/_partials/header/sub-menu.njk","hash":"06480d8ec5f0b87eafd47f082f07968d7282dd5c","modified":1655723668497},{"_id":"node_modules/hexo-theme-next/layout/_partials/post/post-copyright.njk","hash":"133942922e34abae9e4de7ea5591d77c0caa4b37","modified":1655723668488},{"_id":"node_modules/hexo-theme-next/layout/_partials/post/post-followme.njk","hash":"154df0bb323c332d8c25343f258ee865e5553423","modified":1655723668489},{"_id":"node_modules/hexo-theme-next/layout/_partials/post/post-footer.njk","hash":"bde2c7356d9362972bde41cc206d5816f8ed714d","modified":1655723668489},{"_id":"node_modules/hexo-theme-next/layout/_partials/post/post-reward.njk","hash":"002b51d0cae3f2e2e008bdc58be90c728282de5b","modified":1655723668492},{"_id":"node_modules/hexo-theme-next/layout/_partials/post/post-meta.njk","hash":"9fa47e4fb342811da590ee4adc91cf81118c0a39","modified":1655723668490},{"_id":"node_modules/hexo-theme-next/layout/_partials/page/schedule.njk","hash":"0f4bc8e257da60f77c0c1738607b2bde55810684","modified":1655723668494},{"_id":"node_modules/hexo-theme-next/layout/_partials/post/post-related.njk","hash":"57eca76cfbbe9a65bc2a77f1deebf003ed335673","modified":1655723668491},{"_id":"node_modules/hexo-theme-next/layout/_partials/page/breadcrumb.njk","hash":"89825e75cc45e9709fa6ba89883669eedaff6f46","modified":1655723668116},{"_id":"node_modules/hexo-theme-next/layout/_partials/page/page-header.njk","hash":"7ed4f102a1825195cff8d7995bf9219f323a9034","modified":1655723668484},{"_id":"node_modules/hexo-theme-next/layout/_partials/page/categories.njk","hash":"17156d99941f28a225951ffdcfa9a115e20dc2d2","modified":1655723668118},{"_id":"node_modules/hexo-theme-next/layout/_partials/search/algolia-search.njk","hash":"efb2b6f19df02ba5ae623a1f274fff52aed21e6f","modified":1655723668108},{"_id":"node_modules/hexo-theme-next/layout/_partials/page/tags.njk","hash":"a18d1598e36cc72f2b0b24c3cc3c5990dfaa3254","modified":1655723668498},{"_id":"node_modules/hexo-theme-next/layout/_partials/search/index.njk","hash":"8f6f256ab3b351ffc80f1f3f1d9834e9a7cfac31","modified":1655723668471},{"_id":"node_modules/hexo-theme-next/layout/_partials/search/localsearch.njk","hash":"661f7acae43f0be694266323320f977d84119abe","modified":1655723668479},{"_id":"node_modules/hexo-theme-next/layout/_third-party/math/index.njk","hash":"abf37fc55aa86702118e8fdf5bf2d389dd589aa0","modified":1655723668475},{"_id":"node_modules/hexo-theme-next/layout/_third-party/math/katex.njk","hash":"d82c24136bbd3443b85f07f5579845833b594684","modified":1655723668477},{"_id":"node_modules/hexo-theme-next/layout/_partials/sidebar/site-overview.njk","hash":"3d8591bb92df77ceb9d5b07bc76da1ca89e5bd76","modified":1655723668496},{"_id":"node_modules/hexo-theme-next/layout/_third-party/chat/chatra.njk","hash":"d7263fca16d0278ccf1f6aa1c6df6902a6344a09","modified":1655723668122},{"_id":"node_modules/hexo-theme-next/layout/_third-party/math/mathjax.njk","hash":"3677017fd4572b158311f5f5d870590ab25184e0","modified":1655723668480},{"_id":"node_modules/hexo-theme-next/layout/_third-party/analytics/baidu-analytics.njk","hash":"6215309aee028dcb734452beec448c5afb6c63fc","modified":1655723668113},{"_id":"node_modules/hexo-theme-next/layout/_third-party/chat/gitter.njk","hash":"f8cc14b7aa949999a1faaeb7855e2f20b59a386d","modified":1655723668130},{"_id":"node_modules/hexo-theme-next/layout/_third-party/analytics/cloudflare.njk","hash":"a5b8297c2c383124dd6a56e256ecc0c0dcf489be","modified":1655723668123},{"_id":"node_modules/hexo-theme-next/layout/_third-party/chat/tidio.njk","hash":"02aab857c27fc103216029be991688b12a73a525","modified":1655723668498},{"_id":"node_modules/hexo-theme-next/layout/_third-party/analytics/google-analytics.njk","hash":"d89066ff53879693f023e540d59c86137172c529","modified":1655723668131},{"_id":"node_modules/hexo-theme-next/layout/_third-party/analytics/growingio.njk","hash":"8afaa772c390bd9d53a5cff9645ac3168334eb98","modified":1655723668466},{"_id":"node_modules/hexo-theme-next/layout/_third-party/analytics/index.njk","hash":"45477a04cf2b3c077061c8c3ada216c1ae288e0e","modified":1655723668473},{"_id":"node_modules/hexo-theme-next/layout/_third-party/comments/changyan.njk","hash":"d1c950f8fbdf85e7a3eae5463767a89e858e8220","modified":1655723668121},{"_id":"node_modules/hexo-theme-next/layout/_third-party/comments/disqus.njk","hash":"9375b19a89b7fa9474e558d085af5448d4c5c50c","modified":1655723668125},{"_id":"node_modules/hexo-theme-next/layout/_third-party/comments/disqusjs.njk","hash":"0749cb6902baecdfd01f779a2a2513f6d2f6a823","modified":1655723668126},{"_id":"node_modules/hexo-theme-next/layout/_third-party/comments/gitalk.njk","hash":"b63b7e2ede0d3e66e732fa1a06bda9b19e1e85d4","modified":1655723668130},{"_id":"node_modules/hexo-theme-next/scripts/events/lib/config.js","hash":"c8b59b404f5d2a0b3b5cd1a6c9a10af5f30e43b5","modified":1655723667969},{"_id":"node_modules/hexo-theme-next/layout/_third-party/comments/isso.njk","hash":"64cc3bdaf644fd32c0d0a247f29f5b6904da9af3","modified":1655723668476},{"_id":"node_modules/hexo-theme-next/scripts/events/lib/highlight.js","hash":"6aec7b2c38c50989a23bfaa0d560e75c7f553e12","modified":1655723667991},{"_id":"node_modules/hexo-theme-next/scripts/events/lib/injects.js","hash":"d987709267a1bc6e5014411e9983d7c49c102c16","modified":1655723667994},{"_id":"node_modules/hexo-theme-next/scripts/events/lib/navigation.js","hash":"dd3562686d95a50375e6fd32e717ccb0d99c1e3d","modified":1655723668006},{"_id":"node_modules/hexo-theme-next/scripts/events/lib/utils.js","hash":"ec996d0673f766167c86df0966e9da1ae036e103","modified":1655723668019},{"_id":"node_modules/hexo-theme-next/scripts/events/lib/vendors.js","hash":"64e4024376b51fe81be7ad80235abdf0a83853bd","modified":1655723668094},{"_id":"node_modules/hexo-theme-next/layout/_third-party/comments/livere.njk","hash":"3b13b09fba84ec6000886890a6710736a2b8fafe","modified":1655723668479},{"_id":"node_modules/hexo-theme-next/layout/_third-party/analytics/microsoft-clarity.njk","hash":"9dc00fcb0a05899f048eace9f9160b78956655d5","modified":1655723668483},{"_id":"node_modules/hexo-theme-next/layout/_third-party/comments/utterances.njk","hash":"5a94032bc3512a10ad4328fc19ec07b819a1d687","modified":1655723668499},{"_id":"node_modules/hexo-theme-next/layout/_third-party/search/algolia-search.njk","hash":"24ed76e0c72a25ac152820c750a05826a706b6f4","modified":1655723668109},{"_id":"node_modules/hexo-theme-next/layout/_third-party/search/localsearch.njk","hash":"e45ea3542cdc9ed7ec8447b5e6f35df4c5e82758","modified":1655723668480},{"_id":"node_modules/hexo-theme-next/layout/_third-party/tags/pdf.njk","hash":"2c81984cc4f5123103460442f6e046f5b6c97127","modified":1655723668487},{"_id":"node_modules/hexo-theme-next/layout/_third-party/statistics/busuanzi-counter.njk","hash":"a4bc501da0f22f7e420f0ca47e83988ce90b1368","modified":1655723668117},{"_id":"node_modules/hexo-theme-next/layout/_third-party/tags/mermaid.njk","hash":"099e031f52fb8e47b3af5b2684737efc9e643ee7","modified":1655723668482},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/changyan.js","hash":"7fa8701c86485b2fe7324e017101a32417902397","modified":1655723667960},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/common.js","hash":"19a402a225c31edffc50f202a14e0d582d3db23e","modified":1655723667966},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/default-config.js","hash":"93ee5f9109dad885dc38c49bcee630c10f9dce6e","modified":1655723667973},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/disqus.js","hash":"7f71d6b271ba65ff333d5682e7575711d368c0d2","modified":1655723667978},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/gitalk.js","hash":"7bb7dafdd7f6bca8464b54e17e552ce7f1714195","modified":1655723667986},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/disqusjs.js","hash":"135b87d151055eefdbc711d9e704b112b3214a84","modified":1655723667980},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/isso.js","hash":"ff8b5b5145220a17d0ecd9508ba9bd2d3b2da47d","modified":1655723667994},{"_id":"node_modules/hexo-theme-next/layout/_third-party/statistics/firestore.njk","hash":"d32ebe94560fa95824478ebbff531bffc47b194d","modified":1655723668128},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/livere.js","hash":"5a07d8bb52bc1d51a624ca8db54be144566c306b","modified":1655723667998},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/utterances.js","hash":"d3bded697bc32dace689d2a6dfb6eb7514169d15","modified":1655723668093},{"_id":"node_modules/hexo-theme-next/layout/_third-party/statistics/index.njk","hash":"568ddf7955d11d93fb5e842b403a7ac8b1b7fdb1","modified":1655723668475},{"_id":"node_modules/hexo-theme-next/source/css/_variables/Gemini.styl","hash":"96e0a7c2a65ce68215e17e369085b2ea2f1334f2","modified":1655723668619},{"_id":"node_modules/hexo-theme-next/layout/_third-party/statistics/lean-analytics.njk","hash":"2446e748cdc102c78492216319ac02148db7daf6","modified":1655723668478},{"_id":"node_modules/hexo-theme-next/source/css/_variables/Mist.styl","hash":"e1fbf169b9b6a194b518240cbd06ec3c48b83d61","modified":1655723668642},{"_id":"node_modules/hexo-theme-next/source/css/_variables/Muse.styl","hash":"e3be898f5ebcf435a26542653a9297ff2c71aeb0","modified":1655723668643},{"_id":"node_modules/hexo-theme-next/source/css/_variables/Pisces.styl","hash":"c65536a128b9bc9dbe2fbb1b235a3cded2891002","modified":1655723668647},{"_id":"node_modules/hexo-theme-next/source/js/third-party/pace.js","hash":"0ef04218b93561ba4d0ff420d556c3d90a756d32","modified":1655723668011},{"_id":"node_modules/hexo-theme-next/source/js/third-party/fancybox.js","hash":"c098d14e65dd170537134358d4b8359ad0539c2c","modified":1655723667983},{"_id":"node_modules/hexo-theme-next/source/js/schemes/muse.js","hash":"9794bd4fc6a458322949d6a0ade89cd1026bc69f","modified":1655723668005},{"_id":"node_modules/hexo-theme-next/source/css/_variables/base.styl","hash":"163c7441d777bee87042d475e6ce0fde199add28","modified":1655723668609},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/back-to-top.styl","hash":"bab653bcf226311381e8411a0492202f1bf1fce9","modified":1655723668606},{"_id":"node_modules/hexo-theme-next/source/js/third-party/quicklink.js","hash":"eed02e6fced8e5a653077205d4d4d7834ca71472","modified":1655723668015},{"_id":"node_modules/hexo-theme-next/source/js/third-party/rating.js","hash":"4e92c2d107ba47b47826829f9668030d5ea9bfb8","modified":1655723668016},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/index.styl","hash":"fe1868f47681e00a33a96199302be85377282f63","modified":1655723668624},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/buttons.styl","hash":"a042571d85ff7265f799004239a45f36b716b8a6","modified":1655723668614},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/reading-progress.styl","hash":"90a86045a33c1bae49fc2f6fa1e1b53170c7f77b","modified":1655723668657},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/comments.styl","hash":"e4fecc889ba3317a64e9abba5842c79dff9b7827","modified":1655723668615},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/base.styl","hash":"d0a7c99095f490b0d2ed6b1be43d435960798cec","modified":1655723668608},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/pagination.styl","hash":"b5c7782368889fa9fd93807d28ff2daf270e3703","modified":1655723668646},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/index.styl","hash":"523fb7b653b87ae37fc91fc8813e4ffad87b0d7e","modified":1655723668633},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/normalize.styl","hash":"b56367ea676ea8e8783ea89cd4ab150c7da7a060","modified":1655723668644},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tables.styl","hash":"e840b23d33023e6d45e018f6e84b683dd56efd8d","modified":1655723668666},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/toggles.styl","hash":"572a41499391677d84b16d8dbd6a996a3d5ce041","modified":1655723668669},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/index.styl","hash":"8e34df131830d4fa3725e4590a672ba1cf1903e5","modified":1655723668630},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Mist/_header.styl","hash":"4817e77577896ab5c0da434549917ee703a3f4cf","modified":1655723668507},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/mobile.styl","hash":"64775c729512b30b144ab5ae9dc4a4dfd4e13f35","modified":1655723668642},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Mist/_layout.styl","hash":"5604ac1e161099a4d3e5657d53507268866dc717","modified":1655723668510},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Gemini/index.styl","hash":"fd49b521d67eaccc629f77b4e095cb7310327565","modified":1655723668635},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Mist/index.styl","hash":"ab16a3dcdc0393b9b582ef59dcc13db9320e917c","modified":1655723668635},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Mist/_menu.styl","hash":"fb550935d374e0bdf1097fce187337dc05cad3e1","modified":1655723668512},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Pisces/_header.styl","hash":"78406b8400abd5d4c670a11d7fa1b8b3cdeccad7","modified":1655730600802},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Mist/_posts-expand.styl","hash":"be6cf377ae8f4a01ee76f9b3014e74161d4d5d17","modified":1655723668514},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Muse/_header.styl","hash":"06080fd963c904d96c00eff098a284e337953013","modified":1655723668508},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Pisces/_menu.styl","hash":"72dc825c50357402c342d62ab60fc0c478ab6bc1","modified":1655723668513},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Pisces/_layout.styl","hash":"6eee86c8f0175d6c09e434053516cd8556f78d44","modified":1655723668511},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Pisces/_sidebar.styl","hash":"d9141e6e14a56b5952488101e9a8388c2170e270","modified":1655723668603},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Pisces/_sub-menu.styl","hash":"778ed2ad5643b93970c95626b325defeb586733f","modified":1655723668604},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Muse/_layout.styl","hash":"82a29572dd90451f75358a2ee2522b87304a0bb8","modified":1655723668510},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Muse/_menu.styl","hash":"b7f48be3c43bfa393d62142544a5487a67871713","modified":1655723668512},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Muse/_sidebar.styl","hash":"944364893bd7160d954c10ba931af641c91515a4","modified":1655723668599},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Pisces/index.styl","hash":"8000075b227749a7495eaf417cac6ccfbe441580","modified":1655723668636},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Muse/index.styl","hash":"6ad168288b213cec357e9b5a97674ff2ef3a910c","modified":1655723668636},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Muse/_sub-menu.styl","hash":"c48ccd8d6651fe1a01faff8f01179456d39ba9b1","modified":1655723668603},{"_id":"node_modules/hexo-theme-next/source/js/third-party/analytics/baidu-analytics.js","hash":"f629acc46ff40c071ffd31b77d5c7616f0fdd778","modified":1655723667955},{"_id":"node_modules/hexo-theme-next/source/js/third-party/analytics/growingio.js","hash":"78dd3cf04082b7dbe6246e404b2aa8e726922402","modified":1655723667990},{"_id":"node_modules/hexo-theme-next/source/js/third-party/chat/gitter.js","hash":"cc38c94125f90dadde11b5ebac7d8bf99a1a08a2","modified":1655723667988},{"_id":"node_modules/hexo-theme-next/source/js/third-party/chat/tidio.js","hash":"b0079f6a4601e06ca6fe46e83a2f5af553e9bc3c","modified":1655723668018},{"_id":"node_modules/hexo-theme-next/source/js/third-party/chat/chatra.js","hash":"c32180522788c10e51df1803aa6842ef0432ddc9","modified":1655723667963},{"_id":"node_modules/hexo-theme-next/source/js/third-party/analytics/google-analytics.js","hash":"59684383385059dc4f8a1ff85dbbeb703bcdbcb5","modified":1655723667989},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/disqusjs.js","hash":"b6c58f098473b526d6a3cd35655caf34b77f7cff","modified":1655723667981},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/gitalk.js","hash":"0ec038cf83e8ec067534f16a54041e47a3c1e59a","modified":1655723667987},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/isso.js","hash":"753a873b6f566aff5ba77ca23f91b78eb880ca64","modified":1655723667995},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/changyan.js","hash":"260d1a77d6a3bb33a579d3e4cca1997003e799b5","modified":1655723667962},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/disqus.js","hash":"e1cc671b0d524864fd445e3ab4ade9ee6d07e565","modified":1655723667979},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/livere.js","hash":"2247d88c934c765c43013337860774aaa99f0b31","modified":1655723667999},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/utterances.js","hash":"f67f90eb03e284c82da2b8cf2f1e31801813c16d","modified":1655723668094},{"_id":"node_modules/hexo-theme-next/source/js/third-party/math/mathjax.js","hash":"5c749b9c1c3bb738122d0516211ecff6496d4907","modified":1655723668001},{"_id":"node_modules/hexo-theme-next/source/js/third-party/math/katex.js","hash":"83c54ee536e487a1031783443fe0cb63b1b4767e","modified":1655723667996},{"_id":"node_modules/hexo-theme-next/source/js/third-party/search/algolia-search.js","hash":"fdb7b7cef1a147d897e7f7cd8903b58368ec2062","modified":1655723667953},{"_id":"node_modules/hexo-theme-next/source/js/third-party/statistics/firestore.js","hash":"411a72df581f5b21317dc28633c7993207eb9e1c","modified":1655723667984},{"_id":"node_modules/hexo-theme-next/source/js/third-party/search/local-search.js","hash":"4536cb6d0a9bbaaa86fab3fa0101f6a3a3ec5a76","modified":1655723668000},{"_id":"node_modules/hexo-theme-next/source/js/third-party/statistics/lean-analytics.js","hash":"5a928990856b8e456f0663cf3b6b406733672e39","modified":1655723667997},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/index.styl","hash":"d0805a763176b3c0003967401644f41dfe3bc9e8","modified":1655723668626},{"_id":"node_modules/hexo-theme-next/source/js/third-party/tags/pdf.js","hash":"af78c22f0e61c8c8aa8794e585e0d632c6d4fcb8","modified":1655723668013},{"_id":"node_modules/hexo-theme-next/source/js/third-party/tags/mermaid.js","hash":"f27d817b0c2138dd3215b1f46af0753f60a008f3","modified":1655723668003},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-followme.styl","hash":"fc1a7bac6493f24aa50665574f37f3dd954f210c","modified":1655723668651},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-body.styl","hash":"d757768a58743601d0d84158ba955eb15d4c3c01","modified":1655723668648},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-footer.styl","hash":"1d284f3ea03ba9b4feb76b375e539a8e0bccf1c3","modified":1655723668652},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-collapse.styl","hash":"ec37a36e94ba791663607a5022f763915778578f","modified":1655723668650},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-gallery.styl","hash":"aa366d37389760c8595529b850f461569577a1c5","modified":1655723668653},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-nav.styl","hash":"9ac6f477177264c26a46e8333b8456720a0444dc","modified":1655723668655},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-widgets.styl","hash":"b6677dc2a2368084ab82bb4f145ac79e5966c150","modified":1655723668656},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-reward.styl","hash":"07cff69f2d57e6321595f64c16d8b763dc88df6a","modified":1655723668656},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-header.styl","hash":"010c901e4ef49a606f8a350efbf09044e76d2ff3","modified":1655723668654},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/pages/categories.styl","hash":"b6e2eb1550a7845cb2adf86081a4ab6c7bde1e68","modified":1655723668614},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/pages/breadcrumb.styl","hash":"8afdc311c6b8db121758371f95cf1c5e77354f42","modified":1655723668613},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/pages/index.styl","hash":"7504dbc5c70262b048143b2c37d2b5aa2809afa2","modified":1655723668625},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/pages/schedule.styl","hash":"6b816c2511242ee503fb5f34cd3e4dcdafc06b85","modified":1655723668658},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/third-party/gitalk.styl","hash":"070737d101e7cd58e997e8c7af09958268c43a21","modified":1655723668619},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/pages/tag-cloud.styl","hash":"1a81d1a71fcf0699629ce6e72dfd0a15f3a2dd0a","modified":1655723668668},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/third-party/disqusjs.styl","hash":"c2326ee3e8b724d99c24a818ddee32813ea5bf89","modified":1655723668618},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/third-party/math.styl","hash":"9d995eb4871a6c273d9d51558676a1fdabf69e72","modified":1655723668639},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/third-party/index.styl","hash":"979486a41a81f2a9fd8b0b87c4f87d6416c68c7d","modified":1655723668626},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/third-party/related-posts.styl","hash":"41ed817e1eb64078074e245e771446ee041e5790","modified":1655723668657},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/third-party/gitter.styl","hash":"35104dc6883a61c31e0e368dac8ac2f697be62fe","modified":1655723668621},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/footer/index.styl","hash":"8b9407e5cfd0571ef8de7df19022b268f962fa2f","modified":1655723668628},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/highlight/copy-code.styl","hash":"670fc109b56a010b166b86b616823a1aae97a738","modified":1655723668617},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/third-party/utterances.styl","hash":"56d90ae0559caa55b75f3c300ff2711f9ed65fc4","modified":1655723668669},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/highlight/index.styl","hash":"f2328caa94645836e06fb39a6a9c9a84ed68a8b5","modified":1655723668632},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/header/bookmark.styl","hash":"e74f4bb47a101b014ee2a1783c87f3b87323f9a0","modified":1655723668612},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/header/github-banner.styl","hash":"38c64c2d04e46848382bfa246a0e9c508294767b","modified":1655723668620},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/header/index.styl","hash":"ff642130354a0b3be0d708c43044ed4d710b5e83","modified":1655723668629},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/third-party/search.styl","hash":"e72799ce3f9b79753e365b2f8c8ef6c310668d4a","modified":1655723668659},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/group-pictures.styl","hash":"393ff96234e4196b569d4b11496774eb78e147de","modified":1655723668623},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/header/menu.styl","hash":"392fd53a8dd4e3f33a853ebb24290a622300e0ff","modified":1655723668640},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/blockquote-center.styl","hash":"d6418fd2bbfba7b73ddf11ec62db9637fdf5d8af","modified":1655723668610},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/label.styl","hash":"debee14539272fbe3835a7d3853af2230baa3501","modified":1655723668637},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/index.styl","hash":"3f76c73a891bbc10679753e702feba9e8a5ffdd2","modified":1655723668633},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/header/site-meta.styl","hash":"759e582d34d08e3386c55d87a835a9523608619f","modified":1655723668664},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/header/site-nav.styl","hash":"bf3ad8b4268f763a1e26377681644887694bc009","modified":1655723668665},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/mermaid.styl","hash":"48d35dba575a7c9e8845b16652e76b7d4a4646de","modified":1655723668641},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/pdf.styl","hash":"b6654a1d7cf82577d8263faffee8af3ad4a5c0e8","modified":1655723668646},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/note.styl","hash":"d27fbf7799695295dd5860a161a13ac4d90c5ba4","modified":1655723668645},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/sidebar-author-links.styl","hash":"52fc98b1435129eb3edb9293ced9e507741f1350","modified":1655723668659},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/link-grid.styl","hash":"7f8a7345e6537a62cd9e9a94c8f7065b541d9b04","modified":1655723668638},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/tabs.styl","hash":"7a39bcce7274284e87388743db62afc847fe6897","modified":1655723668667},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/index.styl","hash":"cee43480eba028c37d51cb620c2d81486aa24e01","modified":1655723668631},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/sidebar-author.styl","hash":"5b38ac4a0f1ade0e681aff0e3366c481d9cf3dcd","modified":1655723668660},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/sidebar-blogroll.styl","hash":"9950c3188a28e1c63b5498b7bdcd14b12ace3e28","modified":1655723668661},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/sidebar-button.styl","hash":"b926e368f702f8686aaa2eb98d3d2e533418958c","modified":1655723668661},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/sidebar-dimmer.styl","hash":"fbdb63c6a8887d19b7137325ba7d6806f728139c","modified":1655723668662},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/sidebar-toc.styl","hash":"021a37cf178440cc341940a299d3bca359996c6b","modified":1655723668663},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/sidebar-toggle.styl","hash":"3103b81fc76b59e1e2c161e2c484625c770ed66f","modified":1655723668664},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/sidebar-nav.styl","hash":"ee94a1a27090ad24e3ed579093088d97ff96d77d","modified":1655723668662},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/site-state.styl","hash":"26dd0adfcb1db6df29c6090c8d7e9b5a43583fb0","modified":1655723668665},{"_id":"node_modules/hexo-theme-next/source/images/taffy.png","hash":"0d16745194d0564ae77a71f5d3a381b64b5ec1fc","modified":1653934034116},{"_id":"node_modules/hexo-theme-next/source/images/2.jpg","hash":"3b37cdb16228cb83350d9efcf98ba06bfe34c589","modified":1653934034124},{"_id":"public/search.xml","hash":"bfef2dd42123118dba8db28f0a8a2c032a3931a9","modified":1656416773626},{"_id":"public/about/index.html","hash":"dd01328ed5f677b9821ef13358a65edc9df9f9fb","modified":1656416773626},{"_id":"public/categories/index.html","hash":"9d7ce14286fc9d36a858c710654bd9ae66b5726a","modified":1656416773626},{"_id":"public/tags/index.html","hash":"e1ecdeca32c04c7bdea9f93f754a10a92d11ec26","modified":1656416773626},{"_id":"public/2022/05/24/About-Moby-Dick/index.html","hash":"200b52874acfde86bbdb7d2878ef3f01e1b1c64f","modified":1656416773626},{"_id":"public/2022/05/23/testpic/index.html","hash":"eb08a0622b58bf9bad23b0d7f7cb0878fc74d463","modified":1656416773626},{"_id":"public/categories/Metaphysics/index.html","hash":"4fe79b37ae43454870ca92829ff81d48498d4700","modified":1656416773626},{"_id":"public/categories/Substance/index.html","hash":"2cd21ebff11925c7f2532fe1ec5bea6bb00bdbfb","modified":1656416773626},{"_id":"public/categories/Metaphysicsk/index.html","hash":"d459ffc145145942213794bff3713361bdfb1be3","modified":1656416773626},{"_id":"public/categories/Meow/index.html","hash":"07b33c15dc4903c63bcc03e19f3a160c4510cb33","modified":1656416773626},{"_id":"public/archives/index.html","hash":"1f8e68424806a30a2e2757b9c633cf116059ef7a","modified":1656416773626},{"_id":"public/archives/2022/index.html","hash":"8f53d6f9b4659d81ed6ab5f258374a3e5a60b1bc","modified":1656416773626},{"_id":"public/archives/2022/05/index.html","hash":"d5e2e100848d816b1e6c74dc132cca8ab9054f87","modified":1656416773626},{"_id":"public/archives/2022/06/index.html","hash":"97dd1b0460e7306143e382de25c1cc7ac0a70bdf","modified":1656416773626},{"_id":"public/tags/Moby-Dick/index.html","hash":"eafa9ee6a865dc37edeb2d9dbc74b97f89c75991","modified":1656416773626},{"_id":"public/tags/Reading/index.html","hash":"e161655e4afcfcfe287e17fe40b9cf869ac29bea","modified":1656416773626},{"_id":"public/tags/ICSE-2022/index.html","hash":"e8d6a88e82f7605610a639723909f4ff8a9dff7a","modified":1656416773626},{"_id":"public/tags/Code-generation/index.html","hash":"367aabd670da40aa779977aafbf66769ff219024","modified":1656416773626},{"_id":"public/tags/Note/index.html","hash":"707967f960c7ee5b3e2b0a0ecc2214d67a281266","modified":1656416773626},{"_id":"public/tags/Poem/index.html","hash":"a6dda2b87c1aea601db37e4b597e07c3539a839b","modified":1656416773626},{"_id":"public/tags/Literary-criticism/index.html","hash":"33f3f2ec5b323b57e940aeccd43d353e4fc2a5c4","modified":1656416773626},{"_id":"public/tags/Philip-Larkin/index.html","hash":"d0a7f6749f6b92e1db9a7ea96749b9a491c1be74","modified":1656416773626},{"_id":"public/tags/Ace/index.html","hash":"b11fb58183918642fc2dfdfc6cc20de31c8046e0","modified":1656416773626},{"_id":"public/tags/Taffy/index.html","hash":"48669c251a6d319be921df9e9219b484d18b6d13","modified":1656416773626},{"_id":"public/tags/Self/index.html","hash":"38f4591e1aa5b8d40595b6c15166c1f5919389fe","modified":1656416773626},{"_id":"public/tags/ICST/index.html","hash":"ea7efb6e6c72d6a69a4cf78d0138d01265b93813","modified":1656416773626},{"_id":"public/tags/OCL/index.html","hash":"b6fed80c2c261f757c1644279575978445bf6a4b","modified":1656416773626},{"_id":"public/2022/06/20/论文笔记-Automated-Generation-of-Constraints-from-Use-Case-Specifications-to-Support-System-Testing/index.html","hash":"1e8be2b2fceea790a265fb5fb094f7b84f269ab4","modified":1656416773626},{"_id":"public/2022/06/05/Philip-Larkin/index.html","hash":"342b909fd8ba5a081bf2e6103fb4d23690b7c332","modified":1656416773626},{"_id":"public/2022/05/29/火柴/index.html","hash":"e913b8c93531162382d38bf4a7dbd4e27ef6b337","modified":1656416773626},{"_id":"public/2022/05/27/SPT-Code/index.html","hash":"715430a1c5c24ea7f360a1ed8da6d4bd0902f652","modified":1656416773626},{"_id":"public/index.html","hash":"912ad4b9a9be068e984e70f45005d4c8d0cd2e8c","modified":1656416773626},{"_id":"public/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1656416773626},{"_id":"public/images/avatar.gif","hash":"2dbc3e2f2d624b2ca1afe6edc2ca17307f1950c8","modified":1656416773626},{"_id":"public/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1656416773626},{"_id":"public/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1656416773626},{"_id":"public/images/logo-algolia-nebula-blue-full.svg","hash":"b85e274207b1392782476a0430feac98db1e7da0","modified":1656416773626},{"_id":"public/images/mine-32x32.png","hash":"13e8c0230b242bc68c3b54267ef0024d7fb11667","modified":1656416773626},{"_id":"public/images/mine-16x16.png","hash":"46a56b8ee580a508fedfceff822c9dc8be0fa20d","modified":1656416773626},{"_id":"public/images/logo.svg","hash":"2cb74fd3ea2635e015eabc58a8d488aed6cf6417","modified":1656416773626},{"_id":"public/2022/05/27/SPT-Code/image-20220527233752649.png","hash":"add232d275639723c148095061934b6857daf254","modified":1656416773626},{"_id":"public/2022/05/27/SPT-Code/image-20220527233143119.png","hash":"d089096a54c50ed0ab12ae4dacbe4a85b2bcb6bc","modified":1656416773626},{"_id":"public/2022/05/27/SPT-Code/image-20220527231806850.png","hash":"bdd3bb79709080c4bb8493ae66cd290e7b8b8237","modified":1656416773626},{"_id":"public/2022/05/23/testpic/image-20220523211852621.png","hash":"f443eb7116b28a70a135256d0d4521e3710e45ff","modified":1656416773626},{"_id":"public/2022/06/20/论文笔记-Automated-Generation-of-Constraints-from-Use-Case-Specifications-to-Support-System-Testing/image-20220625212958549.png","hash":"8d95c96837046ecb47d5cf99db283be61e67d59a","modified":1656416773626},{"_id":"public/2022/05/27/SPT-Code/image-20220527230530594.png","hash":"ca5795fe19159888c5a97acb4f35f489db7ab7ed","modified":1656416773626},{"_id":"public/2022/06/20/论文笔记-Automated-Generation-of-Constraints-from-Use-Case-Specifications-to-Support-System-Testing/image-20220625173337030.png","hash":"9d4e40d0f44f88d561df76535dccb95bdc58931f","modified":1656416773626},{"_id":"public/2022/06/20/论文笔记-Automated-Generation-of-Constraints-from-Use-Case-Specifications-to-Support-System-Testing/image-20220625181049650.png","hash":"8db94f3137b6f818297c81d3f2609df821327e2b","modified":1656416773626},{"_id":"public/2022/06/20/论文笔记-Automated-Generation-of-Constraints-from-Use-Case-Specifications-to-Support-System-Testing/image-20220626184416896.png","hash":"c3c77c4c9784e8575192d727ee35e4764f73a9b4","modified":1656416773626},{"_id":"public/2022/06/20/论文笔记-Automated-Generation-of-Constraints-from-Use-Case-Specifications-to-Support-System-Testing/image-20220626184653142.png","hash":"93b29c9dd359d85f1f5f9c8a3a010d1d84ee7fc2","modified":1656416773626},{"_id":"public/2022/06/20/论文笔记-Automated-Generation-of-Constraints-from-Use-Case-Specifications-to-Support-System-Testing/image-20220626184748233.png","hash":"b2aa19c215213b86de9410831101e13b3e6bf246","modified":1656416773626},{"_id":"public/2022/06/20/论文笔记-Automated-Generation-of-Constraints-from-Use-Case-Specifications-to-Support-System-Testing/image-20220626184413924.png","hash":"c3c77c4c9784e8575192d727ee35e4764f73a9b4","modified":1656416773626},{"_id":"public/js/comments-buttons.js","hash":"1a7344440321713426a0b2ab17e276b5bdf85ade","modified":1656416773626},{"_id":"public/css/noscript.css","hash":"ec89b3425fbce20863d554c6fd495ea29c3c303d","modified":1656416773626},{"_id":"public/js/motion.js","hash":"f7c825cbff11885fa0dffa64824fd00e505d6a8d","modified":1656416773626},{"_id":"public/js/bookmark.js","hash":"0f563ffbf05fad30e854e413ab17ff7164ab5a53","modified":1656416773626},{"_id":"public/js/comments.js","hash":"66ae2e26ea36a41b72c638ea8b220296638ae952","modified":1656416773626},{"_id":"public/js/schedule.js","hash":"a1333258726caf84f368a8f8454639c7dc1626bb","modified":1656416773626},{"_id":"public/js/next-boot.js","hash":"da0f07f9eaaa83de70128b0feaea3fdadb90457a","modified":1656416773626},{"_id":"public/js/schemes/muse.js","hash":"9794bd4fc6a458322949d6a0ade89cd1026bc69f","modified":1656416773626},{"_id":"public/js/utils.js","hash":"200088bfd042f5304b2a04befab0829148845e0e","modified":1656416773626},{"_id":"public/js/config.js","hash":"4c4ebbe3b3f3841a26f9d5af6d0ba8bc6da01c54","modified":1656416773626},{"_id":"public/js/third-party/fancybox.js","hash":"c098d14e65dd170537134358d4b8359ad0539c2c","modified":1656416773626},{"_id":"public/js/third-party/pace.js","hash":"0ef04218b93561ba4d0ff420d556c3d90a756d32","modified":1656416773626},{"_id":"public/js/third-party/quicklink.js","hash":"eed02e6fced8e5a653077205d4d4d7834ca71472","modified":1656416773626},{"_id":"public/js/pjax.js","hash":"919f5281c4a04d11cfd94573ecf57b3dbabd3cc8","modified":1656416773626},{"_id":"public/js/third-party/rating.js","hash":"4e92c2d107ba47b47826829f9668030d5ea9bfb8","modified":1656416773626},{"_id":"public/js/third-party/analytics/baidu-analytics.js","hash":"f629acc46ff40c071ffd31b77d5c7616f0fdd778","modified":1656416773626},{"_id":"public/js/third-party/analytics/google-analytics.js","hash":"59684383385059dc4f8a1ff85dbbeb703bcdbcb5","modified":1656416773626},{"_id":"public/js/third-party/analytics/growingio.js","hash":"78dd3cf04082b7dbe6246e404b2aa8e726922402","modified":1656416773626},{"_id":"public/js/third-party/chat/gitter.js","hash":"cc38c94125f90dadde11b5ebac7d8bf99a1a08a2","modified":1656416773626},{"_id":"public/js/third-party/comments/changyan.js","hash":"260d1a77d6a3bb33a579d3e4cca1997003e799b5","modified":1656416773626},{"_id":"public/js/third-party/chat/tidio.js","hash":"b0079f6a4601e06ca6fe46e83a2f5af553e9bc3c","modified":1656416773626},{"_id":"public/js/third-party/chat/chatra.js","hash":"c32180522788c10e51df1803aa6842ef0432ddc9","modified":1656416773626},{"_id":"public/js/third-party/comments/gitalk.js","hash":"0ec038cf83e8ec067534f16a54041e47a3c1e59a","modified":1656416773626},{"_id":"public/js/third-party/comments/disqusjs.js","hash":"b6c58f098473b526d6a3cd35655caf34b77f7cff","modified":1656416773626},{"_id":"public/js/third-party/comments/livere.js","hash":"2247d88c934c765c43013337860774aaa99f0b31","modified":1656416773626},{"_id":"public/js/third-party/comments/disqus.js","hash":"e1cc671b0d524864fd445e3ab4ade9ee6d07e565","modified":1656416773626},{"_id":"public/js/third-party/comments/utterances.js","hash":"f67f90eb03e284c82da2b8cf2f1e31801813c16d","modified":1656416773626},{"_id":"public/js/third-party/math/mathjax.js","hash":"5c749b9c1c3bb738122d0516211ecff6496d4907","modified":1656416773626},{"_id":"public/js/third-party/math/katex.js","hash":"83c54ee536e487a1031783443fe0cb63b1b4767e","modified":1656416773626},{"_id":"public/js/third-party/search/local-search.js","hash":"4536cb6d0a9bbaaa86fab3fa0101f6a3a3ec5a76","modified":1656416773626},{"_id":"public/js/third-party/search/algolia-search.js","hash":"fdb7b7cef1a147d897e7f7cd8903b58368ec2062","modified":1656416773626},{"_id":"public/js/third-party/statistics/firestore.js","hash":"411a72df581f5b21317dc28633c7993207eb9e1c","modified":1656416773626},{"_id":"public/js/third-party/statistics/lean-analytics.js","hash":"5a928990856b8e456f0663cf3b6b406733672e39","modified":1656416773626},{"_id":"public/js/third-party/tags/mermaid.js","hash":"f27d817b0c2138dd3215b1f46af0753f60a008f3","modified":1656416773626},{"_id":"public/js/third-party/tags/pdf.js","hash":"af78c22f0e61c8c8aa8794e585e0d632c6d4fcb8","modified":1656416773626},{"_id":"public/js/third-party/comments/isso.js","hash":"753a873b6f566aff5ba77ca23f91b78eb880ca64","modified":1656416773626},{"_id":"public/css/main.css","hash":"85d6f132baeed44c4d8ad6d0f0d140641aa5c4bb","modified":1656416773626},{"_id":"public/2022/05/27/SPT-Code/image-20220527232629535.png","hash":"22e81124540e148c61cb00bc3e3f986c71713087","modified":1656416773626},{"_id":"public/images/taffy.png","hash":"0d16745194d0564ae77a71f5d3a381b64b5ec1fc","modified":1656416773626},{"_id":"public/2022/05/23/testpic/image-20220524021029166.png","hash":"a4f4e62846dc739c9d970f5eea1b4414740f2184","modified":1656416773626},{"_id":"public/2022/06/20/论文笔记-Automated-Generation-of-Constraints-from-Use-Case-Specifications-to-Support-System-Testing/image-20220628194141482.png","hash":"da786f68967b2649b93e0542390542246b16a1f5","modified":1656416773626},{"_id":"public/2022/06/20/论文笔记-Automated-Generation-of-Constraints-from-Use-Case-Specifications-to-Support-System-Testing/image-20220628194155803.png","hash":"d9265012432b33c71d848c67b0a85228acdcbcb3","modified":1656416773626},{"_id":"public/images/2.jpg","hash":"3b37cdb16228cb83350d9efcf98ba06bfe34c589","modified":1656416773626}],"Category":[{"name":"Metaphysics","_id":"cl4y3s2z00003qkw90fua9l4k"},{"name":"Substance","_id":"cl4y3s2zg000cqkw9bbe45ngv"},{"name":"Metaphysicsk","_id":"cl4y3s2zj000hqkw951wrguc4"},{"name":"Meow","_id":"cl4y3s2zk000kqkw93tw2d0pi"}],"Data":[],"Page":[{"title":"About","date":"2022-05-23T17:42:53.000Z","type":"about","_content":"这里用来暂存英语写作积累\n\n```\nRecent years have seen the successful application of A to B, resulting in C\n\n````\n\n`aforementioned`\n上述的\n`henceforth`\n此后\n\n`In light of the above discussion`\n基于上述讨论\n","source":"about/index.md","raw":"---\ntitle: about\ndate: 2022-05-24 01:42:53\ntype: about\n---\n这里用来暂存英语写作积累\n\n```\nRecent years have seen the successful application of A to B, resulting in C\n\n````\n\n`aforementioned`\n上述的\n`henceforth`\n此后\n\n`In light of the above discussion`\n基于上述讨论\n","updated":"2022-06-20T11:16:09.631Z","path":"about/index.html","comments":1,"layout":"page","_id":"cl4y3s2yt0000qkw93fwqh9ph","content":"<p>这里用来暂存英语写作积累</p>\n<pre><code>Recent years have seen the successful application of A to B, resulting in C\n</code></pre>\n<p><code>aforementioned</code><br>上述的<br><code>henceforth</code><br>此后</p>\n<p><code>In light of the above discussion</code><br>基于上述讨论</p>\n","site":{"data":{}},"excerpt":"","more":"<p>这里用来暂存英语写作积累</p>\n<pre><code>Recent years have seen the successful application of A to B, resulting in C\n</code></pre>\n<p><code>aforementioned</code><br>上述的<br><code>henceforth</code><br>此后</p>\n<p><code>In light of the above discussion</code><br>基于上述讨论</p>\n"},{"title":"Categories","date":"2022-05-23T17:41:53.000Z","type":"categories","_content":"","source":"categories/index.md","raw":"---\ntitle: categories\ndate: 2022-05-24 01:41:53\ntype: categories\n---\n","updated":"2022-06-20T11:16:09.631Z","path":"categories/index.html","comments":1,"layout":"page","_id":"cl4y3s2yz0002qkw9hv5zb6w8","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"Tags","date":"2022-05-23T08:44:01.000Z","type":"tags","_content":"","source":"tags/index.md","raw":"---\ntitle: tags\ndate: 2022-05-23 16:44:01\ntype: tags\n---\n","updated":"2022-06-20T11:16:09.632Z","path":"tags/index.html","comments":1,"layout":"page","_id":"cl4y3s2z10005qkw94vgu79n0","content":"","site":{"data":{}},"excerpt":"","more":""}],"Post":[{"title":"About Moby Dick","date":"2022-05-23T19:07:49.000Z","_content":"> 现在，除了一艘轻轻摇晃的船赋予你的摇摆不定的生命，你没有生命；船的生命借自于大海；大海的生命借自于上帝神秘难测的潮汐。可是，当这睡眠，这幻梦将你笼罩，你的手或脚要是稍微挪动一下——你的双手彻底松开——你就会在惊恐中能够恢复自己的本性。你就盘旋在笛卡尔的涡流之上了。而也许，恰当正午，又是响晴的天气，你便随着一声半带窒息的尖叫，穿过透明的空气，坠入夏天的海洋，再也没有浮上来。好好留神吧，你们这些泛神论者！——《白鲸》桅顶瞭望\n\n\n> There is no life in thee, now, expect that rocking life imparted by a gently rolling ship; by her, borrowed from the sea; by the sea, from the inscrutable tides of God. But while this sleep, this dream is on ye, move your foot or hand an inch, slip your hold at all; and your identity comes back in horror. Over Descartian vortices you hover. And perhaps, at mid-day, in the fairest weather, with one half-throttled shriek you drop through that transparent air into the summer sea, no more to rise for ever. Heed it well, ye Pantheists! ——&lt;Moby dick> CHAPTER 35 The Mast-Head\n","source":"_posts/About-Moby-Dick.md","raw":"---\ntitle: About Moby Dick\ndate: 2022-05-24 03:07:49\ntags: \n- Moby Dick\n- Reading\ncategories:\n- Metaphysics\n---\n> 现在，除了一艘轻轻摇晃的船赋予你的摇摆不定的生命，你没有生命；船的生命借自于大海；大海的生命借自于上帝神秘难测的潮汐。可是，当这睡眠，这幻梦将你笼罩，你的手或脚要是稍微挪动一下——你的双手彻底松开——你就会在惊恐中能够恢复自己的本性。你就盘旋在笛卡尔的涡流之上了。而也许，恰当正午，又是响晴的天气，你便随着一声半带窒息的尖叫，穿过透明的空气，坠入夏天的海洋，再也没有浮上来。好好留神吧，你们这些泛神论者！——《白鲸》桅顶瞭望\n\n\n> There is no life in thee, now, expect that rocking life imparted by a gently rolling ship; by her, borrowed from the sea; by the sea, from the inscrutable tides of God. But while this sleep, this dream is on ye, move your foot or hand an inch, slip your hold at all; and your identity comes back in horror. Over Descartian vortices you hover. And perhaps, at mid-day, in the fairest weather, with one half-throttled shriek you drop through that transparent air into the summer sea, no more to rise for ever. Heed it well, ye Pantheists! ——&lt;Moby dick> CHAPTER 35 The Mast-Head\n","slug":"About-Moby-Dick","published":1,"updated":"2022-06-20T11:16:09.616Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl4y3s2yw0001qkw95hc53xj8","content":"<blockquote>\n<p>现在，除了一艘轻轻摇晃的船赋予你的摇摆不定的生命，你没有生命；船的生命借自于大海；大海的生命借自于上帝神秘难测的潮汐。可是，当这睡眠，这幻梦将你笼罩，你的手或脚要是稍微挪动一下——你的双手彻底松开——你就会在惊恐中能够恢复自己的本性。你就盘旋在笛卡尔的涡流之上了。而也许，恰当正午，又是响晴的天气，你便随着一声半带窒息的尖叫，穿过透明的空气，坠入夏天的海洋，再也没有浮上来。好好留神吧，你们这些泛神论者！——《白鲸》桅顶瞭望</p>\n</blockquote>\n<blockquote>\n<p>There is no life in thee, now, expect that rocking life imparted by a gently rolling ship; by her, borrowed from the sea; by the sea, from the inscrutable tides of God. But while this sleep, this dream is on ye, move your foot or hand an inch, slip your hold at all; and your identity comes back in horror. Over Descartian vortices you hover. And perhaps, at mid-day, in the fairest weather, with one half-throttled shriek you drop through that transparent air into the summer sea, no more to rise for ever. Heed it well, ye Pantheists! ——&lt;Moby dick&gt; CHAPTER 35 The Mast-Head</p>\n</blockquote>\n","site":{"data":{}},"excerpt":"","more":"<blockquote>\n<p>现在，除了一艘轻轻摇晃的船赋予你的摇摆不定的生命，你没有生命；船的生命借自于大海；大海的生命借自于上帝神秘难测的潮汐。可是，当这睡眠，这幻梦将你笼罩，你的手或脚要是稍微挪动一下——你的双手彻底松开——你就会在惊恐中能够恢复自己的本性。你就盘旋在笛卡尔的涡流之上了。而也许，恰当正午，又是响晴的天气，你便随着一声半带窒息的尖叫，穿过透明的空气，坠入夏天的海洋，再也没有浮上来。好好留神吧，你们这些泛神论者！——《白鲸》桅顶瞭望</p>\n</blockquote>\n<blockquote>\n<p>There is no life in thee, now, expect that rocking life imparted by a gently rolling ship; by her, borrowed from the sea; by the sea, from the inscrutable tides of God. But while this sleep, this dream is on ye, move your foot or hand an inch, slip your hold at all; and your identity comes back in horror. Over Descartian vortices you hover. And perhaps, at mid-day, in the fairest weather, with one half-throttled shriek you drop through that transparent air into the summer sea, no more to rise for ever. Heed it well, ye Pantheists! ——&lt;Moby dick&gt; CHAPTER 35 The Mast-Head</p>\n</blockquote>\n"},{"title":"SPT-Code","date":"2022-05-27T07:58:33.000Z","_content":"\n\n\n\n\n\n\n这是《Sequence-to-Sequence Pre-Training for Learning Source Code Representations》的读书笔记\n\n<!--more-->\n\n\n\n# Abstract\n\nPre-trained models 用于代码相关下游任务的应用时的 问题？\n\n\n\n1. 仅用了Pre-trained encoder 但生成任务需要两个部件都预训练\n2. 现在许多Pre-trained model 包括T5，只是简单复用了NL的预训练任务，这要求NL-CODE的corpus 这使得数据受限\n\n为了应对这两个问题 提出了SPT-Code ，在微调后可以在5个代码相关任务上SOTA\n\n这是一个seq2seq 预训练模型，通过三个预训练任务使得其能够学习到下面三点，并在下游任务中使用\n\n* 代码知识\n* 对应代码结构\n* 自然语言描述\n\n而不需要双语corpus\n\n# Introduction\n\n作者在第一部分提了自监督训练，然后说预训练模型的成功和这个有关系，下面谈预训练模型到软工SE任务的时候认为的问题是：\n\n* 主流预训练模型仅对encoder搞 ，不够理想\n\n  最有名的encoder也就是BERT吧 也确实就是用了MLM（Masked Language Modeling） 也确实有好多Pre-trained Bert 下面套个任务头或者另一个decoder就开干的……不过这应该算是蛮荒时代了 T5虽然好 但是对一般研究人员来说想改进模型architecture 不是那么容易？\n\n  * 别人的解决：\n\n    T5-learning , TreeBERT 两个工作使得Encoder-Decoder jointly trainded\n\n    \n\n* 这些预训练模型预设输入的是NL-CODE 忽视了代码结构\n\n  为什么呢？因为就是简单偷了NLP的拿来用\n\n  * 别人的解决：\n\n    专门的预训练任务 包括预测数据流图中的边与对齐节点和代码 \n\n    ——dataflow 有语义信息而无语法信息（AST）\n\n* 而且都假设有严格对齐的双语语料\n\n  * T5-learning :\n\n    分别处理两种输入，不要求语料库中展示二者的关系\n\n\n\n—— 没有一个模型能够统一处理这三个问题\n\nSPT-Code就可以！\n\n* 这是一个encoder-decoder共同预训练的模型\n* 数据实例由CODE,AST,NL三部分构成\n* 使用方法名和调用此方法的方式作为自然语言描述（以避免对bilingual corpus的依赖）\n\n\n\n方法：\n\n设计了三种预训练任务，每一种获取一种数据信息\n\n* 改进的MASS-用于CODE：遮蔽Seq2Seq恢复\n* Code-AST Predict CAP：预测code-AST是否匹配\n* Method Name Generation MNG：生成 方法名的 子token\n\n数据集：\n\nCodeSearchNet\n\n贡献：\n\n1. 提出了SPT-Code预训练模型，可用于分类和生成任务\n2. 使用了线性和简化的AST 第一个使用了NL&AST作为输入对于预训练\n3. 通过输入表示和三个与训练任务使得预训练模型不依赖双语语料库（labeled data)\n4. 用未标注数据库在五个下游任务实现了SOTA\n\n# 方法\n\n架构，输入和预训练任务，微调\n\n\n\n## 架构\n\n类似于BART和T5的典型Transformer\n\n分类任务和生成任务，模型采用相同的输入：\n\n* 分类对encoder和decoder输入相同\n* 生成采用传统方法\n\n<img src=\"SPT-Code/image-20220527230530594.png\" alt=\"image-20220527230530594\" style=\"zoom:50%;\" />\n\n## 输入\n\n<img src=\"SPT-Code/image-20220527231806850.png\" alt=\"image-20220527231806850\" style=\"zoom:50%;\" />\n\n由三部分组成，每一部分[SEP]连接\n\n### Code：\n\n没有使用笨蛋tokenizer，而是用了stl for Python 或者 antlr for Java,Php.etc 其他的用了NLTK\n\n###  AST\n\n用的Tree-sitter 搞的 AST \n\n 如何序列化AST?\n\n* 传统方法：SBT （Structure-Based Traversal)\n\n  比先序遍历之类的更有效，但可能产生过长的序列（可能超过代码三倍长）\n\n  <img src=\"SPT-Code/image-20220527232629535.png\" alt=\"image-20220527232629535\" style=\"zoom:50%;\" />\n\n  <center>\n      一种类似中序遍历的说法 来自那篇论文忘了 反正绝对看过\n  </center>\n  \n  \n* 本文的方法：X-SBT：XML-like SBT\n\n  可以减少超过一半的长度\n\n  <img src=\"SPT-Code/image-20220527233143119.png\" alt=\"image-20220527233143119\" style=\"zoom:67%;\" />\n\n  论文自带的图好看一点 这个创新点……只能说是情理之中，毕竟原来那个也太呆了（作者甚至还装模做样证了一下必然更短）\n\n  为了更短: AST——XSBT时，仅取表达式级别以上节点，放弃终结符\n\n  <img src=\"SPT-Code/image-20220527233752649.png\" alt=\"image-20220527233752649\" style=\"zoom:50%;\" />\n\n  **这种优化为可接受的，为什么呢？下面这个说得很漂亮：**\n\n  AST中包含了语法信息和词法信息，舍弃掉终结符丢失了词法信息，但之前的token（Input中Code的部分）都是词法单元，所以这个信息是没有丢掉的，因此改进可接受\n\n### NL\n\n难点：从仅有CODE中提取NL\n\n方法：获取方法名与调用的API序列\n\n对驼峰和下划线命名掰开\n\n问题：怎么提取的API序列：从AST里偷出来的？\n\n——应该就是\n\n## 预训练任务\n\n\n\n𝐼𝑛𝑝𝑢𝑡 = 𝐶,[SEP],𝐴,[SEP],𝑁 \n\n### Code-AST Prediction.\n\n这是第一个\n\n在构建输入𝐼𝑛𝑝𝑢𝑡 时，一半是对应的AST,一半是随机的AST\n\n### MASS\n\n随机遮蔽C中的一部分，将所有遮蔽的token设置为[MASK]（改进前为对应数量个[MASK])\n\n根据别人的论文，最大遮蔽长度是C长度l的一半\n\n\n\n### Method Name Generation\n\n希望可以通过这个任务学到代码的动机\n\n代码名的词汇和对应代码总结的词汇由高度相关，因此希望通过改善 预测代码名 这一任务提升 代码总结 的能力\n\n此任务的输入时，从𝐼𝑛𝑝𝑢𝑡中的C扣掉对应token，并在N中去掉前s个token（方法名总在最前），作为输入，试图让decoder输出扣掉的前s个token，即方法名\n\n\n\n\n\n## 微调\n\n端到端 根据不同任务分成两类，分类或生成，不同任务就缺掉一点输入\n\n# 实验\n\n## 预训练\n\n交代了数据集的数据使用，任务顺序，epoches，用的cross-entropy loss和Adam-W，batchsize和显卡（……）\n\nTokenizer Encoding 用的BPE对CODE和NL，在预训练data上干过 每个下游任务照用\n\n预训练任务的任务量都是每个任务几十个Epoches的量级。\n\n\n\n问题：不是都有token了 还tokenize？\n\n——低级问题，前面的应该是tokenize，这里进行token=>input_ids的步骤\n\n## 下游任务微调\n\n介绍了五个任务 其中介绍部分有点尴尬\n\n\n\n## 评估\n\nRQ1:相比于其他较好的基线 这个性能在下游任务如何？\n\n列个表 比不上人家的扯一点\n\n\n\nRQ2:三个预训练任务对五个下游任务分别有什么贡献？\n\n——消融实验\n\n有趣的是 删除掉MNG （生成 方法的token）在代码完成和代码修复上性能有所提高：\n\n* MNG是目标自然语言的预训练，而这两项任务都是代码到代码\n\n分析一下 什么任务对什么下游有影响\n\n\n\n**稍微有点水平的问题**\n\nRQ3: 可以利用更多无标记的资料是不是本模型的优点呢？\n\n相较于别的预训练模型，由于它的设计，可以使用无标注数据。更好的性能是不是来自于更多的数据呢？（而不是模型本身厉害？）\n\n在同样的数据集上训练——把它当作无标注的——其实和别的比还算吃亏——也能够取得相对别的模型更好的结果。\n\n可以说是赢两遍了。\n\n\n\nRQ4:微调阶段的数据量对下游任务有什么影响？\n\n虽然越小越坏，但是很小也和别的模型差不多 说明真好\n\n\n\n\n\n## 定性分析与定量分析\n\n定量：志愿者评估，多个样本分类列表个\n\n定性：在哪些任务哪些方面表现好 不好的怎么不好\n\n\n\n# 威胁分析\n\n构造：数据集可能有重复\n\n内部：没调过超参数：所以可能有更好的\n\n外部：只用了CodesearchNet\n\n\n\n# 总结：\n\n我们介绍了SPT-Code，这是一个基于编码器架构的源代码的大型型号。首先，我们为预训练SPT代码设计了三个特定代码的预训练任务。其次，我们提出了一种新的输入表示形式，它是第一个考虑自然语言和AST形式的方法，我们还提出了AST遍历方法的改进版本XSBT。我们的预训练任务和输入表示形式都允许在完全未标记的数据集上预先训练SPT代码。然后，对五个与代码相关的下游任务进行了微调。结果表明，微调SPT代码使其能够在五个与代码相关的下游任务上实现最新性能。消融实验表明，这三个预训练任务对不同的下游任务具有不同程度的影响，AST和自然语言输入也有助于提高SPTCODE的性能。为了促进未来的研究，我们还可以在https://github.com/ nougatca/spt-code上公开提供代码和其他。\n\n\n\n\n\n","source":"_posts/SPT-Code.md","raw":"---\ntitle: SPT-Code \ndate: 2022-05-27 15:58:33\ntags:\n- ICSE 2022\n- Code generation\n- Note\ncategories:\n- Substance\n---\n\n\n\n\n\n\n\n这是《Sequence-to-Sequence Pre-Training for Learning Source Code Representations》的读书笔记\n\n<!--more-->\n\n\n\n# Abstract\n\nPre-trained models 用于代码相关下游任务的应用时的 问题？\n\n\n\n1. 仅用了Pre-trained encoder 但生成任务需要两个部件都预训练\n2. 现在许多Pre-trained model 包括T5，只是简单复用了NL的预训练任务，这要求NL-CODE的corpus 这使得数据受限\n\n为了应对这两个问题 提出了SPT-Code ，在微调后可以在5个代码相关任务上SOTA\n\n这是一个seq2seq 预训练模型，通过三个预训练任务使得其能够学习到下面三点，并在下游任务中使用\n\n* 代码知识\n* 对应代码结构\n* 自然语言描述\n\n而不需要双语corpus\n\n# Introduction\n\n作者在第一部分提了自监督训练，然后说预训练模型的成功和这个有关系，下面谈预训练模型到软工SE任务的时候认为的问题是：\n\n* 主流预训练模型仅对encoder搞 ，不够理想\n\n  最有名的encoder也就是BERT吧 也确实就是用了MLM（Masked Language Modeling） 也确实有好多Pre-trained Bert 下面套个任务头或者另一个decoder就开干的……不过这应该算是蛮荒时代了 T5虽然好 但是对一般研究人员来说想改进模型architecture 不是那么容易？\n\n  * 别人的解决：\n\n    T5-learning , TreeBERT 两个工作使得Encoder-Decoder jointly trainded\n\n    \n\n* 这些预训练模型预设输入的是NL-CODE 忽视了代码结构\n\n  为什么呢？因为就是简单偷了NLP的拿来用\n\n  * 别人的解决：\n\n    专门的预训练任务 包括预测数据流图中的边与对齐节点和代码 \n\n    ——dataflow 有语义信息而无语法信息（AST）\n\n* 而且都假设有严格对齐的双语语料\n\n  * T5-learning :\n\n    分别处理两种输入，不要求语料库中展示二者的关系\n\n\n\n—— 没有一个模型能够统一处理这三个问题\n\nSPT-Code就可以！\n\n* 这是一个encoder-decoder共同预训练的模型\n* 数据实例由CODE,AST,NL三部分构成\n* 使用方法名和调用此方法的方式作为自然语言描述（以避免对bilingual corpus的依赖）\n\n\n\n方法：\n\n设计了三种预训练任务，每一种获取一种数据信息\n\n* 改进的MASS-用于CODE：遮蔽Seq2Seq恢复\n* Code-AST Predict CAP：预测code-AST是否匹配\n* Method Name Generation MNG：生成 方法名的 子token\n\n数据集：\n\nCodeSearchNet\n\n贡献：\n\n1. 提出了SPT-Code预训练模型，可用于分类和生成任务\n2. 使用了线性和简化的AST 第一个使用了NL&AST作为输入对于预训练\n3. 通过输入表示和三个与训练任务使得预训练模型不依赖双语语料库（labeled data)\n4. 用未标注数据库在五个下游任务实现了SOTA\n\n# 方法\n\n架构，输入和预训练任务，微调\n\n\n\n## 架构\n\n类似于BART和T5的典型Transformer\n\n分类任务和生成任务，模型采用相同的输入：\n\n* 分类对encoder和decoder输入相同\n* 生成采用传统方法\n\n<img src=\"SPT-Code/image-20220527230530594.png\" alt=\"image-20220527230530594\" style=\"zoom:50%;\" />\n\n## 输入\n\n<img src=\"SPT-Code/image-20220527231806850.png\" alt=\"image-20220527231806850\" style=\"zoom:50%;\" />\n\n由三部分组成，每一部分[SEP]连接\n\n### Code：\n\n没有使用笨蛋tokenizer，而是用了stl for Python 或者 antlr for Java,Php.etc 其他的用了NLTK\n\n###  AST\n\n用的Tree-sitter 搞的 AST \n\n 如何序列化AST?\n\n* 传统方法：SBT （Structure-Based Traversal)\n\n  比先序遍历之类的更有效，但可能产生过长的序列（可能超过代码三倍长）\n\n  <img src=\"SPT-Code/image-20220527232629535.png\" alt=\"image-20220527232629535\" style=\"zoom:50%;\" />\n\n  <center>\n      一种类似中序遍历的说法 来自那篇论文忘了 反正绝对看过\n  </center>\n  \n  \n* 本文的方法：X-SBT：XML-like SBT\n\n  可以减少超过一半的长度\n\n  <img src=\"SPT-Code/image-20220527233143119.png\" alt=\"image-20220527233143119\" style=\"zoom:67%;\" />\n\n  论文自带的图好看一点 这个创新点……只能说是情理之中，毕竟原来那个也太呆了（作者甚至还装模做样证了一下必然更短）\n\n  为了更短: AST——XSBT时，仅取表达式级别以上节点，放弃终结符\n\n  <img src=\"SPT-Code/image-20220527233752649.png\" alt=\"image-20220527233752649\" style=\"zoom:50%;\" />\n\n  **这种优化为可接受的，为什么呢？下面这个说得很漂亮：**\n\n  AST中包含了语法信息和词法信息，舍弃掉终结符丢失了词法信息，但之前的token（Input中Code的部分）都是词法单元，所以这个信息是没有丢掉的，因此改进可接受\n\n### NL\n\n难点：从仅有CODE中提取NL\n\n方法：获取方法名与调用的API序列\n\n对驼峰和下划线命名掰开\n\n问题：怎么提取的API序列：从AST里偷出来的？\n\n——应该就是\n\n## 预训练任务\n\n\n\n𝐼𝑛𝑝𝑢𝑡 = 𝐶,[SEP],𝐴,[SEP],𝑁 \n\n### Code-AST Prediction.\n\n这是第一个\n\n在构建输入𝐼𝑛𝑝𝑢𝑡 时，一半是对应的AST,一半是随机的AST\n\n### MASS\n\n随机遮蔽C中的一部分，将所有遮蔽的token设置为[MASK]（改进前为对应数量个[MASK])\n\n根据别人的论文，最大遮蔽长度是C长度l的一半\n\n\n\n### Method Name Generation\n\n希望可以通过这个任务学到代码的动机\n\n代码名的词汇和对应代码总结的词汇由高度相关，因此希望通过改善 预测代码名 这一任务提升 代码总结 的能力\n\n此任务的输入时，从𝐼𝑛𝑝𝑢𝑡中的C扣掉对应token，并在N中去掉前s个token（方法名总在最前），作为输入，试图让decoder输出扣掉的前s个token，即方法名\n\n\n\n\n\n## 微调\n\n端到端 根据不同任务分成两类，分类或生成，不同任务就缺掉一点输入\n\n# 实验\n\n## 预训练\n\n交代了数据集的数据使用，任务顺序，epoches，用的cross-entropy loss和Adam-W，batchsize和显卡（……）\n\nTokenizer Encoding 用的BPE对CODE和NL，在预训练data上干过 每个下游任务照用\n\n预训练任务的任务量都是每个任务几十个Epoches的量级。\n\n\n\n问题：不是都有token了 还tokenize？\n\n——低级问题，前面的应该是tokenize，这里进行token=>input_ids的步骤\n\n## 下游任务微调\n\n介绍了五个任务 其中介绍部分有点尴尬\n\n\n\n## 评估\n\nRQ1:相比于其他较好的基线 这个性能在下游任务如何？\n\n列个表 比不上人家的扯一点\n\n\n\nRQ2:三个预训练任务对五个下游任务分别有什么贡献？\n\n——消融实验\n\n有趣的是 删除掉MNG （生成 方法的token）在代码完成和代码修复上性能有所提高：\n\n* MNG是目标自然语言的预训练，而这两项任务都是代码到代码\n\n分析一下 什么任务对什么下游有影响\n\n\n\n**稍微有点水平的问题**\n\nRQ3: 可以利用更多无标记的资料是不是本模型的优点呢？\n\n相较于别的预训练模型，由于它的设计，可以使用无标注数据。更好的性能是不是来自于更多的数据呢？（而不是模型本身厉害？）\n\n在同样的数据集上训练——把它当作无标注的——其实和别的比还算吃亏——也能够取得相对别的模型更好的结果。\n\n可以说是赢两遍了。\n\n\n\nRQ4:微调阶段的数据量对下游任务有什么影响？\n\n虽然越小越坏，但是很小也和别的模型差不多 说明真好\n\n\n\n\n\n## 定性分析与定量分析\n\n定量：志愿者评估，多个样本分类列表个\n\n定性：在哪些任务哪些方面表现好 不好的怎么不好\n\n\n\n# 威胁分析\n\n构造：数据集可能有重复\n\n内部：没调过超参数：所以可能有更好的\n\n外部：只用了CodesearchNet\n\n\n\n# 总结：\n\n我们介绍了SPT-Code，这是一个基于编码器架构的源代码的大型型号。首先，我们为预训练SPT代码设计了三个特定代码的预训练任务。其次，我们提出了一种新的输入表示形式，它是第一个考虑自然语言和AST形式的方法，我们还提出了AST遍历方法的改进版本XSBT。我们的预训练任务和输入表示形式都允许在完全未标记的数据集上预先训练SPT代码。然后，对五个与代码相关的下游任务进行了微调。结果表明，微调SPT代码使其能够在五个与代码相关的下游任务上实现最新性能。消融实验表明，这三个预训练任务对不同的下游任务具有不同程度的影响，AST和自然语言输入也有助于提高SPTCODE的性能。为了促进未来的研究，我们还可以在https://github.com/ nougatca/spt-code上公开提供代码和其他。\n\n\n\n\n\n","slug":"SPT-Code","published":1,"updated":"2022-06-20T13:14:47.373Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl4y3s2ze000aqkw911pd3zf8","content":"<p>这是《Sequence-to-Sequence Pre-Training for Learning Source Code Representations》的读书笔记</p>\n<span id=\"more\"></span>\n\n\n\n<h1 id=\"Abstract\"><a href=\"#Abstract\" class=\"headerlink\" title=\"Abstract\"></a>Abstract</h1><p>Pre-trained models 用于代码相关下游任务的应用时的 问题？</p>\n<ol>\n<li>仅用了Pre-trained encoder 但生成任务需要两个部件都预训练</li>\n<li>现在许多Pre-trained model 包括T5，只是简单复用了NL的预训练任务，这要求NL-CODE的corpus 这使得数据受限</li>\n</ol>\n<p>为了应对这两个问题 提出了SPT-Code ，在微调后可以在5个代码相关任务上SOTA</p>\n<p>这是一个seq2seq 预训练模型，通过三个预训练任务使得其能够学习到下面三点，并在下游任务中使用</p>\n<ul>\n<li>代码知识</li>\n<li>对应代码结构</li>\n<li>自然语言描述</li>\n</ul>\n<p>而不需要双语corpus</p>\n<h1 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h1><p>作者在第一部分提了自监督训练，然后说预训练模型的成功和这个有关系，下面谈预训练模型到软工SE任务的时候认为的问题是：</p>\n<ul>\n<li><p>主流预训练模型仅对encoder搞 ，不够理想</p>\n<p>最有名的encoder也就是BERT吧 也确实就是用了MLM（Masked Language Modeling） 也确实有好多Pre-trained Bert 下面套个任务头或者另一个decoder就开干的……不过这应该算是蛮荒时代了 T5虽然好 但是对一般研究人员来说想改进模型architecture 不是那么容易？</p>\n<ul>\n<li><p>别人的解决：</p>\n<p>T5-learning , TreeBERT 两个工作使得Encoder-Decoder jointly trainded</p>\n</li>\n</ul>\n</li>\n<li><p>这些预训练模型预设输入的是NL-CODE 忽视了代码结构</p>\n<p>为什么呢？因为就是简单偷了NLP的拿来用</p>\n<ul>\n<li><p>别人的解决：</p>\n<p>专门的预训练任务 包括预测数据流图中的边与对齐节点和代码 </p>\n<p>——dataflow 有语义信息而无语法信息（AST）</p>\n</li>\n</ul>\n</li>\n<li><p>而且都假设有严格对齐的双语语料</p>\n<ul>\n<li><p>T5-learning :</p>\n<p>分别处理两种输入，不要求语料库中展示二者的关系</p>\n</li>\n</ul>\n</li>\n</ul>\n<p>—— 没有一个模型能够统一处理这三个问题</p>\n<p>SPT-Code就可以！</p>\n<ul>\n<li>这是一个encoder-decoder共同预训练的模型</li>\n<li>数据实例由CODE,AST,NL三部分构成</li>\n<li>使用方法名和调用此方法的方式作为自然语言描述（以避免对bilingual corpus的依赖）</li>\n</ul>\n<p>方法：</p>\n<p>设计了三种预训练任务，每一种获取一种数据信息</p>\n<ul>\n<li>改进的MASS-用于CODE：遮蔽Seq2Seq恢复</li>\n<li>Code-AST Predict CAP：预测code-AST是否匹配</li>\n<li>Method Name Generation MNG：生成 方法名的 子token</li>\n</ul>\n<p>数据集：</p>\n<p>CodeSearchNet</p>\n<p>贡献：</p>\n<ol>\n<li>提出了SPT-Code预训练模型，可用于分类和生成任务</li>\n<li>使用了线性和简化的AST 第一个使用了NL&amp;AST作为输入对于预训练</li>\n<li>通过输入表示和三个与训练任务使得预训练模型不依赖双语语料库（labeled data)</li>\n<li>用未标注数据库在五个下游任务实现了SOTA</li>\n</ol>\n<h1 id=\"方法\"><a href=\"#方法\" class=\"headerlink\" title=\"方法\"></a>方法</h1><p>架构，输入和预训练任务，微调</p>\n<h2 id=\"架构\"><a href=\"#架构\" class=\"headerlink\" title=\"架构\"></a>架构</h2><p>类似于BART和T5的典型Transformer</p>\n<p>分类任务和生成任务，模型采用相同的输入：</p>\n<ul>\n<li>分类对encoder和decoder输入相同</li>\n<li>生成采用传统方法</li>\n</ul>\n<img src=\"/2022/05/27/SPT-Code/image-20220527230530594.png\" alt=\"image-20220527230530594\" style=\"zoom:50%;\">\n\n<h2 id=\"输入\"><a href=\"#输入\" class=\"headerlink\" title=\"输入\"></a>输入</h2><img src=\"/2022/05/27/SPT-Code/image-20220527231806850.png\" alt=\"image-20220527231806850\" style=\"zoom:50%;\">\n\n<p>由三部分组成，每一部分[SEP]连接</p>\n<h3 id=\"Code：\"><a href=\"#Code：\" class=\"headerlink\" title=\"Code：\"></a>Code：</h3><p>没有使用笨蛋tokenizer，而是用了stl for Python 或者 antlr for Java,Php.etc 其他的用了NLTK</p>\n<h3 id=\"AST\"><a href=\"#AST\" class=\"headerlink\" title=\"AST\"></a>AST</h3><p>用的Tree-sitter 搞的 AST </p>\n<p> 如何序列化AST?</p>\n<ul>\n<li><p>传统方法：SBT （Structure-Based Traversal)</p>\n<p>比先序遍历之类的更有效，但可能产生过长的序列（可能超过代码三倍长）</p>\n<img src=\"/2022/05/27/SPT-Code/image-20220527232629535.png\" alt=\"image-20220527232629535\" style=\"zoom:50%;\">\n\n<center>\n    一种类似中序遍历的说法 来自那篇论文忘了 反正绝对看过\n</center>\n\n</li>\n<li><p>本文的方法：X-SBT：XML-like SBT</p>\n<p>可以减少超过一半的长度</p>\n<img src=\"/2022/05/27/SPT-Code/image-20220527233143119.png\" alt=\"image-20220527233143119\" style=\"zoom:67%;\">\n\n<p>论文自带的图好看一点 这个创新点……只能说是情理之中，毕竟原来那个也太呆了（作者甚至还装模做样证了一下必然更短）</p>\n<p>为了更短: AST——XSBT时，仅取表达式级别以上节点，放弃终结符</p>\n<img src=\"/2022/05/27/SPT-Code/image-20220527233752649.png\" alt=\"image-20220527233752649\" style=\"zoom:50%;\">\n\n<p><strong>这种优化为可接受的，为什么呢？下面这个说得很漂亮：</strong></p>\n<p>AST中包含了语法信息和词法信息，舍弃掉终结符丢失了词法信息，但之前的token（Input中Code的部分）都是词法单元，所以这个信息是没有丢掉的，因此改进可接受</p>\n</li>\n</ul>\n<h3 id=\"NL\"><a href=\"#NL\" class=\"headerlink\" title=\"NL\"></a>NL</h3><p>难点：从仅有CODE中提取NL</p>\n<p>方法：获取方法名与调用的API序列</p>\n<p>对驼峰和下划线命名掰开</p>\n<p>问题：怎么提取的API序列：从AST里偷出来的？</p>\n<p>——应该就是</p>\n<h2 id=\"预训练任务\"><a href=\"#预训练任务\" class=\"headerlink\" title=\"预训练任务\"></a>预训练任务</h2><p>𝐼𝑛𝑝𝑢𝑡 &#x3D; 𝐶,[SEP],𝐴,[SEP],𝑁 </p>\n<h3 id=\"Code-AST-Prediction\"><a href=\"#Code-AST-Prediction\" class=\"headerlink\" title=\"Code-AST Prediction.\"></a>Code-AST Prediction.</h3><p>这是第一个</p>\n<p>在构建输入𝐼𝑛𝑝𝑢𝑡 时，一半是对应的AST,一半是随机的AST</p>\n<h3 id=\"MASS\"><a href=\"#MASS\" class=\"headerlink\" title=\"MASS\"></a>MASS</h3><p>随机遮蔽C中的一部分，将所有遮蔽的token设置为[MASK]（改进前为对应数量个[MASK])</p>\n<p>根据别人的论文，最大遮蔽长度是C长度l的一半</p>\n<h3 id=\"Method-Name-Generation\"><a href=\"#Method-Name-Generation\" class=\"headerlink\" title=\"Method Name Generation\"></a>Method Name Generation</h3><p>希望可以通过这个任务学到代码的动机</p>\n<p>代码名的词汇和对应代码总结的词汇由高度相关，因此希望通过改善 预测代码名 这一任务提升 代码总结 的能力</p>\n<p>此任务的输入时，从𝐼𝑛𝑝𝑢𝑡中的C扣掉对应token，并在N中去掉前s个token（方法名总在最前），作为输入，试图让decoder输出扣掉的前s个token，即方法名</p>\n<h2 id=\"微调\"><a href=\"#微调\" class=\"headerlink\" title=\"微调\"></a>微调</h2><p>端到端 根据不同任务分成两类，分类或生成，不同任务就缺掉一点输入</p>\n<h1 id=\"实验\"><a href=\"#实验\" class=\"headerlink\" title=\"实验\"></a>实验</h1><h2 id=\"预训练\"><a href=\"#预训练\" class=\"headerlink\" title=\"预训练\"></a>预训练</h2><p>交代了数据集的数据使用，任务顺序，epoches，用的cross-entropy loss和Adam-W，batchsize和显卡（……）</p>\n<p>Tokenizer Encoding 用的BPE对CODE和NL，在预训练data上干过 每个下游任务照用</p>\n<p>预训练任务的任务量都是每个任务几十个Epoches的量级。</p>\n<p>问题：不是都有token了 还tokenize？</p>\n<p>——低级问题，前面的应该是tokenize，这里进行token&#x3D;&gt;input_ids的步骤</p>\n<h2 id=\"下游任务微调\"><a href=\"#下游任务微调\" class=\"headerlink\" title=\"下游任务微调\"></a>下游任务微调</h2><p>介绍了五个任务 其中介绍部分有点尴尬</p>\n<h2 id=\"评估\"><a href=\"#评估\" class=\"headerlink\" title=\"评估\"></a>评估</h2><p>RQ1:相比于其他较好的基线 这个性能在下游任务如何？</p>\n<p>列个表 比不上人家的扯一点</p>\n<p>RQ2:三个预训练任务对五个下游任务分别有什么贡献？</p>\n<p>——消融实验</p>\n<p>有趣的是 删除掉MNG （生成 方法的token）在代码完成和代码修复上性能有所提高：</p>\n<ul>\n<li>MNG是目标自然语言的预训练，而这两项任务都是代码到代码</li>\n</ul>\n<p>分析一下 什么任务对什么下游有影响</p>\n<p><strong>稍微有点水平的问题</strong></p>\n<p>RQ3: 可以利用更多无标记的资料是不是本模型的优点呢？</p>\n<p>相较于别的预训练模型，由于它的设计，可以使用无标注数据。更好的性能是不是来自于更多的数据呢？（而不是模型本身厉害？）</p>\n<p>在同样的数据集上训练——把它当作无标注的——其实和别的比还算吃亏——也能够取得相对别的模型更好的结果。</p>\n<p>可以说是赢两遍了。</p>\n<p>RQ4:微调阶段的数据量对下游任务有什么影响？</p>\n<p>虽然越小越坏，但是很小也和别的模型差不多 说明真好</p>\n<h2 id=\"定性分析与定量分析\"><a href=\"#定性分析与定量分析\" class=\"headerlink\" title=\"定性分析与定量分析\"></a>定性分析与定量分析</h2><p>定量：志愿者评估，多个样本分类列表个</p>\n<p>定性：在哪些任务哪些方面表现好 不好的怎么不好</p>\n<h1 id=\"威胁分析\"><a href=\"#威胁分析\" class=\"headerlink\" title=\"威胁分析\"></a>威胁分析</h1><p>构造：数据集可能有重复</p>\n<p>内部：没调过超参数：所以可能有更好的</p>\n<p>外部：只用了CodesearchNet</p>\n<h1 id=\"总结：\"><a href=\"#总结：\" class=\"headerlink\" title=\"总结：\"></a>总结：</h1><p>我们介绍了SPT-Code，这是一个基于编码器架构的源代码的大型型号。首先，我们为预训练SPT代码设计了三个特定代码的预训练任务。其次，我们提出了一种新的输入表示形式，它是第一个考虑自然语言和AST形式的方法，我们还提出了AST遍历方法的改进版本XSBT。我们的预训练任务和输入表示形式都允许在完全未标记的数据集上预先训练SPT代码。然后，对五个与代码相关的下游任务进行了微调。结果表明，微调SPT代码使其能够在五个与代码相关的下游任务上实现最新性能。消融实验表明，这三个预训练任务对不同的下游任务具有不同程度的影响，AST和自然语言输入也有助于提高SPTCODE的性能。为了促进未来的研究，我们还可以在<a href=\"https://github.com/\">https://github.com/</a> nougatca&#x2F;spt-code上公开提供代码和其他。</p>\n","site":{"data":{}},"excerpt":"<p>这是《Sequence-to-Sequence Pre-Training for Learning Source Code Representations》的读书笔记</p>","more":"<h1 id=\"Abstract\"><a href=\"#Abstract\" class=\"headerlink\" title=\"Abstract\"></a>Abstract</h1><p>Pre-trained models 用于代码相关下游任务的应用时的 问题？</p>\n<ol>\n<li>仅用了Pre-trained encoder 但生成任务需要两个部件都预训练</li>\n<li>现在许多Pre-trained model 包括T5，只是简单复用了NL的预训练任务，这要求NL-CODE的corpus 这使得数据受限</li>\n</ol>\n<p>为了应对这两个问题 提出了SPT-Code ，在微调后可以在5个代码相关任务上SOTA</p>\n<p>这是一个seq2seq 预训练模型，通过三个预训练任务使得其能够学习到下面三点，并在下游任务中使用</p>\n<ul>\n<li>代码知识</li>\n<li>对应代码结构</li>\n<li>自然语言描述</li>\n</ul>\n<p>而不需要双语corpus</p>\n<h1 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h1><p>作者在第一部分提了自监督训练，然后说预训练模型的成功和这个有关系，下面谈预训练模型到软工SE任务的时候认为的问题是：</p>\n<ul>\n<li><p>主流预训练模型仅对encoder搞 ，不够理想</p>\n<p>最有名的encoder也就是BERT吧 也确实就是用了MLM（Masked Language Modeling） 也确实有好多Pre-trained Bert 下面套个任务头或者另一个decoder就开干的……不过这应该算是蛮荒时代了 T5虽然好 但是对一般研究人员来说想改进模型architecture 不是那么容易？</p>\n<ul>\n<li><p>别人的解决：</p>\n<p>T5-learning , TreeBERT 两个工作使得Encoder-Decoder jointly trainded</p>\n</li>\n</ul>\n</li>\n<li><p>这些预训练模型预设输入的是NL-CODE 忽视了代码结构</p>\n<p>为什么呢？因为就是简单偷了NLP的拿来用</p>\n<ul>\n<li><p>别人的解决：</p>\n<p>专门的预训练任务 包括预测数据流图中的边与对齐节点和代码 </p>\n<p>——dataflow 有语义信息而无语法信息（AST）</p>\n</li>\n</ul>\n</li>\n<li><p>而且都假设有严格对齐的双语语料</p>\n<ul>\n<li><p>T5-learning :</p>\n<p>分别处理两种输入，不要求语料库中展示二者的关系</p>\n</li>\n</ul>\n</li>\n</ul>\n<p>—— 没有一个模型能够统一处理这三个问题</p>\n<p>SPT-Code就可以！</p>\n<ul>\n<li>这是一个encoder-decoder共同预训练的模型</li>\n<li>数据实例由CODE,AST,NL三部分构成</li>\n<li>使用方法名和调用此方法的方式作为自然语言描述（以避免对bilingual corpus的依赖）</li>\n</ul>\n<p>方法：</p>\n<p>设计了三种预训练任务，每一种获取一种数据信息</p>\n<ul>\n<li>改进的MASS-用于CODE：遮蔽Seq2Seq恢复</li>\n<li>Code-AST Predict CAP：预测code-AST是否匹配</li>\n<li>Method Name Generation MNG：生成 方法名的 子token</li>\n</ul>\n<p>数据集：</p>\n<p>CodeSearchNet</p>\n<p>贡献：</p>\n<ol>\n<li>提出了SPT-Code预训练模型，可用于分类和生成任务</li>\n<li>使用了线性和简化的AST 第一个使用了NL&amp;AST作为输入对于预训练</li>\n<li>通过输入表示和三个与训练任务使得预训练模型不依赖双语语料库（labeled data)</li>\n<li>用未标注数据库在五个下游任务实现了SOTA</li>\n</ol>\n<h1 id=\"方法\"><a href=\"#方法\" class=\"headerlink\" title=\"方法\"></a>方法</h1><p>架构，输入和预训练任务，微调</p>\n<h2 id=\"架构\"><a href=\"#架构\" class=\"headerlink\" title=\"架构\"></a>架构</h2><p>类似于BART和T5的典型Transformer</p>\n<p>分类任务和生成任务，模型采用相同的输入：</p>\n<ul>\n<li>分类对encoder和decoder输入相同</li>\n<li>生成采用传统方法</li>\n</ul>\n<img src=\"/2022/05/27/SPT-Code/image-20220527230530594.png\" alt=\"image-20220527230530594\" style=\"zoom:50%;\">\n\n<h2 id=\"输入\"><a href=\"#输入\" class=\"headerlink\" title=\"输入\"></a>输入</h2><img src=\"/2022/05/27/SPT-Code/image-20220527231806850.png\" alt=\"image-20220527231806850\" style=\"zoom:50%;\">\n\n<p>由三部分组成，每一部分[SEP]连接</p>\n<h3 id=\"Code：\"><a href=\"#Code：\" class=\"headerlink\" title=\"Code：\"></a>Code：</h3><p>没有使用笨蛋tokenizer，而是用了stl for Python 或者 antlr for Java,Php.etc 其他的用了NLTK</p>\n<h3 id=\"AST\"><a href=\"#AST\" class=\"headerlink\" title=\"AST\"></a>AST</h3><p>用的Tree-sitter 搞的 AST </p>\n<p> 如何序列化AST?</p>\n<ul>\n<li><p>传统方法：SBT （Structure-Based Traversal)</p>\n<p>比先序遍历之类的更有效，但可能产生过长的序列（可能超过代码三倍长）</p>\n<img src=\"/2022/05/27/SPT-Code/image-20220527232629535.png\" alt=\"image-20220527232629535\" style=\"zoom:50%;\">\n\n<center>\n    一种类似中序遍历的说法 来自那篇论文忘了 反正绝对看过\n</center>\n\n</li>\n<li><p>本文的方法：X-SBT：XML-like SBT</p>\n<p>可以减少超过一半的长度</p>\n<img src=\"/2022/05/27/SPT-Code/image-20220527233143119.png\" alt=\"image-20220527233143119\" style=\"zoom:67%;\">\n\n<p>论文自带的图好看一点 这个创新点……只能说是情理之中，毕竟原来那个也太呆了（作者甚至还装模做样证了一下必然更短）</p>\n<p>为了更短: AST——XSBT时，仅取表达式级别以上节点，放弃终结符</p>\n<img src=\"/2022/05/27/SPT-Code/image-20220527233752649.png\" alt=\"image-20220527233752649\" style=\"zoom:50%;\">\n\n<p><strong>这种优化为可接受的，为什么呢？下面这个说得很漂亮：</strong></p>\n<p>AST中包含了语法信息和词法信息，舍弃掉终结符丢失了词法信息，但之前的token（Input中Code的部分）都是词法单元，所以这个信息是没有丢掉的，因此改进可接受</p>\n</li>\n</ul>\n<h3 id=\"NL\"><a href=\"#NL\" class=\"headerlink\" title=\"NL\"></a>NL</h3><p>难点：从仅有CODE中提取NL</p>\n<p>方法：获取方法名与调用的API序列</p>\n<p>对驼峰和下划线命名掰开</p>\n<p>问题：怎么提取的API序列：从AST里偷出来的？</p>\n<p>——应该就是</p>\n<h2 id=\"预训练任务\"><a href=\"#预训练任务\" class=\"headerlink\" title=\"预训练任务\"></a>预训练任务</h2><p>𝐼𝑛𝑝𝑢𝑡 &#x3D; 𝐶,[SEP],𝐴,[SEP],𝑁 </p>\n<h3 id=\"Code-AST-Prediction\"><a href=\"#Code-AST-Prediction\" class=\"headerlink\" title=\"Code-AST Prediction.\"></a>Code-AST Prediction.</h3><p>这是第一个</p>\n<p>在构建输入𝐼𝑛𝑝𝑢𝑡 时，一半是对应的AST,一半是随机的AST</p>\n<h3 id=\"MASS\"><a href=\"#MASS\" class=\"headerlink\" title=\"MASS\"></a>MASS</h3><p>随机遮蔽C中的一部分，将所有遮蔽的token设置为[MASK]（改进前为对应数量个[MASK])</p>\n<p>根据别人的论文，最大遮蔽长度是C长度l的一半</p>\n<h3 id=\"Method-Name-Generation\"><a href=\"#Method-Name-Generation\" class=\"headerlink\" title=\"Method Name Generation\"></a>Method Name Generation</h3><p>希望可以通过这个任务学到代码的动机</p>\n<p>代码名的词汇和对应代码总结的词汇由高度相关，因此希望通过改善 预测代码名 这一任务提升 代码总结 的能力</p>\n<p>此任务的输入时，从𝐼𝑛𝑝𝑢𝑡中的C扣掉对应token，并在N中去掉前s个token（方法名总在最前），作为输入，试图让decoder输出扣掉的前s个token，即方法名</p>\n<h2 id=\"微调\"><a href=\"#微调\" class=\"headerlink\" title=\"微调\"></a>微调</h2><p>端到端 根据不同任务分成两类，分类或生成，不同任务就缺掉一点输入</p>\n<h1 id=\"实验\"><a href=\"#实验\" class=\"headerlink\" title=\"实验\"></a>实验</h1><h2 id=\"预训练\"><a href=\"#预训练\" class=\"headerlink\" title=\"预训练\"></a>预训练</h2><p>交代了数据集的数据使用，任务顺序，epoches，用的cross-entropy loss和Adam-W，batchsize和显卡（……）</p>\n<p>Tokenizer Encoding 用的BPE对CODE和NL，在预训练data上干过 每个下游任务照用</p>\n<p>预训练任务的任务量都是每个任务几十个Epoches的量级。</p>\n<p>问题：不是都有token了 还tokenize？</p>\n<p>——低级问题，前面的应该是tokenize，这里进行token&#x3D;&gt;input_ids的步骤</p>\n<h2 id=\"下游任务微调\"><a href=\"#下游任务微调\" class=\"headerlink\" title=\"下游任务微调\"></a>下游任务微调</h2><p>介绍了五个任务 其中介绍部分有点尴尬</p>\n<h2 id=\"评估\"><a href=\"#评估\" class=\"headerlink\" title=\"评估\"></a>评估</h2><p>RQ1:相比于其他较好的基线 这个性能在下游任务如何？</p>\n<p>列个表 比不上人家的扯一点</p>\n<p>RQ2:三个预训练任务对五个下游任务分别有什么贡献？</p>\n<p>——消融实验</p>\n<p>有趣的是 删除掉MNG （生成 方法的token）在代码完成和代码修复上性能有所提高：</p>\n<ul>\n<li>MNG是目标自然语言的预训练，而这两项任务都是代码到代码</li>\n</ul>\n<p>分析一下 什么任务对什么下游有影响</p>\n<p><strong>稍微有点水平的问题</strong></p>\n<p>RQ3: 可以利用更多无标记的资料是不是本模型的优点呢？</p>\n<p>相较于别的预训练模型，由于它的设计，可以使用无标注数据。更好的性能是不是来自于更多的数据呢？（而不是模型本身厉害？）</p>\n<p>在同样的数据集上训练——把它当作无标注的——其实和别的比还算吃亏——也能够取得相对别的模型更好的结果。</p>\n<p>可以说是赢两遍了。</p>\n<p>RQ4:微调阶段的数据量对下游任务有什么影响？</p>\n<p>虽然越小越坏，但是很小也和别的模型差不多 说明真好</p>\n<h2 id=\"定性分析与定量分析\"><a href=\"#定性分析与定量分析\" class=\"headerlink\" title=\"定性分析与定量分析\"></a>定性分析与定量分析</h2><p>定量：志愿者评估，多个样本分类列表个</p>\n<p>定性：在哪些任务哪些方面表现好 不好的怎么不好</p>\n<h1 id=\"威胁分析\"><a href=\"#威胁分析\" class=\"headerlink\" title=\"威胁分析\"></a>威胁分析</h1><p>构造：数据集可能有重复</p>\n<p>内部：没调过超参数：所以可能有更好的</p>\n<p>外部：只用了CodesearchNet</p>\n<h1 id=\"总结：\"><a href=\"#总结：\" class=\"headerlink\" title=\"总结：\"></a>总结：</h1><p>我们介绍了SPT-Code，这是一个基于编码器架构的源代码的大型型号。首先，我们为预训练SPT代码设计了三个特定代码的预训练任务。其次，我们提出了一种新的输入表示形式，它是第一个考虑自然语言和AST形式的方法，我们还提出了AST遍历方法的改进版本XSBT。我们的预训练任务和输入表示形式都允许在完全未标记的数据集上预先训练SPT代码。然后，对五个与代码相关的下游任务进行了微调。结果表明，微调SPT代码使其能够在五个与代码相关的下游任务上实现最新性能。消融实验表明，这三个预训练任务对不同的下游任务具有不同程度的影响，AST和自然语言输入也有助于提高SPTCODE的性能。为了促进未来的研究，我们还可以在<a href=\"https://github.com/\">https://github.com/</a> nougatca&#x2F;spt-code上公开提供代码和其他。</p>"},{"title":"Philip Larkin 诗鉴赏","date":"2022-06-04T17:26:03.000Z","_content":"\n\n\n今天读到了Philip Larkin(菲利普·拉金) 觉得他确实写出了一些人类的共有困境，是超越东西方的 也不好说是现代视角或者古典视角。用诗歌描述一种……symptom？没有意象的堆叠或者是故作惊人之语。恰到好处的建筑与音韵意识又不喧宾夺主，克制的情感流露并着个人色彩。 类似于散文诗？\n<!--more-->\n现摘在这里这一首，「Love Songs in Age」\nShe kept her songs, they kept so little space,\nThe covers pleased her:\nOne bleached[^1] from lying in a sunny place,\nOne marked in circles by a vase of water,\nOne mended[^2], when a tidy fit had seized her,\nAnd coloured, by her daughter -\nSo they had waited, till, in widowhood\nShe found them, looking for something else, and stood\n\nRelearning how each frank submissive chord[^3]\nHad ushered[^4] in\nWord after sprawling hyphenated[^5] word,\nAnd the unfailing sense of being young\nSpread out like a spring-woken tree, wherein\nThat hidden freshness sung,\nThat certainty of time laid up in store\nAs when she played them first. But, even more,\n\nThe glare[^6] of that much-mentionned brilliance, love,\nBroke out, to show\nIts bright incipience[^7] sailing above,\nStill promising to solve, and satisfy,\nAnd set unchangeably in order. So\nTo pile them back, to cry,\nWas hard, without lamely[^8] admitting how\nIt had not done so then, and could not now.\n\n\n\n[^1]: floating, drift,漂白\n[^2]: repair,patch,修补\n[^3]:和弦\n[^4]:（迎宾员式的）引导\n[^5]:连字符\n[^6 ]:great brightness,耀眼的\n[^7]:beginning to exist or to be apparent\n[^8]:in a weak and unconvincing manner\n\n","source":"_posts/Philip-Larkin.md","raw":"---\ntitle: Philip Larkin 诗鉴赏\ndate: 2022-06-05 01:26:03\ntags:\n- Poem\n- Literary criticism\n- Philip Larkin\n\ncategories:\n- Metaphysicsk\n---\n\n\n\n今天读到了Philip Larkin(菲利普·拉金) 觉得他确实写出了一些人类的共有困境，是超越东西方的 也不好说是现代视角或者古典视角。用诗歌描述一种……symptom？没有意象的堆叠或者是故作惊人之语。恰到好处的建筑与音韵意识又不喧宾夺主，克制的情感流露并着个人色彩。 类似于散文诗？\n<!--more-->\n现摘在这里这一首，「Love Songs in Age」\nShe kept her songs, they kept so little space,\nThe covers pleased her:\nOne bleached[^1] from lying in a sunny place,\nOne marked in circles by a vase of water,\nOne mended[^2], when a tidy fit had seized her,\nAnd coloured, by her daughter -\nSo they had waited, till, in widowhood\nShe found them, looking for something else, and stood\n\nRelearning how each frank submissive chord[^3]\nHad ushered[^4] in\nWord after sprawling hyphenated[^5] word,\nAnd the unfailing sense of being young\nSpread out like a spring-woken tree, wherein\nThat hidden freshness sung,\nThat certainty of time laid up in store\nAs when she played them first. But, even more,\n\nThe glare[^6] of that much-mentionned brilliance, love,\nBroke out, to show\nIts bright incipience[^7] sailing above,\nStill promising to solve, and satisfy,\nAnd set unchangeably in order. So\nTo pile them back, to cry,\nWas hard, without lamely[^8] admitting how\nIt had not done so then, and could not now.\n\n\n\n[^1]: floating, drift,漂白\n[^2]: repair,patch,修补\n[^3]:和弦\n[^4]:（迎宾员式的）引导\n[^5]:连字符\n[^6 ]:great brightness,耀眼的\n[^7]:beginning to exist or to be apparent\n[^8]:in a weak and unconvincing manner\n\n","slug":"Philip-Larkin","published":1,"updated":"2022-06-28T11:45:56.876Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl4y3s2zf000bqkw9ci6q1fiv","content":"<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css\"><p>今天读到了Philip Larkin(菲利普·拉金) 觉得他确实写出了一些人类的共有困境，是超越东西方的 也不好说是现代视角或者古典视角。用诗歌描述一种……symptom？没有意象的堆叠或者是故作惊人之语。恰到好处的建筑与音韵意识又不喧宾夺主，克制的情感流露并着个人色彩。 类似于散文诗？</p>\n<span id=\"more\"></span>\n<p>现摘在这里这一首，「Love Songs in Age」<br>She kept her songs, they kept so little space,<br>The covers pleased her:<br>One bleached<sup id=\"fnref:1\"><a href=\"#fn:1\" rel=\"footnote\"><span class=\"hint--top hint--error hint--medium hint--rounded hint--bounce\" aria-label=\"floating, drift,漂白\n\">1</span></a></sup> from lying in a sunny place,<br>One marked in circles by a vase of water,<br>One mended<sup id=\"fnref:2\"><a href=\"#fn:2\" rel=\"footnote\"><span class=\"hint--top hint--error hint--medium hint--rounded hint--bounce\" aria-label=\"repair,patch,修补\n\">2</span></a></sup>, when a tidy fit had seized her,<br>And coloured, by her daughter -<br>So they had waited, till, in widowhood<br>She found them, looking for something else, and stood</p>\n<p>Relearning how each frank submissive chord<sup id=\"fnref:3\"><a href=\"#fn:3\" rel=\"footnote\"><span class=\"hint--top hint--error hint--medium hint--rounded hint--bounce\" aria-label=\"和弦\n\">3</span></a></sup><br>Had ushered<sup id=\"fnref:4\"><a href=\"#fn:4\" rel=\"footnote\"><span class=\"hint--top hint--error hint--medium hint--rounded hint--bounce\" aria-label=\"（迎宾员式的）引导\n\">4</span></a></sup> in<br>Word after sprawling hyphenated<sup id=\"fnref:5\"><a href=\"#fn:5\" rel=\"footnote\"><span class=\"hint--top hint--error hint--medium hint--rounded hint--bounce\" aria-label=\"连字符\n[^6 ]:great brightness,耀眼的\n\">5</span></a></sup> word,<br>And the unfailing sense of being young<br>Spread out like a spring-woken tree, wherein<br>That hidden freshness sung,<br>That certainty of time laid up in store<br>As when she played them first. But, even more,</p>\n<p>The glare<sup id=\"fnref:6\"><a href=\"#fn:6\" rel=\"footnote\"><span class=\"hint--top hint--error hint--medium hint--rounded hint--bounce\" aria-label>6</span></a></sup> of that much-mentionned brilliance, love,<br>Broke out, to show<br>Its bright incipience<sup id=\"fnref:7\"><a href=\"#fn:7\" rel=\"footnote\"><span class=\"hint--top hint--error hint--medium hint--rounded hint--bounce\" aria-label=\"beginning to exist or to be apparent\n\">7</span></a></sup> sailing above,<br>Still promising to solve, and satisfy,<br>And set unchangeably in order. So<br>To pile them back, to cry,<br>Was hard, without lamely<sup id=\"fnref:8\"><a href=\"#fn:8\" rel=\"footnote\"><span class=\"hint--top hint--error hint--medium hint--rounded hint--bounce\" aria-label=\"in a weak and unconvincing manner\">8</span></a></sup> admitting how<br>It had not done so then, and could not now.</p>\n<div id=\"footnotes\"><hr><div id=\"footnotelist\"><ol style=\"list-style: none; padding-left: 0; margin-left: 40px\"><li id=\"fn:1\"><span style=\"display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px\">1.</span><span style=\"display: inline-block; vertical-align: top; margin-left: 10px;\">floating, drift,漂白<a href=\"#fnref:1\" rev=\"footnote\">↩</a></span></li><li id=\"fn:2\"><span style=\"display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px\">2.</span><span style=\"display: inline-block; vertical-align: top; margin-left: 10px;\">repair,patch,修补<a href=\"#fnref:2\" rev=\"footnote\">↩</a></span></li><li id=\"fn:3\"><span style=\"display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px\">3.</span><span style=\"display: inline-block; vertical-align: top; margin-left: 10px;\">和弦<a href=\"#fnref:3\" rev=\"footnote\">↩</a></span></li><li id=\"fn:4\"><span style=\"display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px\">4.</span><span style=\"display: inline-block; vertical-align: top; margin-left: 10px;\">（迎宾员式的）引导<a href=\"#fnref:4\" rev=\"footnote\">↩</a></span></li><li id=\"fn:5\"><span style=\"display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px\">5.</span><span style=\"display: inline-block; vertical-align: top; margin-left: 10px;\">连字符\n[^6 ]:great brightness,耀眼的<a href=\"#fnref:5\" rev=\"footnote\">↩</a></span></li><li id=\"fn:7\"><span style=\"display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px\">7.</span><span style=\"display: inline-block; vertical-align: top; margin-left: 10px;\">beginning to exist or to be apparent<a href=\"#fnref:7\" rev=\"footnote\">↩</a></span></li><li id=\"fn:8\"><span style=\"display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px\">8.</span><span style=\"display: inline-block; vertical-align: top; margin-left: 10px;\">in a weak and unconvincing manner<a href=\"#fnref:8\" rev=\"footnote\">↩</a></span></li></ol></div></div>","site":{"data":{}},"excerpt":"<p>今天读到了Philip Larkin(菲利普·拉金) 觉得他确实写出了一些人类的共有困境，是超越东西方的 也不好说是现代视角或者古典视角。用诗歌描述一种……symptom？没有意象的堆叠或者是故作惊人之语。恰到好处的建筑与音韵意识又不喧宾夺主，克制的情感流露并着个人色彩。 类似于散文诗？</p>","more":"<p>现摘在这里这一首，「Love Songs in Age」<br>She kept her songs, they kept so little space,<br>The covers pleased her:<br>One bleached<sup id=\"fnref:1\"><a href=\"#fn:1\" rel=\"footnote\"><span class=\"hint--top hint--error hint--medium hint--rounded hint--bounce\" aria-label=\"floating, drift,漂白\n\">1</span></a></sup> from lying in a sunny place,<br>One marked in circles by a vase of water,<br>One mended<sup id=\"fnref:2\"><a href=\"#fn:2\" rel=\"footnote\"><span class=\"hint--top hint--error hint--medium hint--rounded hint--bounce\" aria-label=\"repair,patch,修补\n\">2</span></a></sup>, when a tidy fit had seized her,<br>And coloured, by her daughter -<br>So they had waited, till, in widowhood<br>She found them, looking for something else, and stood</p>\n<p>Relearning how each frank submissive chord<sup id=\"fnref:3\"><a href=\"#fn:3\" rel=\"footnote\"><span class=\"hint--top hint--error hint--medium hint--rounded hint--bounce\" aria-label=\"和弦\n\">3</span></a></sup><br>Had ushered<sup id=\"fnref:4\"><a href=\"#fn:4\" rel=\"footnote\"><span class=\"hint--top hint--error hint--medium hint--rounded hint--bounce\" aria-label=\"（迎宾员式的）引导\n\">4</span></a></sup> in<br>Word after sprawling hyphenated<sup id=\"fnref:5\"><a href=\"#fn:5\" rel=\"footnote\"><span class=\"hint--top hint--error hint--medium hint--rounded hint--bounce\" aria-label=\"连字符\n[^6 ]:great brightness,耀眼的\n\">5</span></a></sup> word,<br>And the unfailing sense of being young<br>Spread out like a spring-woken tree, wherein<br>That hidden freshness sung,<br>That certainty of time laid up in store<br>As when she played them first. But, even more,</p>\n<p>The glare<sup id=\"fnref:6\"><a href=\"#fn:6\" rel=\"footnote\"><span class=\"hint--top hint--error hint--medium hint--rounded hint--bounce\" aria-label>6</span></a></sup> of that much-mentionned brilliance, love,<br>Broke out, to show<br>Its bright incipience<sup id=\"fnref:7\"><a href=\"#fn:7\" rel=\"footnote\"><span class=\"hint--top hint--error hint--medium hint--rounded hint--bounce\" aria-label=\"beginning to exist or to be apparent\n\">7</span></a></sup> sailing above,<br>Still promising to solve, and satisfy,<br>And set unchangeably in order. So<br>To pile them back, to cry,<br>Was hard, without lamely<sup id=\"fnref:8\"><a href=\"#fn:8\" rel=\"footnote\"><span class=\"hint--top hint--error hint--medium hint--rounded hint--bounce\" aria-label=\"in a weak and unconvincing manner\">8</span></a></sup> admitting how<br>It had not done so then, and could not now.</p>\n<div id=\"footnotes\"><hr><div id=\"footnotelist\"><ol style=\"list-style: none; padding-left: 0; margin-left: 40px\"><li id=\"fn:1\"><span style=\"display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px\">1.</span><span style=\"display: inline-block; vertical-align: top; margin-left: 10px;\">floating, drift,漂白<a href=\"#fnref:1\" rev=\"footnote\">↩</a></span></li><li id=\"fn:2\"><span style=\"display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px\">2.</span><span style=\"display: inline-block; vertical-align: top; margin-left: 10px;\">repair,patch,修补<a href=\"#fnref:2\" rev=\"footnote\">↩</a></span></li><li id=\"fn:3\"><span style=\"display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px\">3.</span><span style=\"display: inline-block; vertical-align: top; margin-left: 10px;\">和弦<a href=\"#fnref:3\" rev=\"footnote\">↩</a></span></li><li id=\"fn:4\"><span style=\"display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px\">4.</span><span style=\"display: inline-block; vertical-align: top; margin-left: 10px;\">（迎宾员式的）引导<a href=\"#fnref:4\" rev=\"footnote\">↩</a></span></li><li id=\"fn:5\"><span style=\"display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px\">5.</span><span style=\"display: inline-block; vertical-align: top; margin-left: 10px;\">连字符\n[^6 ]:great brightness,耀眼的<a href=\"#fnref:5\" rev=\"footnote\">↩</a></span></li><li id=\"fn:7\"><span style=\"display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px\">7.</span><span style=\"display: inline-block; vertical-align: top; margin-left: 10px;\">beginning to exist or to be apparent<a href=\"#fnref:7\" rev=\"footnote\">↩</a></span></li><li id=\"fn:8\"><span style=\"display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px\">8.</span><span style=\"display: inline-block; vertical-align: top; margin-left: 10px;\">in a weak and unconvincing manner<a href=\"#fnref:8\" rev=\"footnote\">↩</a></span></li></ol></div></div>"},{"title":"Follow Ace Taffy Meow！","date":"2022-05-23T12:33:42.000Z","_content":"```python\nprint('Follow Ace Taffy thanks Meow!')\n```\n![image-20220524021029166](testpic/image-20220524021029166.png)\n","source":"_posts/testpic.md","raw":"---\ntitle: Follow Ace Taffy Meow！\ndate: 2022-05-23 20:33:42\ncategories: \n- Meow\ntags:\n- Ace\n- Taffy\n---\n```python\nprint('Follow Ace Taffy thanks Meow!')\n```\n![image-20220524021029166](testpic/image-20220524021029166.png)\n","slug":"testpic","published":1,"updated":"2022-06-20T11:16:09.626Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl4y3s2zg000eqkw9hpsm6jes","content":"<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;Follow Ace Taffy thanks Meow!&#x27;</span>)</span><br></pre></td></tr></table></figure>\n<p><img src=\"/2022/05/23/testpic/image-20220524021029166.png\" alt=\"image-20220524021029166\"></p>\n","site":{"data":{}},"excerpt":"","more":"<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;Follow Ace Taffy thanks Meow!&#x27;</span>)</span><br></pre></td></tr></table></figure>\n<p><img src=\"/2022/05/23/testpic/image-20220524021029166.png\" alt=\"image-20220524021029166\"></p>\n"},{"title":"火柴","date":"2022-05-28T18:20:04.000Z","_content":"我在[^6]水中桥下[^1] 饮酒[^2]/~~忘相泉涸[^4]前日的红烛[^5]泪眼[^3]~~红烛淡忘镜中的泪眼\n\n[^1]:「尾生與女子期於梁下，女子不來，水至不去，抱樑柱而死。」《莊子·盜跖》\n[^2]:  「夢飲酒者，旦而哭泣；夢哭泣者，旦而田獵。方其夢也，不知其夢也。」《莊子·齊物論》\n[^3]: 「文侯與虞人期獵。是日，飲酒樂，天雨。文侯將出，左右曰：「今日飲酒樂，天又雨，公將焉之？」文侯曰：「吾與虞人期獵，雖樂，豈可不一會期哉！」乃往，身自罷之。」《戰國策·魏策》\n[^4]: 「泉涸，魚相與處於陸，相呴以溼，相濡以沫，不如相忘於江湖。」《莊子·大宗師》\n\n[^5]: 「蠟炬成灰淚始幹」 李商隐\n[^6]: 火柴梗快要烧完的时候，会因为火焰过分接近而忍不住松手\n\n\n\n自己写完还解诗的人肯定是天下第一无聊 但这只不过是记录灵感媾和时的一些脉络和取舍。\n<!--more-->\n\n\n***\n\n2022/5/30日凌晨睡不着床上对后半句做了修正\n\n第二天起来看前天写得也太迫真了 简直就是笨蛋版本李商隐\n\n当时是怎么想的呢？「忘相」是什么表达？生怕别人看不出来是你直接从庄子里偷来的？「泉涸」同理，太白太直了 反而失去了解读空间。「前日」也不明所以的。都要删掉。「红烛泪眼」是核心意象要留下来。\n\n想要留下来的是什么呢？遗忘肯定是要有的，这是我给出的解答。互相也是要有的，起源就是由防风火柴想到的尾生抱柱抛出的一问。「红烛泪眼」没法共轭，还是拆开好了。\n\n互相的话 就用镜子好了。破妄，吊诡，空间的拓展却又重复，自反中带有异质性。很好。\n\n「红烛在镜中总是泪眼」？当时还开心的把手机翻出来赶紧记下，记完想想又觉得不好 啰啰嗦嗦的。\n\n「红烛向镜中抛去泪眼」？我很喜欢这个动作带有的力量感，和伴随而来的主体性。用什么迎接你？以眼泪，以沉默。这是有力量的沉默。但是汉语还是半通不通的 忘记也没了。不好。\n\n「忘记」这个字其实很好，自反又偏义，但是口语中用得太多了 读来感觉不到妙处 不好。\n\n我想，烛火燃烧时上腾的青烟，蒸腾的雾气凝结成雨，落下化为沙尘，恰好就有一种复调式的演出效果。水汽也好，腾烟也好，怎么放在这里处理”忘“这个要素呢？想到了溶解，融化，但都用不好。这里卡了很久没想出来。\n\n灵光一闪，就用「淡忘」。「淡」字自己就好像是拿来给水墨化开的，要是到token级别就是又有水又有火的自反，不管是前句的湖中还是镜子都能超距作用。「淡忘」本来不是什么僻词，但放在这里就妙得没话说。\n\n「红烛淡忘镜中的泪眼」，真好。\n\n「我」「饮酒」，「烛」「忘眼」。好像比兴一样的氛围，又构成了复调的演奏。「水」与「镜」，「泪」与「眼」，几乎每一个元素都能够进行笛卡尔式呼应。比兴之中，阅读顺序的先后带来的时序性还为文本增添了并列以外的递进因素，自问自答。很好。我很喜欢。\n\n如果说昨天是向义山一样堆叠典故，这次就是处理意象了，也是很好玩呐。\n\n下面又试着加点东西。一方面是平衡语感。这两句佶屈聱牙，像极了祭祀用的七言律诗，但是要是能像冯君一样，神神叨叨念完“长剑归来乎，”，令人不容小觑，立马接上一句“食无鱼”产生节目效果。那就可以说是非常成功了。\n\n此外，也和我想要追求的吊诡氛围有点差异。水中饮酒还有镜子，好像是月光下的水晶湖一样，太明亮通透了些/\n\n加什么呢？四个字的好。「烟波浩渺」？我想到洞庭湖，蹭蹭湘君的隐喻正好在调上。「烟涛微茫」？直接偷过来好像也不坏。像是舞台布景的话，很明白的小舞台放在巨大的烟幕里，也是那个意思。\n\n不过怎么放怎么感觉不妙，况且下来也不知怎么接手。先这么放着好了。","source":"_posts/火柴.md","raw":"---\ntitle: 火柴\ndate: 2022-05-29 02:20:04\ntags:\n- Poem\n- Self\ncategories:\n- Metaphysics\n---\n我在[^6]水中桥下[^1] 饮酒[^2]/~~忘相泉涸[^4]前日的红烛[^5]泪眼[^3]~~红烛淡忘镜中的泪眼\n\n[^1]:「尾生與女子期於梁下，女子不來，水至不去，抱樑柱而死。」《莊子·盜跖》\n[^2]:  「夢飲酒者，旦而哭泣；夢哭泣者，旦而田獵。方其夢也，不知其夢也。」《莊子·齊物論》\n[^3]: 「文侯與虞人期獵。是日，飲酒樂，天雨。文侯將出，左右曰：「今日飲酒樂，天又雨，公將焉之？」文侯曰：「吾與虞人期獵，雖樂，豈可不一會期哉！」乃往，身自罷之。」《戰國策·魏策》\n[^4]: 「泉涸，魚相與處於陸，相呴以溼，相濡以沫，不如相忘於江湖。」《莊子·大宗師》\n\n[^5]: 「蠟炬成灰淚始幹」 李商隐\n[^6]: 火柴梗快要烧完的时候，会因为火焰过分接近而忍不住松手\n\n\n\n自己写完还解诗的人肯定是天下第一无聊 但这只不过是记录灵感媾和时的一些脉络和取舍。\n<!--more-->\n\n\n***\n\n2022/5/30日凌晨睡不着床上对后半句做了修正\n\n第二天起来看前天写得也太迫真了 简直就是笨蛋版本李商隐\n\n当时是怎么想的呢？「忘相」是什么表达？生怕别人看不出来是你直接从庄子里偷来的？「泉涸」同理，太白太直了 反而失去了解读空间。「前日」也不明所以的。都要删掉。「红烛泪眼」是核心意象要留下来。\n\n想要留下来的是什么呢？遗忘肯定是要有的，这是我给出的解答。互相也是要有的，起源就是由防风火柴想到的尾生抱柱抛出的一问。「红烛泪眼」没法共轭，还是拆开好了。\n\n互相的话 就用镜子好了。破妄，吊诡，空间的拓展却又重复，自反中带有异质性。很好。\n\n「红烛在镜中总是泪眼」？当时还开心的把手机翻出来赶紧记下，记完想想又觉得不好 啰啰嗦嗦的。\n\n「红烛向镜中抛去泪眼」？我很喜欢这个动作带有的力量感，和伴随而来的主体性。用什么迎接你？以眼泪，以沉默。这是有力量的沉默。但是汉语还是半通不通的 忘记也没了。不好。\n\n「忘记」这个字其实很好，自反又偏义，但是口语中用得太多了 读来感觉不到妙处 不好。\n\n我想，烛火燃烧时上腾的青烟，蒸腾的雾气凝结成雨，落下化为沙尘，恰好就有一种复调式的演出效果。水汽也好，腾烟也好，怎么放在这里处理”忘“这个要素呢？想到了溶解，融化，但都用不好。这里卡了很久没想出来。\n\n灵光一闪，就用「淡忘」。「淡」字自己就好像是拿来给水墨化开的，要是到token级别就是又有水又有火的自反，不管是前句的湖中还是镜子都能超距作用。「淡忘」本来不是什么僻词，但放在这里就妙得没话说。\n\n「红烛淡忘镜中的泪眼」，真好。\n\n「我」「饮酒」，「烛」「忘眼」。好像比兴一样的氛围，又构成了复调的演奏。「水」与「镜」，「泪」与「眼」，几乎每一个元素都能够进行笛卡尔式呼应。比兴之中，阅读顺序的先后带来的时序性还为文本增添了并列以外的递进因素，自问自答。很好。我很喜欢。\n\n如果说昨天是向义山一样堆叠典故，这次就是处理意象了，也是很好玩呐。\n\n下面又试着加点东西。一方面是平衡语感。这两句佶屈聱牙，像极了祭祀用的七言律诗，但是要是能像冯君一样，神神叨叨念完“长剑归来乎，”，令人不容小觑，立马接上一句“食无鱼”产生节目效果。那就可以说是非常成功了。\n\n此外，也和我想要追求的吊诡氛围有点差异。水中饮酒还有镜子，好像是月光下的水晶湖一样，太明亮通透了些/\n\n加什么呢？四个字的好。「烟波浩渺」？我想到洞庭湖，蹭蹭湘君的隐喻正好在调上。「烟涛微茫」？直接偷过来好像也不坏。像是舞台布景的话，很明白的小舞台放在巨大的烟幕里，也是那个意思。\n\n不过怎么放怎么感觉不妙，况且下来也不知怎么接手。先这么放着好了。","slug":"火柴","published":1,"updated":"2022-06-20T11:16:09.630Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl4y3s2zh000fqkw9eo9y357h","content":"<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css\"><p>我在<sup id=\"fnref:6\"><a href=\"#fn:6\" rel=\"footnote\"><span class=\"hint--top hint--error hint--medium hint--rounded hint--bounce\" aria-label=\"火柴梗快要烧完的时候，会因为火焰过分接近而忍不住松手\">6</span></a></sup>水中桥下<sup id=\"fnref:1\"><a href=\"#fn:1\" rel=\"footnote\"><span class=\"hint--top hint--error hint--medium hint--rounded hint--bounce\" aria-label=\"「尾生與女子期於梁下，女子不來，水至不去，抱樑柱而死。」《莊子·盜跖》\n\">1</span></a></sup> 饮酒<sup id=\"fnref:2\"><a href=\"#fn:2\" rel=\"footnote\"><span class=\"hint--top hint--error hint--medium hint--rounded hint--bounce\" aria-label=\" 「夢飲酒者，旦而哭泣；夢哭泣者，旦而田獵。方其夢也，不知其夢也。」《莊子·齊物論》\n\">2</span></a></sup>&#x2F;<del>忘相泉涸<sup id=\"fnref:4\"><a href=\"#fn:4\" rel=\"footnote\"><span class=\"hint--top hint--error hint--medium hint--rounded hint--bounce\" aria-label=\"「泉涸，魚相與處於陸，相呴以溼，相濡以沫，不如相忘於江湖。」《莊子·大宗師》\">4</span></a></sup>前日的红烛<sup id=\"fnref:5\"><a href=\"#fn:5\" rel=\"footnote\"><span class=\"hint--top hint--error hint--medium hint--rounded hint--bounce\" aria-label=\"「蠟炬成灰淚始幹」 李商隐\n\">5</span></a></sup>泪眼<sup id=\"fnref:3\"><a href=\"#fn:3\" rel=\"footnote\"><span class=\"hint--top hint--error hint--medium hint--rounded hint--bounce\" aria-label=\"「文侯與虞人期獵。是日，飲酒樂，天雨。文侯將出，左右曰：「今日飲酒樂，天又雨，公將焉之？」文侯曰：「吾與虞人期獵，雖樂，豈可不一會期哉！」乃往，身自罷之。」《戰國策·魏策》\n\">3</span></a></sup></del>红烛淡忘镜中的泪眼</p>\n<p>自己写完还解诗的人肯定是天下第一无聊 但这只不过是记录灵感媾和时的一些脉络和取舍。</p>\n<span id=\"more\"></span>\n\n\n<hr>\n<p>2022&#x2F;5&#x2F;30日凌晨睡不着床上对后半句做了修正</p>\n<p>第二天起来看前天写得也太迫真了 简直就是笨蛋版本李商隐</p>\n<p>当时是怎么想的呢？「忘相」是什么表达？生怕别人看不出来是你直接从庄子里偷来的？「泉涸」同理，太白太直了 反而失去了解读空间。「前日」也不明所以的。都要删掉。「红烛泪眼」是核心意象要留下来。</p>\n<p>想要留下来的是什么呢？遗忘肯定是要有的，这是我给出的解答。互相也是要有的，起源就是由防风火柴想到的尾生抱柱抛出的一问。「红烛泪眼」没法共轭，还是拆开好了。</p>\n<p>互相的话 就用镜子好了。破妄，吊诡，空间的拓展却又重复，自反中带有异质性。很好。</p>\n<p>「红烛在镜中总是泪眼」？当时还开心的把手机翻出来赶紧记下，记完想想又觉得不好 啰啰嗦嗦的。</p>\n<p>「红烛向镜中抛去泪眼」？我很喜欢这个动作带有的力量感，和伴随而来的主体性。用什么迎接你？以眼泪，以沉默。这是有力量的沉默。但是汉语还是半通不通的 忘记也没了。不好。</p>\n<p>「忘记」这个字其实很好，自反又偏义，但是口语中用得太多了 读来感觉不到妙处 不好。</p>\n<p>我想，烛火燃烧时上腾的青烟，蒸腾的雾气凝结成雨，落下化为沙尘，恰好就有一种复调式的演出效果。水汽也好，腾烟也好，怎么放在这里处理”忘“这个要素呢？想到了溶解，融化，但都用不好。这里卡了很久没想出来。</p>\n<p>灵光一闪，就用「淡忘」。「淡」字自己就好像是拿来给水墨化开的，要是到token级别就是又有水又有火的自反，不管是前句的湖中还是镜子都能超距作用。「淡忘」本来不是什么僻词，但放在这里就妙得没话说。</p>\n<p>「红烛淡忘镜中的泪眼」，真好。</p>\n<p>「我」「饮酒」，「烛」「忘眼」。好像比兴一样的氛围，又构成了复调的演奏。「水」与「镜」，「泪」与「眼」，几乎每一个元素都能够进行笛卡尔式呼应。比兴之中，阅读顺序的先后带来的时序性还为文本增添了并列以外的递进因素，自问自答。很好。我很喜欢。</p>\n<p>如果说昨天是向义山一样堆叠典故，这次就是处理意象了，也是很好玩呐。</p>\n<p>下面又试着加点东西。一方面是平衡语感。这两句佶屈聱牙，像极了祭祀用的七言律诗，但是要是能像冯君一样，神神叨叨念完“长剑归来乎，”，令人不容小觑，立马接上一句“食无鱼”产生节目效果。那就可以说是非常成功了。</p>\n<p>此外，也和我想要追求的吊诡氛围有点差异。水中饮酒还有镜子，好像是月光下的水晶湖一样，太明亮通透了些&#x2F;</p>\n<p>加什么呢？四个字的好。「烟波浩渺」？我想到洞庭湖，蹭蹭湘君的隐喻正好在调上。「烟涛微茫」？直接偷过来好像也不坏。像是舞台布景的话，很明白的小舞台放在巨大的烟幕里，也是那个意思。</p>\n<p>不过怎么放怎么感觉不妙，况且下来也不知怎么接手。先这么放着好了。<div id=\"footnotes\"><hr><div id=\"footnotelist\"><ol style=\"list-style: none; padding-left: 0; margin-left: 40px\"><li id=\"fn:1\"><span style=\"display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px\">1.</span><span style=\"display: inline-block; vertical-align: top; margin-left: 10px;\">「尾生與女子期於梁下，女子不來，水至不去，抱樑柱而死。」《莊子·盜跖》<a href=\"#fnref:1\" rev=\"footnote\">↩</a></span></li><li id=\"fn:2\"><span style=\"display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px\">2.</span><span style=\"display: inline-block; vertical-align: top; margin-left: 10px;\">「夢飲酒者，旦而哭泣；夢哭泣者，旦而田獵。方其夢也，不知其夢也。」《莊子·齊物論》<a href=\"#fnref:2\" rev=\"footnote\">↩</a></span></li><li id=\"fn:3\"><span style=\"display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px\">3.</span><span style=\"display: inline-block; vertical-align: top; margin-left: 10px;\">「文侯與虞人期獵。是日，飲酒樂，天雨。文侯將出，左右曰：「今日飲酒樂，天又雨，公將焉之？」文侯曰：「吾與虞人期獵，雖樂，豈可不一會期哉！」乃往，身自罷之。」《戰國策·魏策》<a href=\"#fnref:3\" rev=\"footnote\">↩</a></span></li><li id=\"fn:4\"><span style=\"display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px\">4.</span><span style=\"display: inline-block; vertical-align: top; margin-left: 10px;\">「泉涸，魚相與處於陸，相呴以溼，相濡以沫，不如相忘於江湖。」《莊子·大宗師》<a href=\"#fnref:4\" rev=\"footnote\">↩</a></span></li><li id=\"fn:5\"><span style=\"display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px\">5.</span><span style=\"display: inline-block; vertical-align: top; margin-left: 10px;\">「蠟炬成灰淚始幹」 李商隐<a href=\"#fnref:5\" rev=\"footnote\">↩</a></span></li><li id=\"fn:6\"><span style=\"display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px\">6.</span><span style=\"display: inline-block; vertical-align: top; margin-left: 10px;\">火柴梗快要烧完的时候，会因为火焰过分接近而忍不住松手<a href=\"#fnref:6\" rev=\"footnote\">↩</a></span></li></ol></div></div></p>\n","site":{"data":{}},"excerpt":"<p>我在<sup id=\"fnref:6\"><a href=\"#fn:6\" rel=\"footnote\"><span class=\"hint--top hint--error hint--medium hint--rounded hint--bounce\" aria-label=\"火柴梗快要烧完的时候，会因为火焰过分接近而忍不住松手\">6</span></a></sup>水中桥下<sup id=\"fnref:1\"><a href=\"#fn:1\" rel=\"footnote\"><span class=\"hint--top hint--error hint--medium hint--rounded hint--bounce\" aria-label=\"「尾生與女子期於梁下，女子不來，水至不去，抱樑柱而死。」《莊子·盜跖》\n\">1</span></a></sup> 饮酒<sup id=\"fnref:2\"><a href=\"#fn:2\" rel=\"footnote\"><span class=\"hint--top hint--error hint--medium hint--rounded hint--bounce\" aria-label=\" 「夢飲酒者，旦而哭泣；夢哭泣者，旦而田獵。方其夢也，不知其夢也。」《莊子·齊物論》\n\">2</span></a></sup>&#x2F;<del>忘相泉涸<sup id=\"fnref:4\"><a href=\"#fn:4\" rel=\"footnote\"><span class=\"hint--top hint--error hint--medium hint--rounded hint--bounce\" aria-label=\"「泉涸，魚相與處於陸，相呴以溼，相濡以沫，不如相忘於江湖。」《莊子·大宗師》\">4</span></a></sup>前日的红烛<sup id=\"fnref:5\"><a href=\"#fn:5\" rel=\"footnote\"><span class=\"hint--top hint--error hint--medium hint--rounded hint--bounce\" aria-label=\"「蠟炬成灰淚始幹」 李商隐\n\">5</span></a></sup>泪眼<sup id=\"fnref:3\"><a href=\"#fn:3\" rel=\"footnote\"><span class=\"hint--top hint--error hint--medium hint--rounded hint--bounce\" aria-label=\"「文侯與虞人期獵。是日，飲酒樂，天雨。文侯將出，左右曰：「今日飲酒樂，天又雨，公將焉之？」文侯曰：「吾與虞人期獵，雖樂，豈可不一會期哉！」乃往，身自罷之。」《戰國策·魏策》\n\">3</span></a></sup></del>红烛淡忘镜中的泪眼</p>\n<p>自己写完还解诗的人肯定是天下第一无聊 但这只不过是记录灵感媾和时的一些脉络和取舍。</p>","more":"<hr>\n<p>2022&#x2F;5&#x2F;30日凌晨睡不着床上对后半句做了修正</p>\n<p>第二天起来看前天写得也太迫真了 简直就是笨蛋版本李商隐</p>\n<p>当时是怎么想的呢？「忘相」是什么表达？生怕别人看不出来是你直接从庄子里偷来的？「泉涸」同理，太白太直了 反而失去了解读空间。「前日」也不明所以的。都要删掉。「红烛泪眼」是核心意象要留下来。</p>\n<p>想要留下来的是什么呢？遗忘肯定是要有的，这是我给出的解答。互相也是要有的，起源就是由防风火柴想到的尾生抱柱抛出的一问。「红烛泪眼」没法共轭，还是拆开好了。</p>\n<p>互相的话 就用镜子好了。破妄，吊诡，空间的拓展却又重复，自反中带有异质性。很好。</p>\n<p>「红烛在镜中总是泪眼」？当时还开心的把手机翻出来赶紧记下，记完想想又觉得不好 啰啰嗦嗦的。</p>\n<p>「红烛向镜中抛去泪眼」？我很喜欢这个动作带有的力量感，和伴随而来的主体性。用什么迎接你？以眼泪，以沉默。这是有力量的沉默。但是汉语还是半通不通的 忘记也没了。不好。</p>\n<p>「忘记」这个字其实很好，自反又偏义，但是口语中用得太多了 读来感觉不到妙处 不好。</p>\n<p>我想，烛火燃烧时上腾的青烟，蒸腾的雾气凝结成雨，落下化为沙尘，恰好就有一种复调式的演出效果。水汽也好，腾烟也好，怎么放在这里处理”忘“这个要素呢？想到了溶解，融化，但都用不好。这里卡了很久没想出来。</p>\n<p>灵光一闪，就用「淡忘」。「淡」字自己就好像是拿来给水墨化开的，要是到token级别就是又有水又有火的自反，不管是前句的湖中还是镜子都能超距作用。「淡忘」本来不是什么僻词，但放在这里就妙得没话说。</p>\n<p>「红烛淡忘镜中的泪眼」，真好。</p>\n<p>「我」「饮酒」，「烛」「忘眼」。好像比兴一样的氛围，又构成了复调的演奏。「水」与「镜」，「泪」与「眼」，几乎每一个元素都能够进行笛卡尔式呼应。比兴之中，阅读顺序的先后带来的时序性还为文本增添了并列以外的递进因素，自问自答。很好。我很喜欢。</p>\n<p>如果说昨天是向义山一样堆叠典故，这次就是处理意象了，也是很好玩呐。</p>\n<p>下面又试着加点东西。一方面是平衡语感。这两句佶屈聱牙，像极了祭祀用的七言律诗，但是要是能像冯君一样，神神叨叨念完“长剑归来乎，”，令人不容小觑，立马接上一句“食无鱼”产生节目效果。那就可以说是非常成功了。</p>\n<p>此外，也和我想要追求的吊诡氛围有点差异。水中饮酒还有镜子，好像是月光下的水晶湖一样，太明亮通透了些&#x2F;</p>\n<p>加什么呢？四个字的好。「烟波浩渺」？我想到洞庭湖，蹭蹭湘君的隐喻正好在调上。「烟涛微茫」？直接偷过来好像也不坏。像是舞台布景的话，很明白的小舞台放在巨大的烟幕里，也是那个意思。</p>\n<p>不过怎么放怎么感觉不妙，况且下来也不知怎么接手。先这么放着好了。<div id=\"footnotes\"><hr><div id=\"footnotelist\"><ol style=\"list-style: none; padding-left: 0; margin-left: 40px\"><li id=\"fn:1\"><span style=\"display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px\">1.</span><span style=\"display: inline-block; vertical-align: top; margin-left: 10px;\">「尾生與女子期於梁下，女子不來，水至不去，抱樑柱而死。」《莊子·盜跖》<a href=\"#fnref:1\" rev=\"footnote\">↩</a></span></li><li id=\"fn:2\"><span style=\"display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px\">2.</span><span style=\"display: inline-block; vertical-align: top; margin-left: 10px;\">「夢飲酒者，旦而哭泣；夢哭泣者，旦而田獵。方其夢也，不知其夢也。」《莊子·齊物論》<a href=\"#fnref:2\" rev=\"footnote\">↩</a></span></li><li id=\"fn:3\"><span style=\"display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px\">3.</span><span style=\"display: inline-block; vertical-align: top; margin-left: 10px;\">「文侯與虞人期獵。是日，飲酒樂，天雨。文侯將出，左右曰：「今日飲酒樂，天又雨，公將焉之？」文侯曰：「吾與虞人期獵，雖樂，豈可不一會期哉！」乃往，身自罷之。」《戰國策·魏策》<a href=\"#fnref:3\" rev=\"footnote\">↩</a></span></li><li id=\"fn:4\"><span style=\"display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px\">4.</span><span style=\"display: inline-block; vertical-align: top; margin-left: 10px;\">「泉涸，魚相與處於陸，相呴以溼，相濡以沫，不如相忘於江湖。」《莊子·大宗師》<a href=\"#fnref:4\" rev=\"footnote\">↩</a></span></li><li id=\"fn:5\"><span style=\"display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px\">5.</span><span style=\"display: inline-block; vertical-align: top; margin-left: 10px;\">「蠟炬成灰淚始幹」 李商隐<a href=\"#fnref:5\" rev=\"footnote\">↩</a></span></li><li id=\"fn:6\"><span style=\"display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px\">6.</span><span style=\"display: inline-block; vertical-align: top; margin-left: 10px;\">火柴梗快要烧完的时候，会因为火焰过分接近而忍不住松手<a href=\"#fnref:6\" rev=\"footnote\">↩</a></span></li></ol></div></div></p>"},{"layout":"post","title":"论文笔记 Automated Generation of Constraints From Use Case Specifications to Support System Testing","date":"2022-06-20T14:43:42.000Z","_content":"\n\n\n\n\n\n\n\n\n后记：\n\n这篇文章真没搞懂 看的时候觉得实在有点车轱辘话嗯说 前面说UMTG只要RUCM和OCL 后面说OCLgen的输入还需要类图……\n\n前面说verbnet里也不全是同义词 后面又说都是同义词\n\n前面说这个可以转换所有语言 后面就是简单推测了一下用较少规则可以转换好多 而现在还只实现了较少的较少……\n\n当然 也可能是我水平还不够看得颠三倒四 总之 就只是把他非常「作为手段地」分析了一通\n\n<!--more-->\n\n* 这篇文章要解决什么问题？\n  * 需要从自然语言的需求规范中自动生成可执行测试用例，使用UMTG工具\n  * UMTG需要的东西：RUCM的自然语言规范和OCL写的约束\n  * OCLgen就要生成OCL约束——主要是前后置条件\n\n* OCLgen的输入输出是什么？\n  * 输入：UMTG一致(?)——RUCM的NL和UML类图的系统领域模型\n  * 输出：每一个用例步骤对应的OCL约束\n\n\n\n![image-20220628194141482](论文笔记-Automated-Generation-of-Constraints-from-Use-Case-Specifications-to-Support-System-Testing/image-20220628194141482.png)\n\n\n\n<img src=\"论文笔记-Automated-Generation-of-Constraints-from-Use-Case-Specifications-to-Support-System-Testing/image-20220628194155803.png\" alt=\"image-20220628194155803\" style=\"zoom: 33%;\" />\n\n* 它厉害在哪里？\n  * 别人的方法 要使用CNL（受控的自然语言）来写软件要求，即，采用有限的动词\n  * 他的方法——需要UMTG格式的语言——但不限制动词，还需要类图\n  * 在测试的工业案例中，可以达到75%的正确\n  * 使用了语义标注和同义词合并的方法，使得只需要很少的规则就可以实现大范围的转换\n* 他的方法是什么？\n\n  * 先用SRL（语义角色标注）标记句子中词汇的角色\n  * 识别到之后 根据一定的规则再去识别类或自然语言中之中别的属性和操作符\n  * 还有通用的——元动词转换规则——因为目的是生成测试用例\n  * 然后（如果能生成多个）就打分评估最好的输出\n  * 打分方法：完整性和正确性——用例中角色在OCL中出现的比例，变量名和用例名一致(**这里完全没搞懂**)\n* 他的评估方法是什么？\n\n  * 比较了生成的和手动编写的 比较了正确率\n  * 对于需求 说了87个测试中生成除了多少\n  * \n* 他的不足在哪？\n  * 聚焦于前后置条件 主要测试场景就是这个工业案例 所以对输入有预设，对输出有范式\n  * 吹得很猛 不需要限制 可以处理所有 其实目前只实现了7类规则，可以处理408个动词\n  * 对语言的规格还是有需求 需要输入UML类图\n  * 实验很弱\n* 有什么启发？\n  * 比较的时候 应用场景 生成的灵活性都会比他好不少\n  * 对于方案设计启发不大\n\n\n\n目的：\n\n从自然语言的需求规范中自动生成系统测试用例\n\n——》\n\n为UMTG生成其需要的正则标准，提出了OCLGEN\n\n使用语义分析技术来识别用力规范的前后置条件\n\n可以75%正确生成前后置条件\n\n\n\n\n\n\n\n系统测试很重要 其测试用例要展示功能和安全需求，\n\n\n\n软件需求用NL写，然后由工程师手动转换，昂贵且易错\n\n现有的自动化解决方案依靠限制过的，简单的自然语言解决\n\n\n\n\n\n别人的方法（生成测试用例的）：\n\n基于特殊关键词侦测，如 if then=>抽象，高层次 给测试人员\n\n用受控的自然语言(CNL)写软件规格,再基于规则转换为正则标准=>可用语言非常有限\n\n不用CNL 但需要其他的建模工作——UMTG就是这样的\n\n\n\nOCLgen——捕获语句中的后置条件或前置条件——采用了文本转换规则，依赖自动语义分析技术，无需受控语言\n\nSRL（语义角色标注）实现词汇的标注，例如，收到动作最直接影响的成员就应该出现在后置条件中\n\n同义词识别，判断不同的词汇能否用相同的规则处理\n\n在测试的工业案例中，75%精度，25%由于精度不足\n\n\n\n\n\nUMTG: RUCM(用于写用例的一种语言格式 基本流替代流什么的)+OCL——测试用例\n\n\n\n语义标注：搞清楚短语的角色对前后置条件的生成是必要的\n\n别人搞的自然语言-用例生成不少用了语法识别 有一定用但搞不清短语作用\n\nSRL有许多种 但用了CNP是因为他是唯一一个还在积极开发的 也有接口\n\n\n\n同义检测：\n\nVerbNet不仅包括同义词类，还包括模式，如主语+不及物或主系表结构 \n\n使用了和PropBANK(CNP使用的）不同的模型，也会有不同的标签，但存在映射关系\n\n同一类中的词共用一种模式，帮助定义可重用的转换规则，但不是同一类中的都是同义词（？）用来识别同义词最先进的方法是WordNet\n\n\n\n\n\n# 方法\n\nUMTG需要人写ocl捕获这两种信息：\n\n* 行动对于系统状态的影响——后置条件\n* 用例的前置条件\n\nOCLgen就要自动化这一步骤\n\n需要的输入与UMTG一致：\n\n* RUCM写的用例规范\n* 以UML类图形式的 系统的领域模型\n\n可以输出每一个用例步骤对应的OCL约束\n\n\n\nOCLgen的方法：\n\n* 通过SRL\n  * 挑选要出现在约束中的元素，\n  * 决定要使用的比较运算符\n  * 额外的操作符，如否定\n* 针对每一个动词 使用不同转换规则转换——为了可行，使用VerbNet合并词类，需要的规则更少\n\n转换步骤：\n\n标记——选择规则——转换候选——挑选最高分\n\n![image-20220625173337030](论文笔记-Automated-Generation-of-Constraints-from-Use-Case-Specifications-to-Support-System-Testing/image-20220625173337030.png)\n\n可能有多个候选因为选择了多种规则\n\n最高分赋予使用了用例步中最多可用信息的?\n\n# OCL约束的格式\n\n一般就是比较笨蛋的 前置条件与条件步通常就是安全检查确保环境恰当，较容易捕获赋值，相等和不等关系\n\n![image-20220625181049650](论文笔记-Automated-Generation-of-Constraints-from-Use-Case-Specifications-to-Support-System-Testing/image-20220625181049650.png)\n\n是这样一套简化的EBNF 其中不少东西都是来自领域模型的，如类名和属性**\n\n# 转换规则\n\n\n\n所有转换规则共享相同的规则 每个规则和一组动词关联，如果一个动词出现，就执行一个步骤\n\n在第X部分会讨论规则对英语（动词）的覆盖性，在这一部分主要讨论动词 be set enable的规则\n\n\n\nSRL会识别出 左手边变量 left-hand side variable，操作符，选择元素和右手边变量right-hand side terms.\n\nA1一般就是lhs varible\n\n有两种转换规则：\n\n* specific verb transformation rule: 对每个动词定制的转换规则\n\n* META verb transformation rule: 对任何动词使用的转换规则，\n\n  这种规则基于这样一种常见的现象 ，就是语句的LHSvarible是一个名称与其动词匹配或相同的属性(后面还会介绍)\n\n\n\n# VI 识别要用到OCL左侧的变量\n\n真没搞懂在干嘛\n\n\n\n# VII 识别右侧的变量\n\n根据左侧变量的类型 支持角色来在输入的自然语言和模型中寻找类似的或可匹配到的\n\n\n\n# VIII 识别操作符\n\n用了别人的方法[35] 普遍都是类似于 be这样的动词\n\n对于 除了……都……这样的范式 发明了一套方法 也是和语义角色标记有关系的\n\n\n\n# IX 打分\n\n从完整性和正确性两个维度\n\n完整性：自然语言中所有概念被说明的程度有关。——用例中角色在OCL中出现的百分比\n\n正确性：OCL中的变量名和用例中的名字一致\n\n![image-20220625212958549](论文笔记-Automated-Generation-of-Constraints-from-Use-Case-Specifications-to-Support-System-Testing/image-20220625212958549.png)\n\n\n\n# X 完整性和普遍性\n\n为了让更多的动词可以用：\n\n* 使用Verbnet 让同一规则可以用于更多词（同一类的词）\n* 排除了形容人的感受/行为/动物行为的词\n\n经过分析，33个转换规则就可以给87类词转换 目前实现了7类规则 包括元转换规则。可以处理408个动词\n\n\n\n# XI 经验评估\n\nRQ1: 生成的OCL约束对吗？\n\nRQ2: 对于用例规范 oclgen的自动生成效果如何？\n\nRQ3: 限制生成效率的要素是什么？\n\n## RQ1: 生成的OCL约束对吗？\n\n比较了生成的和手动写的\n\n可能会： \n\n* 生成对的\n* 生成错的\n* 没有结果\n\n总正确率：66/69/87\n\n## RQ2: 对于用例规范 oclgen的自动生成效果如何？\n\n66/87\n\n## RQ3: 限制生成效率的要素是什么？\n\n手动检查没有生成的句子\n\n* 信息不足\n* 在用例规范和领域模型中表述不一样（is valid ——翻译不成——<>Error)\n\n\n\n# Threats to Validity\n\n普遍性：只试了这个工业案例 BodySensetM\n\n# 相关工作\n\n自动生成可执行测试案例需要需求规格用CNL（只有有限的动词）写\n\n\n\nOCLgen不需要受限的语言，而是RUCM，他引入了一些关键字，但并不限定使用的动名词\n\nNL2OCL \n\n处理UML类图和NL需求来得出类不变性和前后置条件，也用语义分析 来确定角色，靠侦测特定的关键字来确定被动语态或操作符\n\n缺点：\n\n   没有简化众多的动词怎么办——OCLgen的meta verb rule\n\n已经没法拿来比较了 \n\nNL2OCL更能抽取包括简单比较操作符的类不变量，而非生成前后置条件——这个对于测试用例生成更有用\n\n\n\n\n\n提点问题：\n\n* 如何评估生成的OCL的质量？\n\n\n\n* 在回答RQ时，使用了——产生了多少个OCL约束中多少个是正确的——没有给出“正确”的定义\n* 此外 还评估了一下87个需求多少个能生成出来\n* 在进行选择时，使用了打分机制，\n* \n\n\n\n* 输入输出是什么？\n\n输入包括两部分：\n\n* RUCM格式的自然语言撰写的用例步骤\n\n<img src=\"论文笔记-Automated-Generation-of-Constraints-from-Use-Case-Specifications-to-Support-System-Testing/image-20220626184416896.png\" alt=\"image-20220626184416896\"  />\n\n* 类图\n\n<img src=\"论文笔记-Automated-Generation-of-Constraints-from-Use-Case-Specifications-to-Support-System-Testing/image-20220626184653142.png\" alt=\"image-20220626184653142\" style=\"zoom: 25%;\" />\n\n输出似乎是有限的一种范式？\n\n<img src=\"论文笔记-Automated-Generation-of-Constraints-from-Use-Case-Specifications-to-Support-System-Testing/image-20220626184748233.png\" alt=\"image-20220626184748233\" style=\"zoom:25%;\" />\n\n\n\n\n\n* 创新点在哪？\n\n\n\n* 借鉴 比较在哪\n\n\n\n* 他做了什么事？方法是什么？\n","source":"_posts/论文笔记-Automated-Generation-of-Constraints-from-Use-Case-Specifications-to-Support-System-Testing.md","raw":"---\nlayout: post\ntitle: >-\n  论文笔记 Automated Generation of Constraints from Use Case Specifications to\n  Support System Testing\ndate: 2022-06-20 22:43:42\ntags:\n- Code generation\n- ICST\n- OCL\ncategories:\n- Substance\n---\n\n\n\n\n\n\n\n\n\n后记：\n\n这篇文章真没搞懂 看的时候觉得实在有点车轱辘话嗯说 前面说UMTG只要RUCM和OCL 后面说OCLgen的输入还需要类图……\n\n前面说verbnet里也不全是同义词 后面又说都是同义词\n\n前面说这个可以转换所有语言 后面就是简单推测了一下用较少规则可以转换好多 而现在还只实现了较少的较少……\n\n当然 也可能是我水平还不够看得颠三倒四 总之 就只是把他非常「作为手段地」分析了一通\n\n<!--more-->\n\n* 这篇文章要解决什么问题？\n  * 需要从自然语言的需求规范中自动生成可执行测试用例，使用UMTG工具\n  * UMTG需要的东西：RUCM的自然语言规范和OCL写的约束\n  * OCLgen就要生成OCL约束——主要是前后置条件\n\n* OCLgen的输入输出是什么？\n  * 输入：UMTG一致(?)——RUCM的NL和UML类图的系统领域模型\n  * 输出：每一个用例步骤对应的OCL约束\n\n\n\n![image-20220628194141482](论文笔记-Automated-Generation-of-Constraints-from-Use-Case-Specifications-to-Support-System-Testing/image-20220628194141482.png)\n\n\n\n<img src=\"论文笔记-Automated-Generation-of-Constraints-from-Use-Case-Specifications-to-Support-System-Testing/image-20220628194155803.png\" alt=\"image-20220628194155803\" style=\"zoom: 33%;\" />\n\n* 它厉害在哪里？\n  * 别人的方法 要使用CNL（受控的自然语言）来写软件要求，即，采用有限的动词\n  * 他的方法——需要UMTG格式的语言——但不限制动词，还需要类图\n  * 在测试的工业案例中，可以达到75%的正确\n  * 使用了语义标注和同义词合并的方法，使得只需要很少的规则就可以实现大范围的转换\n* 他的方法是什么？\n\n  * 先用SRL（语义角色标注）标记句子中词汇的角色\n  * 识别到之后 根据一定的规则再去识别类或自然语言中之中别的属性和操作符\n  * 还有通用的——元动词转换规则——因为目的是生成测试用例\n  * 然后（如果能生成多个）就打分评估最好的输出\n  * 打分方法：完整性和正确性——用例中角色在OCL中出现的比例，变量名和用例名一致(**这里完全没搞懂**)\n* 他的评估方法是什么？\n\n  * 比较了生成的和手动编写的 比较了正确率\n  * 对于需求 说了87个测试中生成除了多少\n  * \n* 他的不足在哪？\n  * 聚焦于前后置条件 主要测试场景就是这个工业案例 所以对输入有预设，对输出有范式\n  * 吹得很猛 不需要限制 可以处理所有 其实目前只实现了7类规则，可以处理408个动词\n  * 对语言的规格还是有需求 需要输入UML类图\n  * 实验很弱\n* 有什么启发？\n  * 比较的时候 应用场景 生成的灵活性都会比他好不少\n  * 对于方案设计启发不大\n\n\n\n目的：\n\n从自然语言的需求规范中自动生成系统测试用例\n\n——》\n\n为UMTG生成其需要的正则标准，提出了OCLGEN\n\n使用语义分析技术来识别用力规范的前后置条件\n\n可以75%正确生成前后置条件\n\n\n\n\n\n\n\n系统测试很重要 其测试用例要展示功能和安全需求，\n\n\n\n软件需求用NL写，然后由工程师手动转换，昂贵且易错\n\n现有的自动化解决方案依靠限制过的，简单的自然语言解决\n\n\n\n\n\n别人的方法（生成测试用例的）：\n\n基于特殊关键词侦测，如 if then=>抽象，高层次 给测试人员\n\n用受控的自然语言(CNL)写软件规格,再基于规则转换为正则标准=>可用语言非常有限\n\n不用CNL 但需要其他的建模工作——UMTG就是这样的\n\n\n\nOCLgen——捕获语句中的后置条件或前置条件——采用了文本转换规则，依赖自动语义分析技术，无需受控语言\n\nSRL（语义角色标注）实现词汇的标注，例如，收到动作最直接影响的成员就应该出现在后置条件中\n\n同义词识别，判断不同的词汇能否用相同的规则处理\n\n在测试的工业案例中，75%精度，25%由于精度不足\n\n\n\n\n\nUMTG: RUCM(用于写用例的一种语言格式 基本流替代流什么的)+OCL——测试用例\n\n\n\n语义标注：搞清楚短语的角色对前后置条件的生成是必要的\n\n别人搞的自然语言-用例生成不少用了语法识别 有一定用但搞不清短语作用\n\nSRL有许多种 但用了CNP是因为他是唯一一个还在积极开发的 也有接口\n\n\n\n同义检测：\n\nVerbNet不仅包括同义词类，还包括模式，如主语+不及物或主系表结构 \n\n使用了和PropBANK(CNP使用的）不同的模型，也会有不同的标签，但存在映射关系\n\n同一类中的词共用一种模式，帮助定义可重用的转换规则，但不是同一类中的都是同义词（？）用来识别同义词最先进的方法是WordNet\n\n\n\n\n\n# 方法\n\nUMTG需要人写ocl捕获这两种信息：\n\n* 行动对于系统状态的影响——后置条件\n* 用例的前置条件\n\nOCLgen就要自动化这一步骤\n\n需要的输入与UMTG一致：\n\n* RUCM写的用例规范\n* 以UML类图形式的 系统的领域模型\n\n可以输出每一个用例步骤对应的OCL约束\n\n\n\nOCLgen的方法：\n\n* 通过SRL\n  * 挑选要出现在约束中的元素，\n  * 决定要使用的比较运算符\n  * 额外的操作符，如否定\n* 针对每一个动词 使用不同转换规则转换——为了可行，使用VerbNet合并词类，需要的规则更少\n\n转换步骤：\n\n标记——选择规则——转换候选——挑选最高分\n\n![image-20220625173337030](论文笔记-Automated-Generation-of-Constraints-from-Use-Case-Specifications-to-Support-System-Testing/image-20220625173337030.png)\n\n可能有多个候选因为选择了多种规则\n\n最高分赋予使用了用例步中最多可用信息的?\n\n# OCL约束的格式\n\n一般就是比较笨蛋的 前置条件与条件步通常就是安全检查确保环境恰当，较容易捕获赋值，相等和不等关系\n\n![image-20220625181049650](论文笔记-Automated-Generation-of-Constraints-from-Use-Case-Specifications-to-Support-System-Testing/image-20220625181049650.png)\n\n是这样一套简化的EBNF 其中不少东西都是来自领域模型的，如类名和属性**\n\n# 转换规则\n\n\n\n所有转换规则共享相同的规则 每个规则和一组动词关联，如果一个动词出现，就执行一个步骤\n\n在第X部分会讨论规则对英语（动词）的覆盖性，在这一部分主要讨论动词 be set enable的规则\n\n\n\nSRL会识别出 左手边变量 left-hand side variable，操作符，选择元素和右手边变量right-hand side terms.\n\nA1一般就是lhs varible\n\n有两种转换规则：\n\n* specific verb transformation rule: 对每个动词定制的转换规则\n\n* META verb transformation rule: 对任何动词使用的转换规则，\n\n  这种规则基于这样一种常见的现象 ，就是语句的LHSvarible是一个名称与其动词匹配或相同的属性(后面还会介绍)\n\n\n\n# VI 识别要用到OCL左侧的变量\n\n真没搞懂在干嘛\n\n\n\n# VII 识别右侧的变量\n\n根据左侧变量的类型 支持角色来在输入的自然语言和模型中寻找类似的或可匹配到的\n\n\n\n# VIII 识别操作符\n\n用了别人的方法[35] 普遍都是类似于 be这样的动词\n\n对于 除了……都……这样的范式 发明了一套方法 也是和语义角色标记有关系的\n\n\n\n# IX 打分\n\n从完整性和正确性两个维度\n\n完整性：自然语言中所有概念被说明的程度有关。——用例中角色在OCL中出现的百分比\n\n正确性：OCL中的变量名和用例中的名字一致\n\n![image-20220625212958549](论文笔记-Automated-Generation-of-Constraints-from-Use-Case-Specifications-to-Support-System-Testing/image-20220625212958549.png)\n\n\n\n# X 完整性和普遍性\n\n为了让更多的动词可以用：\n\n* 使用Verbnet 让同一规则可以用于更多词（同一类的词）\n* 排除了形容人的感受/行为/动物行为的词\n\n经过分析，33个转换规则就可以给87类词转换 目前实现了7类规则 包括元转换规则。可以处理408个动词\n\n\n\n# XI 经验评估\n\nRQ1: 生成的OCL约束对吗？\n\nRQ2: 对于用例规范 oclgen的自动生成效果如何？\n\nRQ3: 限制生成效率的要素是什么？\n\n## RQ1: 生成的OCL约束对吗？\n\n比较了生成的和手动写的\n\n可能会： \n\n* 生成对的\n* 生成错的\n* 没有结果\n\n总正确率：66/69/87\n\n## RQ2: 对于用例规范 oclgen的自动生成效果如何？\n\n66/87\n\n## RQ3: 限制生成效率的要素是什么？\n\n手动检查没有生成的句子\n\n* 信息不足\n* 在用例规范和领域模型中表述不一样（is valid ——翻译不成——<>Error)\n\n\n\n# Threats to Validity\n\n普遍性：只试了这个工业案例 BodySensetM\n\n# 相关工作\n\n自动生成可执行测试案例需要需求规格用CNL（只有有限的动词）写\n\n\n\nOCLgen不需要受限的语言，而是RUCM，他引入了一些关键字，但并不限定使用的动名词\n\nNL2OCL \n\n处理UML类图和NL需求来得出类不变性和前后置条件，也用语义分析 来确定角色，靠侦测特定的关键字来确定被动语态或操作符\n\n缺点：\n\n   没有简化众多的动词怎么办——OCLgen的meta verb rule\n\n已经没法拿来比较了 \n\nNL2OCL更能抽取包括简单比较操作符的类不变量，而非生成前后置条件——这个对于测试用例生成更有用\n\n\n\n\n\n提点问题：\n\n* 如何评估生成的OCL的质量？\n\n\n\n* 在回答RQ时，使用了——产生了多少个OCL约束中多少个是正确的——没有给出“正确”的定义\n* 此外 还评估了一下87个需求多少个能生成出来\n* 在进行选择时，使用了打分机制，\n* \n\n\n\n* 输入输出是什么？\n\n输入包括两部分：\n\n* RUCM格式的自然语言撰写的用例步骤\n\n<img src=\"论文笔记-Automated-Generation-of-Constraints-from-Use-Case-Specifications-to-Support-System-Testing/image-20220626184416896.png\" alt=\"image-20220626184416896\"  />\n\n* 类图\n\n<img src=\"论文笔记-Automated-Generation-of-Constraints-from-Use-Case-Specifications-to-Support-System-Testing/image-20220626184653142.png\" alt=\"image-20220626184653142\" style=\"zoom: 25%;\" />\n\n输出似乎是有限的一种范式？\n\n<img src=\"论文笔记-Automated-Generation-of-Constraints-from-Use-Case-Specifications-to-Support-System-Testing/image-20220626184748233.png\" alt=\"image-20220626184748233\" style=\"zoom:25%;\" />\n\n\n\n\n\n* 创新点在哪？\n\n\n\n* 借鉴 比较在哪\n\n\n\n* 他做了什么事？方法是什么？\n","slug":"论文笔记-Automated-Generation-of-Constraints-from-Use-Case-Specifications-to-Support-System-Testing","published":1,"updated":"2022-06-28T11:45:20.079Z","comments":1,"photos":[],"link":"","_id":"cl4y3s2zi000gqkw971wmex5o","content":"<p>后记：</p>\n<p>这篇文章真没搞懂 看的时候觉得实在有点车轱辘话嗯说 前面说UMTG只要RUCM和OCL 后面说OCLgen的输入还需要类图……</p>\n<p>前面说verbnet里也不全是同义词 后面又说都是同义词</p>\n<p>前面说这个可以转换所有语言 后面就是简单推测了一下用较少规则可以转换好多 而现在还只实现了较少的较少……</p>\n<p>当然 也可能是我水平还不够看得颠三倒四 总之 就只是把他非常「作为手段地」分析了一通</p>\n<span id=\"more\"></span>\n\n<ul>\n<li><p>这篇文章要解决什么问题？</p>\n<ul>\n<li>需要从自然语言的需求规范中自动生成可执行测试用例，使用UMTG工具</li>\n<li>UMTG需要的东西：RUCM的自然语言规范和OCL写的约束</li>\n<li>OCLgen就要生成OCL约束——主要是前后置条件</li>\n</ul>\n</li>\n<li><p>OCLgen的输入输出是什么？</p>\n<ul>\n<li>输入：UMTG一致(?)——RUCM的NL和UML类图的系统领域模型</li>\n<li>输出：每一个用例步骤对应的OCL约束</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"/2022/06/20/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Automated-Generation-of-Constraints-from-Use-Case-Specifications-to-Support-System-Testing/image-20220628194141482.png\" alt=\"image-20220628194141482\"></p>\n<img src=\"/2022/06/20/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Automated-Generation-of-Constraints-from-Use-Case-Specifications-to-Support-System-Testing/image-20220628194155803.png\" alt=\"image-20220628194155803\" style=\"zoom: 33%;\">\n\n<ul>\n<li><p>它厉害在哪里？</p>\n<ul>\n<li>别人的方法 要使用CNL（受控的自然语言）来写软件要求，即，采用有限的动词</li>\n<li>他的方法——需要UMTG格式的语言——但不限制动词，还需要类图</li>\n<li>在测试的工业案例中，可以达到75%的正确</li>\n<li>使用了语义标注和同义词合并的方法，使得只需要很少的规则就可以实现大范围的转换</li>\n</ul>\n</li>\n<li><p>他的方法是什么？</p>\n<ul>\n<li>先用SRL（语义角色标注）标记句子中词汇的角色</li>\n<li>识别到之后 根据一定的规则再去识别类或自然语言中之中别的属性和操作符</li>\n<li>还有通用的——元动词转换规则——因为目的是生成测试用例</li>\n<li>然后（如果能生成多个）就打分评估最好的输出</li>\n<li>打分方法：完整性和正确性——用例中角色在OCL中出现的比例，变量名和用例名一致(<strong>这里完全没搞懂</strong>)</li>\n</ul>\n</li>\n<li><p>他的评估方法是什么？</p>\n<ul>\n<li>比较了生成的和手动编写的 比较了正确率</li>\n<li>对于需求 说了87个测试中生成除了多少</li>\n<li></li>\n</ul>\n</li>\n<li><p>他的不足在哪？</p>\n<ul>\n<li>聚焦于前后置条件 主要测试场景就是这个工业案例 所以对输入有预设，对输出有范式</li>\n<li>吹得很猛 不需要限制 可以处理所有 其实目前只实现了7类规则，可以处理408个动词</li>\n<li>对语言的规格还是有需求 需要输入UML类图</li>\n<li>实验很弱</li>\n</ul>\n</li>\n<li><p>有什么启发？</p>\n<ul>\n<li>比较的时候 应用场景 生成的灵活性都会比他好不少</li>\n<li>对于方案设计启发不大</li>\n</ul>\n</li>\n</ul>\n<p>目的：</p>\n<p>从自然语言的需求规范中自动生成系统测试用例</p>\n<p>——》</p>\n<p>为UMTG生成其需要的正则标准，提出了OCLGEN</p>\n<p>使用语义分析技术来识别用力规范的前后置条件</p>\n<p>可以75%正确生成前后置条件</p>\n<p>系统测试很重要 其测试用例要展示功能和安全需求，</p>\n<p>软件需求用NL写，然后由工程师手动转换，昂贵且易错</p>\n<p>现有的自动化解决方案依靠限制过的，简单的自然语言解决</p>\n<p>别人的方法（生成测试用例的）：</p>\n<p>基于特殊关键词侦测，如 if then&#x3D;&gt;抽象，高层次 给测试人员</p>\n<p>用受控的自然语言(CNL)写软件规格,再基于规则转换为正则标准&#x3D;&gt;可用语言非常有限</p>\n<p>不用CNL 但需要其他的建模工作——UMTG就是这样的</p>\n<p>OCLgen——捕获语句中的后置条件或前置条件——采用了文本转换规则，依赖自动语义分析技术，无需受控语言</p>\n<p>SRL（语义角色标注）实现词汇的标注，例如，收到动作最直接影响的成员就应该出现在后置条件中</p>\n<p>同义词识别，判断不同的词汇能否用相同的规则处理</p>\n<p>在测试的工业案例中，75%精度，25%由于精度不足</p>\n<p>UMTG: RUCM(用于写用例的一种语言格式 基本流替代流什么的)+OCL——测试用例</p>\n<p>语义标注：搞清楚短语的角色对前后置条件的生成是必要的</p>\n<p>别人搞的自然语言-用例生成不少用了语法识别 有一定用但搞不清短语作用</p>\n<p>SRL有许多种 但用了CNP是因为他是唯一一个还在积极开发的 也有接口</p>\n<p>同义检测：</p>\n<p>VerbNet不仅包括同义词类，还包括模式，如主语+不及物或主系表结构 </p>\n<p>使用了和PropBANK(CNP使用的）不同的模型，也会有不同的标签，但存在映射关系</p>\n<p>同一类中的词共用一种模式，帮助定义可重用的转换规则，但不是同一类中的都是同义词（？）用来识别同义词最先进的方法是WordNet</p>\n<h1 id=\"方法\"><a href=\"#方法\" class=\"headerlink\" title=\"方法\"></a>方法</h1><p>UMTG需要人写ocl捕获这两种信息：</p>\n<ul>\n<li>行动对于系统状态的影响——后置条件</li>\n<li>用例的前置条件</li>\n</ul>\n<p>OCLgen就要自动化这一步骤</p>\n<p>需要的输入与UMTG一致：</p>\n<ul>\n<li>RUCM写的用例规范</li>\n<li>以UML类图形式的 系统的领域模型</li>\n</ul>\n<p>可以输出每一个用例步骤对应的OCL约束</p>\n<p>OCLgen的方法：</p>\n<ul>\n<li>通过SRL<ul>\n<li>挑选要出现在约束中的元素，</li>\n<li>决定要使用的比较运算符</li>\n<li>额外的操作符，如否定</li>\n</ul>\n</li>\n<li>针对每一个动词 使用不同转换规则转换——为了可行，使用VerbNet合并词类，需要的规则更少</li>\n</ul>\n<p>转换步骤：</p>\n<p>标记——选择规则——转换候选——挑选最高分</p>\n<p><img src=\"/2022/06/20/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Automated-Generation-of-Constraints-from-Use-Case-Specifications-to-Support-System-Testing/image-20220625173337030.png\" alt=\"image-20220625173337030\"></p>\n<p>可能有多个候选因为选择了多种规则</p>\n<p>最高分赋予使用了用例步中最多可用信息的?</p>\n<h1 id=\"OCL约束的格式\"><a href=\"#OCL约束的格式\" class=\"headerlink\" title=\"OCL约束的格式\"></a>OCL约束的格式</h1><p>一般就是比较笨蛋的 前置条件与条件步通常就是安全检查确保环境恰当，较容易捕获赋值，相等和不等关系</p>\n<p><img src=\"/2022/06/20/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Automated-Generation-of-Constraints-from-Use-Case-Specifications-to-Support-System-Testing/image-20220625181049650.png\" alt=\"image-20220625181049650\"></p>\n<p>是这样一套简化的EBNF 其中不少东西都是来自领域模型的，如类名和属性**</p>\n<h1 id=\"转换规则\"><a href=\"#转换规则\" class=\"headerlink\" title=\"转换规则\"></a>转换规则</h1><p>所有转换规则共享相同的规则 每个规则和一组动词关联，如果一个动词出现，就执行一个步骤</p>\n<p>在第X部分会讨论规则对英语（动词）的覆盖性，在这一部分主要讨论动词 be set enable的规则</p>\n<p>SRL会识别出 左手边变量 left-hand side variable，操作符，选择元素和右手边变量right-hand side terms.</p>\n<p>A1一般就是lhs varible</p>\n<p>有两种转换规则：</p>\n<ul>\n<li><p>specific verb transformation rule: 对每个动词定制的转换规则</p>\n</li>\n<li><p>META verb transformation rule: 对任何动词使用的转换规则，</p>\n<p>这种规则基于这样一种常见的现象 ，就是语句的LHSvarible是一个名称与其动词匹配或相同的属性(后面还会介绍)</p>\n</li>\n</ul>\n<h1 id=\"VI-识别要用到OCL左侧的变量\"><a href=\"#VI-识别要用到OCL左侧的变量\" class=\"headerlink\" title=\"VI 识别要用到OCL左侧的变量\"></a>VI 识别要用到OCL左侧的变量</h1><p>真没搞懂在干嘛</p>\n<h1 id=\"VII-识别右侧的变量\"><a href=\"#VII-识别右侧的变量\" class=\"headerlink\" title=\"VII 识别右侧的变量\"></a>VII 识别右侧的变量</h1><p>根据左侧变量的类型 支持角色来在输入的自然语言和模型中寻找类似的或可匹配到的</p>\n<h1 id=\"VIII-识别操作符\"><a href=\"#VIII-识别操作符\" class=\"headerlink\" title=\"VIII 识别操作符\"></a>VIII 识别操作符</h1><p>用了别人的方法[35] 普遍都是类似于 be这样的动词</p>\n<p>对于 除了……都……这样的范式 发明了一套方法 也是和语义角色标记有关系的</p>\n<h1 id=\"IX-打分\"><a href=\"#IX-打分\" class=\"headerlink\" title=\"IX 打分\"></a>IX 打分</h1><p>从完整性和正确性两个维度</p>\n<p>完整性：自然语言中所有概念被说明的程度有关。——用例中角色在OCL中出现的百分比</p>\n<p>正确性：OCL中的变量名和用例中的名字一致</p>\n<p><img src=\"/2022/06/20/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Automated-Generation-of-Constraints-from-Use-Case-Specifications-to-Support-System-Testing/image-20220625212958549.png\" alt=\"image-20220625212958549\"></p>\n<h1 id=\"X-完整性和普遍性\"><a href=\"#X-完整性和普遍性\" class=\"headerlink\" title=\"X 完整性和普遍性\"></a>X 完整性和普遍性</h1><p>为了让更多的动词可以用：</p>\n<ul>\n<li>使用Verbnet 让同一规则可以用于更多词（同一类的词）</li>\n<li>排除了形容人的感受&#x2F;行为&#x2F;动物行为的词</li>\n</ul>\n<p>经过分析，33个转换规则就可以给87类词转换 目前实现了7类规则 包括元转换规则。可以处理408个动词</p>\n<h1 id=\"XI-经验评估\"><a href=\"#XI-经验评估\" class=\"headerlink\" title=\"XI 经验评估\"></a>XI 经验评估</h1><p>RQ1: 生成的OCL约束对吗？</p>\n<p>RQ2: 对于用例规范 oclgen的自动生成效果如何？</p>\n<p>RQ3: 限制生成效率的要素是什么？</p>\n<h2 id=\"RQ1-生成的OCL约束对吗？\"><a href=\"#RQ1-生成的OCL约束对吗？\" class=\"headerlink\" title=\"RQ1: 生成的OCL约束对吗？\"></a>RQ1: 生成的OCL约束对吗？</h2><p>比较了生成的和手动写的</p>\n<p>可能会： </p>\n<ul>\n<li>生成对的</li>\n<li>生成错的</li>\n<li>没有结果</li>\n</ul>\n<p>总正确率：66&#x2F;69&#x2F;87</p>\n<h2 id=\"RQ2-对于用例规范-oclgen的自动生成效果如何？\"><a href=\"#RQ2-对于用例规范-oclgen的自动生成效果如何？\" class=\"headerlink\" title=\"RQ2: 对于用例规范 oclgen的自动生成效果如何？\"></a>RQ2: 对于用例规范 oclgen的自动生成效果如何？</h2><p>66&#x2F;87</p>\n<h2 id=\"RQ3-限制生成效率的要素是什么？\"><a href=\"#RQ3-限制生成效率的要素是什么？\" class=\"headerlink\" title=\"RQ3: 限制生成效率的要素是什么？\"></a>RQ3: 限制生成效率的要素是什么？</h2><p>手动检查没有生成的句子</p>\n<ul>\n<li>信息不足</li>\n<li>在用例规范和领域模型中表述不一样（is valid ——翻译不成——&lt;&gt;Error)</li>\n</ul>\n<h1 id=\"Threats-to-Validity\"><a href=\"#Threats-to-Validity\" class=\"headerlink\" title=\"Threats to Validity\"></a>Threats to Validity</h1><p>普遍性：只试了这个工业案例 BodySensetM</p>\n<h1 id=\"相关工作\"><a href=\"#相关工作\" class=\"headerlink\" title=\"相关工作\"></a>相关工作</h1><p>自动生成可执行测试案例需要需求规格用CNL（只有有限的动词）写</p>\n<p>OCLgen不需要受限的语言，而是RUCM，他引入了一些关键字，但并不限定使用的动名词</p>\n<p>NL2OCL </p>\n<p>处理UML类图和NL需求来得出类不变性和前后置条件，也用语义分析 来确定角色，靠侦测特定的关键字来确定被动语态或操作符</p>\n<p>缺点：</p>\n<p>   没有简化众多的动词怎么办——OCLgen的meta verb rule</p>\n<p>已经没法拿来比较了 </p>\n<p>NL2OCL更能抽取包括简单比较操作符的类不变量，而非生成前后置条件——这个对于测试用例生成更有用</p>\n<p>提点问题：</p>\n<ul>\n<li><p>如何评估生成的OCL的质量？</p>\n</li>\n<li><p>在回答RQ时，使用了——产生了多少个OCL约束中多少个是正确的——没有给出“正确”的定义</p>\n</li>\n<li><p>此外 还评估了一下87个需求多少个能生成出来</p>\n</li>\n<li><p>在进行选择时，使用了打分机制，</p>\n</li>\n<li></li>\n<li><p>输入输出是什么？</p>\n</li>\n</ul>\n<p>输入包括两部分：</p>\n<ul>\n<li>RUCM格式的自然语言撰写的用例步骤</li>\n</ul>\n<img src=\"/2022/06/20/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Automated-Generation-of-Constraints-from-Use-Case-Specifications-to-Support-System-Testing/image-20220626184416896.png\" alt=\"image-20220626184416896\">\n\n<ul>\n<li>类图</li>\n</ul>\n<img src=\"/2022/06/20/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Automated-Generation-of-Constraints-from-Use-Case-Specifications-to-Support-System-Testing/image-20220626184653142.png\" alt=\"image-20220626184653142\" style=\"zoom: 25%;\">\n\n<p>输出似乎是有限的一种范式？</p>\n<img src=\"/2022/06/20/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Automated-Generation-of-Constraints-from-Use-Case-Specifications-to-Support-System-Testing/image-20220626184748233.png\" alt=\"image-20220626184748233\" style=\"zoom:25%;\">\n\n\n\n\n\n<ul>\n<li><p>创新点在哪？</p>\n</li>\n<li><p>借鉴 比较在哪</p>\n</li>\n<li><p>他做了什么事？方法是什么？</p>\n</li>\n</ul>\n","site":{"data":{}},"excerpt":"<p>后记：</p>\n<p>这篇文章真没搞懂 看的时候觉得实在有点车轱辘话嗯说 前面说UMTG只要RUCM和OCL 后面说OCLgen的输入还需要类图……</p>\n<p>前面说verbnet里也不全是同义词 后面又说都是同义词</p>\n<p>前面说这个可以转换所有语言 后面就是简单推测了一下用较少规则可以转换好多 而现在还只实现了较少的较少……</p>\n<p>当然 也可能是我水平还不够看得颠三倒四 总之 就只是把他非常「作为手段地」分析了一通</p>","more":"<ul>\n<li><p>这篇文章要解决什么问题？</p>\n<ul>\n<li>需要从自然语言的需求规范中自动生成可执行测试用例，使用UMTG工具</li>\n<li>UMTG需要的东西：RUCM的自然语言规范和OCL写的约束</li>\n<li>OCLgen就要生成OCL约束——主要是前后置条件</li>\n</ul>\n</li>\n<li><p>OCLgen的输入输出是什么？</p>\n<ul>\n<li>输入：UMTG一致(?)——RUCM的NL和UML类图的系统领域模型</li>\n<li>输出：每一个用例步骤对应的OCL约束</li>\n</ul>\n</li>\n</ul>\n<p><img src=\"/2022/06/20/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Automated-Generation-of-Constraints-from-Use-Case-Specifications-to-Support-System-Testing/image-20220628194141482.png\" alt=\"image-20220628194141482\"></p>\n<img src=\"/2022/06/20/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Automated-Generation-of-Constraints-from-Use-Case-Specifications-to-Support-System-Testing/image-20220628194155803.png\" alt=\"image-20220628194155803\" style=\"zoom: 33%;\">\n\n<ul>\n<li><p>它厉害在哪里？</p>\n<ul>\n<li>别人的方法 要使用CNL（受控的自然语言）来写软件要求，即，采用有限的动词</li>\n<li>他的方法——需要UMTG格式的语言——但不限制动词，还需要类图</li>\n<li>在测试的工业案例中，可以达到75%的正确</li>\n<li>使用了语义标注和同义词合并的方法，使得只需要很少的规则就可以实现大范围的转换</li>\n</ul>\n</li>\n<li><p>他的方法是什么？</p>\n<ul>\n<li>先用SRL（语义角色标注）标记句子中词汇的角色</li>\n<li>识别到之后 根据一定的规则再去识别类或自然语言中之中别的属性和操作符</li>\n<li>还有通用的——元动词转换规则——因为目的是生成测试用例</li>\n<li>然后（如果能生成多个）就打分评估最好的输出</li>\n<li>打分方法：完整性和正确性——用例中角色在OCL中出现的比例，变量名和用例名一致(<strong>这里完全没搞懂</strong>)</li>\n</ul>\n</li>\n<li><p>他的评估方法是什么？</p>\n<ul>\n<li>比较了生成的和手动编写的 比较了正确率</li>\n<li>对于需求 说了87个测试中生成除了多少</li>\n<li></li>\n</ul>\n</li>\n<li><p>他的不足在哪？</p>\n<ul>\n<li>聚焦于前后置条件 主要测试场景就是这个工业案例 所以对输入有预设，对输出有范式</li>\n<li>吹得很猛 不需要限制 可以处理所有 其实目前只实现了7类规则，可以处理408个动词</li>\n<li>对语言的规格还是有需求 需要输入UML类图</li>\n<li>实验很弱</li>\n</ul>\n</li>\n<li><p>有什么启发？</p>\n<ul>\n<li>比较的时候 应用场景 生成的灵活性都会比他好不少</li>\n<li>对于方案设计启发不大</li>\n</ul>\n</li>\n</ul>\n<p>目的：</p>\n<p>从自然语言的需求规范中自动生成系统测试用例</p>\n<p>——》</p>\n<p>为UMTG生成其需要的正则标准，提出了OCLGEN</p>\n<p>使用语义分析技术来识别用力规范的前后置条件</p>\n<p>可以75%正确生成前后置条件</p>\n<p>系统测试很重要 其测试用例要展示功能和安全需求，</p>\n<p>软件需求用NL写，然后由工程师手动转换，昂贵且易错</p>\n<p>现有的自动化解决方案依靠限制过的，简单的自然语言解决</p>\n<p>别人的方法（生成测试用例的）：</p>\n<p>基于特殊关键词侦测，如 if then&#x3D;&gt;抽象，高层次 给测试人员</p>\n<p>用受控的自然语言(CNL)写软件规格,再基于规则转换为正则标准&#x3D;&gt;可用语言非常有限</p>\n<p>不用CNL 但需要其他的建模工作——UMTG就是这样的</p>\n<p>OCLgen——捕获语句中的后置条件或前置条件——采用了文本转换规则，依赖自动语义分析技术，无需受控语言</p>\n<p>SRL（语义角色标注）实现词汇的标注，例如，收到动作最直接影响的成员就应该出现在后置条件中</p>\n<p>同义词识别，判断不同的词汇能否用相同的规则处理</p>\n<p>在测试的工业案例中，75%精度，25%由于精度不足</p>\n<p>UMTG: RUCM(用于写用例的一种语言格式 基本流替代流什么的)+OCL——测试用例</p>\n<p>语义标注：搞清楚短语的角色对前后置条件的生成是必要的</p>\n<p>别人搞的自然语言-用例生成不少用了语法识别 有一定用但搞不清短语作用</p>\n<p>SRL有许多种 但用了CNP是因为他是唯一一个还在积极开发的 也有接口</p>\n<p>同义检测：</p>\n<p>VerbNet不仅包括同义词类，还包括模式，如主语+不及物或主系表结构 </p>\n<p>使用了和PropBANK(CNP使用的）不同的模型，也会有不同的标签，但存在映射关系</p>\n<p>同一类中的词共用一种模式，帮助定义可重用的转换规则，但不是同一类中的都是同义词（？）用来识别同义词最先进的方法是WordNet</p>\n<h1 id=\"方法\"><a href=\"#方法\" class=\"headerlink\" title=\"方法\"></a>方法</h1><p>UMTG需要人写ocl捕获这两种信息：</p>\n<ul>\n<li>行动对于系统状态的影响——后置条件</li>\n<li>用例的前置条件</li>\n</ul>\n<p>OCLgen就要自动化这一步骤</p>\n<p>需要的输入与UMTG一致：</p>\n<ul>\n<li>RUCM写的用例规范</li>\n<li>以UML类图形式的 系统的领域模型</li>\n</ul>\n<p>可以输出每一个用例步骤对应的OCL约束</p>\n<p>OCLgen的方法：</p>\n<ul>\n<li>通过SRL<ul>\n<li>挑选要出现在约束中的元素，</li>\n<li>决定要使用的比较运算符</li>\n<li>额外的操作符，如否定</li>\n</ul>\n</li>\n<li>针对每一个动词 使用不同转换规则转换——为了可行，使用VerbNet合并词类，需要的规则更少</li>\n</ul>\n<p>转换步骤：</p>\n<p>标记——选择规则——转换候选——挑选最高分</p>\n<p><img src=\"/2022/06/20/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Automated-Generation-of-Constraints-from-Use-Case-Specifications-to-Support-System-Testing/image-20220625173337030.png\" alt=\"image-20220625173337030\"></p>\n<p>可能有多个候选因为选择了多种规则</p>\n<p>最高分赋予使用了用例步中最多可用信息的?</p>\n<h1 id=\"OCL约束的格式\"><a href=\"#OCL约束的格式\" class=\"headerlink\" title=\"OCL约束的格式\"></a>OCL约束的格式</h1><p>一般就是比较笨蛋的 前置条件与条件步通常就是安全检查确保环境恰当，较容易捕获赋值，相等和不等关系</p>\n<p><img src=\"/2022/06/20/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Automated-Generation-of-Constraints-from-Use-Case-Specifications-to-Support-System-Testing/image-20220625181049650.png\" alt=\"image-20220625181049650\"></p>\n<p>是这样一套简化的EBNF 其中不少东西都是来自领域模型的，如类名和属性**</p>\n<h1 id=\"转换规则\"><a href=\"#转换规则\" class=\"headerlink\" title=\"转换规则\"></a>转换规则</h1><p>所有转换规则共享相同的规则 每个规则和一组动词关联，如果一个动词出现，就执行一个步骤</p>\n<p>在第X部分会讨论规则对英语（动词）的覆盖性，在这一部分主要讨论动词 be set enable的规则</p>\n<p>SRL会识别出 左手边变量 left-hand side variable，操作符，选择元素和右手边变量right-hand side terms.</p>\n<p>A1一般就是lhs varible</p>\n<p>有两种转换规则：</p>\n<ul>\n<li><p>specific verb transformation rule: 对每个动词定制的转换规则</p>\n</li>\n<li><p>META verb transformation rule: 对任何动词使用的转换规则，</p>\n<p>这种规则基于这样一种常见的现象 ，就是语句的LHSvarible是一个名称与其动词匹配或相同的属性(后面还会介绍)</p>\n</li>\n</ul>\n<h1 id=\"VI-识别要用到OCL左侧的变量\"><a href=\"#VI-识别要用到OCL左侧的变量\" class=\"headerlink\" title=\"VI 识别要用到OCL左侧的变量\"></a>VI 识别要用到OCL左侧的变量</h1><p>真没搞懂在干嘛</p>\n<h1 id=\"VII-识别右侧的变量\"><a href=\"#VII-识别右侧的变量\" class=\"headerlink\" title=\"VII 识别右侧的变量\"></a>VII 识别右侧的变量</h1><p>根据左侧变量的类型 支持角色来在输入的自然语言和模型中寻找类似的或可匹配到的</p>\n<h1 id=\"VIII-识别操作符\"><a href=\"#VIII-识别操作符\" class=\"headerlink\" title=\"VIII 识别操作符\"></a>VIII 识别操作符</h1><p>用了别人的方法[35] 普遍都是类似于 be这样的动词</p>\n<p>对于 除了……都……这样的范式 发明了一套方法 也是和语义角色标记有关系的</p>\n<h1 id=\"IX-打分\"><a href=\"#IX-打分\" class=\"headerlink\" title=\"IX 打分\"></a>IX 打分</h1><p>从完整性和正确性两个维度</p>\n<p>完整性：自然语言中所有概念被说明的程度有关。——用例中角色在OCL中出现的百分比</p>\n<p>正确性：OCL中的变量名和用例中的名字一致</p>\n<p><img src=\"/2022/06/20/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Automated-Generation-of-Constraints-from-Use-Case-Specifications-to-Support-System-Testing/image-20220625212958549.png\" alt=\"image-20220625212958549\"></p>\n<h1 id=\"X-完整性和普遍性\"><a href=\"#X-完整性和普遍性\" class=\"headerlink\" title=\"X 完整性和普遍性\"></a>X 完整性和普遍性</h1><p>为了让更多的动词可以用：</p>\n<ul>\n<li>使用Verbnet 让同一规则可以用于更多词（同一类的词）</li>\n<li>排除了形容人的感受&#x2F;行为&#x2F;动物行为的词</li>\n</ul>\n<p>经过分析，33个转换规则就可以给87类词转换 目前实现了7类规则 包括元转换规则。可以处理408个动词</p>\n<h1 id=\"XI-经验评估\"><a href=\"#XI-经验评估\" class=\"headerlink\" title=\"XI 经验评估\"></a>XI 经验评估</h1><p>RQ1: 生成的OCL约束对吗？</p>\n<p>RQ2: 对于用例规范 oclgen的自动生成效果如何？</p>\n<p>RQ3: 限制生成效率的要素是什么？</p>\n<h2 id=\"RQ1-生成的OCL约束对吗？\"><a href=\"#RQ1-生成的OCL约束对吗？\" class=\"headerlink\" title=\"RQ1: 生成的OCL约束对吗？\"></a>RQ1: 生成的OCL约束对吗？</h2><p>比较了生成的和手动写的</p>\n<p>可能会： </p>\n<ul>\n<li>生成对的</li>\n<li>生成错的</li>\n<li>没有结果</li>\n</ul>\n<p>总正确率：66&#x2F;69&#x2F;87</p>\n<h2 id=\"RQ2-对于用例规范-oclgen的自动生成效果如何？\"><a href=\"#RQ2-对于用例规范-oclgen的自动生成效果如何？\" class=\"headerlink\" title=\"RQ2: 对于用例规范 oclgen的自动生成效果如何？\"></a>RQ2: 对于用例规范 oclgen的自动生成效果如何？</h2><p>66&#x2F;87</p>\n<h2 id=\"RQ3-限制生成效率的要素是什么？\"><a href=\"#RQ3-限制生成效率的要素是什么？\" class=\"headerlink\" title=\"RQ3: 限制生成效率的要素是什么？\"></a>RQ3: 限制生成效率的要素是什么？</h2><p>手动检查没有生成的句子</p>\n<ul>\n<li>信息不足</li>\n<li>在用例规范和领域模型中表述不一样（is valid ——翻译不成——&lt;&gt;Error)</li>\n</ul>\n<h1 id=\"Threats-to-Validity\"><a href=\"#Threats-to-Validity\" class=\"headerlink\" title=\"Threats to Validity\"></a>Threats to Validity</h1><p>普遍性：只试了这个工业案例 BodySensetM</p>\n<h1 id=\"相关工作\"><a href=\"#相关工作\" class=\"headerlink\" title=\"相关工作\"></a>相关工作</h1><p>自动生成可执行测试案例需要需求规格用CNL（只有有限的动词）写</p>\n<p>OCLgen不需要受限的语言，而是RUCM，他引入了一些关键字，但并不限定使用的动名词</p>\n<p>NL2OCL </p>\n<p>处理UML类图和NL需求来得出类不变性和前后置条件，也用语义分析 来确定角色，靠侦测特定的关键字来确定被动语态或操作符</p>\n<p>缺点：</p>\n<p>   没有简化众多的动词怎么办——OCLgen的meta verb rule</p>\n<p>已经没法拿来比较了 </p>\n<p>NL2OCL更能抽取包括简单比较操作符的类不变量，而非生成前后置条件——这个对于测试用例生成更有用</p>\n<p>提点问题：</p>\n<ul>\n<li><p>如何评估生成的OCL的质量？</p>\n</li>\n<li><p>在回答RQ时，使用了——产生了多少个OCL约束中多少个是正确的——没有给出“正确”的定义</p>\n</li>\n<li><p>此外 还评估了一下87个需求多少个能生成出来</p>\n</li>\n<li><p>在进行选择时，使用了打分机制，</p>\n</li>\n<li></li>\n<li><p>输入输出是什么？</p>\n</li>\n</ul>\n<p>输入包括两部分：</p>\n<ul>\n<li>RUCM格式的自然语言撰写的用例步骤</li>\n</ul>\n<img src=\"/2022/06/20/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Automated-Generation-of-Constraints-from-Use-Case-Specifications-to-Support-System-Testing/image-20220626184416896.png\" alt=\"image-20220626184416896\">\n\n<ul>\n<li>类图</li>\n</ul>\n<img src=\"/2022/06/20/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Automated-Generation-of-Constraints-from-Use-Case-Specifications-to-Support-System-Testing/image-20220626184653142.png\" alt=\"image-20220626184653142\" style=\"zoom: 25%;\">\n\n<p>输出似乎是有限的一种范式？</p>\n<img src=\"/2022/06/20/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0-Automated-Generation-of-Constraints-from-Use-Case-Specifications-to-Support-System-Testing/image-20220626184748233.png\" alt=\"image-20220626184748233\" style=\"zoom:25%;\">\n\n\n\n\n\n<ul>\n<li><p>创新点在哪？</p>\n</li>\n<li><p>借鉴 比较在哪</p>\n</li>\n<li><p>他做了什么事？方法是什么？</p>\n</li>\n</ul>"}],"PostAsset":[{"_id":"source/_posts/SPT-Code/image-20220527230530594.png","post":"cl4y3s2ze000aqkw911pd3zf8","slug":"image-20220527230530594.png","modified":1,"renderable":1},{"_id":"source/_posts/SPT-Code/image-20220527231806850.png","post":"cl4y3s2ze000aqkw911pd3zf8","slug":"image-20220527231806850.png","modified":1,"renderable":1},{"_id":"source/_posts/SPT-Code/image-20220527232629535.png","post":"cl4y3s2ze000aqkw911pd3zf8","slug":"image-20220527232629535.png","modified":1,"renderable":1},{"_id":"source/_posts/SPT-Code/image-20220527233143119.png","post":"cl4y3s2ze000aqkw911pd3zf8","slug":"image-20220527233143119.png","modified":1,"renderable":1},{"_id":"source/_posts/SPT-Code/image-20220527233752649.png","post":"cl4y3s2ze000aqkw911pd3zf8","slug":"image-20220527233752649.png","modified":1,"renderable":1},{"_id":"source/_posts/testpic/image-20220523211852621.png","post":"cl4y3s2zg000eqkw9hpsm6jes","slug":"image-20220523211852621.png","modified":1,"renderable":1},{"_id":"source/_posts/testpic/image-20220524021029166.png","post":"cl4y3s2zg000eqkw9hpsm6jes","slug":"image-20220524021029166.png","modified":1,"renderable":1},{"_id":"source/_posts/论文笔记-Automated-Generation-of-Constraints-from-Use-Case-Specifications-to-Support-System-Testing/image-20220625173337030.png","post":"cl4y3s2zi000gqkw971wmex5o","slug":"image-20220625173337030.png","modified":1,"renderable":1},{"_id":"source/_posts/论文笔记-Automated-Generation-of-Constraints-from-Use-Case-Specifications-to-Support-System-Testing/image-20220625181049650.png","post":"cl4y3s2zi000gqkw971wmex5o","slug":"image-20220625181049650.png","modified":1,"renderable":1},{"_id":"source/_posts/论文笔记-Automated-Generation-of-Constraints-from-Use-Case-Specifications-to-Support-System-Testing/image-20220625212958549.png","post":"cl4y3s2zi000gqkw971wmex5o","slug":"image-20220625212958549.png","modified":1,"renderable":1},{"_id":"source/_posts/论文笔记-Automated-Generation-of-Constraints-from-Use-Case-Specifications-to-Support-System-Testing/image-20220626184413924.png","post":"cl4y3s2zi000gqkw971wmex5o","slug":"image-20220626184413924.png","modified":1,"renderable":1},{"_id":"source/_posts/论文笔记-Automated-Generation-of-Constraints-from-Use-Case-Specifications-to-Support-System-Testing/image-20220626184416896.png","post":"cl4y3s2zi000gqkw971wmex5o","slug":"image-20220626184416896.png","modified":1,"renderable":1},{"_id":"source/_posts/论文笔记-Automated-Generation-of-Constraints-from-Use-Case-Specifications-to-Support-System-Testing/image-20220626184653142.png","post":"cl4y3s2zi000gqkw971wmex5o","slug":"image-20220626184653142.png","modified":1,"renderable":1},{"_id":"source/_posts/论文笔记-Automated-Generation-of-Constraints-from-Use-Case-Specifications-to-Support-System-Testing/image-20220626184748233.png","post":"cl4y3s2zi000gqkw971wmex5o","slug":"image-20220626184748233.png","modified":1,"renderable":1},{"_id":"source/_posts/论文笔记-Automated-Generation-of-Constraints-from-Use-Case-Specifications-to-Support-System-Testing/image-20220628194141482.png","post":"cl4y3s2zi000gqkw971wmex5o","slug":"image-20220628194141482.png","modified":1,"renderable":1},{"_id":"source/_posts/论文笔记-Automated-Generation-of-Constraints-from-Use-Case-Specifications-to-Support-System-Testing/image-20220628194155803.png","post":"cl4y3s2zi000gqkw971wmex5o","slug":"image-20220628194155803.png","modified":1,"renderable":1}],"PostCategory":[{"post_id":"cl4y3s2yw0001qkw95hc53xj8","category_id":"cl4y3s2z00003qkw90fua9l4k","_id":"cl4y3s2z20007qkw9es4f55b5"},{"post_id":"cl4y3s2zh000fqkw9eo9y357h","category_id":"cl4y3s2z00003qkw90fua9l4k","_id":"cl4y3s2zk000jqkw94hlne2ui"},{"post_id":"cl4y3s2ze000aqkw911pd3zf8","category_id":"cl4y3s2zg000cqkw9bbe45ngv","_id":"cl4y3s2zl000mqkw99nsn71ll"},{"post_id":"cl4y3s2zi000gqkw971wmex5o","category_id":"cl4y3s2zg000cqkw9bbe45ngv","_id":"cl4y3s2zl000nqkw9ea0pebtx"},{"post_id":"cl4y3s2zf000bqkw9ci6q1fiv","category_id":"cl4y3s2zj000hqkw951wrguc4","_id":"cl4y3s2zl000pqkw90c296ih4"},{"post_id":"cl4y3s2zg000eqkw9hpsm6jes","category_id":"cl4y3s2zk000kqkw93tw2d0pi","_id":"cl4y3s2zl000sqkw929w6buhn"}],"PostTag":[{"post_id":"cl4y3s2yw0001qkw95hc53xj8","tag_id":"cl4y3s2z10004qkw9f4njbjiz","_id":"cl4y3s2z30008qkw9fgbs9b4f"},{"post_id":"cl4y3s2yw0001qkw95hc53xj8","tag_id":"cl4y3s2z20006qkw9axaf0zd8","_id":"cl4y3s2z30009qkw93ox35ygj"},{"post_id":"cl4y3s2ze000aqkw911pd3zf8","tag_id":"cl4y3s2zg000dqkw95o0dgxzj","_id":"cl4y3s2zl000qqkw94rfe68za"},{"post_id":"cl4y3s2ze000aqkw911pd3zf8","tag_id":"cl4y3s2zj000iqkw90ffyb0ii","_id":"cl4y3s2zl000rqkw93bbra8wm"},{"post_id":"cl4y3s2ze000aqkw911pd3zf8","tag_id":"cl4y3s2zk000lqkw9fl3e8bnj","_id":"cl4y3s2zm000uqkw99kldcqdg"},{"post_id":"cl4y3s2zf000bqkw9ci6q1fiv","tag_id":"cl4y3s2zl000oqkw91jaee0jr","_id":"cl4y3s2zp000xqkw91f8m1bsy"},{"post_id":"cl4y3s2zf000bqkw9ci6q1fiv","tag_id":"cl4y3s2zl000tqkw9hjfq2l5f","_id":"cl4y3s2zp000yqkw9eupw80xp"},{"post_id":"cl4y3s2zf000bqkw9ci6q1fiv","tag_id":"cl4y3s2zm000vqkw99clbgn54","_id":"cl4y3s2zq0010qkw90wq01m48"},{"post_id":"cl4y3s2zg000eqkw9hpsm6jes","tag_id":"cl4y3s2zm000wqkw92hxy9mtx","_id":"cl4y3s2zr0012qkw91nxe5vap"},{"post_id":"cl4y3s2zg000eqkw9hpsm6jes","tag_id":"cl4y3s2zp000zqkw94z68busl","_id":"cl4y3s2zr0013qkw97cxw0qw5"},{"post_id":"cl4y3s2zh000fqkw9eo9y357h","tag_id":"cl4y3s2zl000oqkw91jaee0jr","_id":"cl4y3s2zr0016qkw99ojd0nj0"},{"post_id":"cl4y3s2zh000fqkw9eo9y357h","tag_id":"cl4y3s2zr0014qkw988y1cub8","_id":"cl4y3s2zs0017qkw9cvy79i3g"},{"post_id":"cl4y3s2zi000gqkw971wmex5o","tag_id":"cl4y3s2zj000iqkw90ffyb0ii","_id":"cl4y3s2zs001aqkw9bp8j7an1"},{"post_id":"cl4y3s2zi000gqkw971wmex5o","tag_id":"cl4y3s2zs0018qkw90siig21t","_id":"cl4y3s2zs001bqkw953iy45h1"},{"post_id":"cl4y3s2zi000gqkw971wmex5o","tag_id":"cl4y3s2zs0019qkw99gtf5tmv","_id":"cl4y3s2zs001cqkw93238aygt"}],"Tag":[{"name":"Moby Dick","_id":"cl4y3s2z10004qkw9f4njbjiz"},{"name":"Reading","_id":"cl4y3s2z20006qkw9axaf0zd8"},{"name":"ICSE 2022","_id":"cl4y3s2zg000dqkw95o0dgxzj"},{"name":"Code generation","_id":"cl4y3s2zj000iqkw90ffyb0ii"},{"name":"Note","_id":"cl4y3s2zk000lqkw9fl3e8bnj"},{"name":"Poem","_id":"cl4y3s2zl000oqkw91jaee0jr"},{"name":"Literary criticism","_id":"cl4y3s2zl000tqkw9hjfq2l5f"},{"name":"Philip Larkin","_id":"cl4y3s2zm000vqkw99clbgn54"},{"name":"Ace","_id":"cl4y3s2zm000wqkw92hxy9mtx"},{"name":"Taffy","_id":"cl4y3s2zp000zqkw94z68busl"},{"name":"Self","_id":"cl4y3s2zr0014qkw988y1cub8"},{"name":"ICST","_id":"cl4y3s2zs0018qkw90siig21t"},{"name":"OCL","_id":"cl4y3s2zs0019qkw99gtf5tmv"}]}}