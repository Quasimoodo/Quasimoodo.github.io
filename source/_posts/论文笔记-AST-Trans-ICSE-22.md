---
layout: post
title: 论文笔记 AST-Trans ICSE`22
date: 2022-07-03 17:20:05
tags:
- ICSE 2022
- Code generation
- Note
categories:
- Substance
---
这是《AST-Trans: Code Summarization with Efficient Tree-Structured Attention》的读书笔记

<!--more-->

# 摘要

代码总结是干什么
最先进的方法用了E-D架构
源代码的特征之一是AST，常用于编码信息
AST太长
现在的方法忽视了大小限制 直接就塞进去序列化的AST
我们认为其问题在于难以提取信息，计算成本大

为了更好地编码AST 提出了AST-Trans，利用了两种节点关系——上下和左右
使用了树状注意力动态分配权重给相关的节点
还还提出了一种**支持**高效为树状注意力并行运算的实现

在两个相关数据集上，超过了SOTA

# 介绍
 程序总结分为多个模块 本作聚焦于子例程或方法
 短描述可以让开发者快速理解 但是 好的总结需要大量劳动——错配 丢失 过时——自动方法可以避免
 传统方法：
 * 手工规则来——命名太烂就不行了
 * 信息检索技术：没见过就不行了

最近：

开源代码库让数据驱动的神经网络引起了更多注意

 代码需要AST-AST序列化-简单介绍几个方法

------------

在introduction就介绍了许多现有工作 更具体地说是提到啥就引用

------------

问题：L-AST太长了 

仔细解释

介绍我们的工作

* 假设：AST中的节点受影响最大的是：

  * 祖先-子孙:不同区块的等级关系
  * 兄弟：时序关系

  —— 画了个图作证 

  捕获这两种关系就可以了 不用用全部注意力为所有节点建模			

​	提出了 这个东西 是一个Transformer的简单变体来 处理树状AST

 	用了 这两种关系的矩阵来代表树结构 然后用了这个矩阵来动态排除不同注意力层中的无关节点

​	绝对位置嵌入也被换成了由两种关系矩阵的相对位置嵌入

还进一步描述了实现与计算分析

贡献：

* 可以用线性复杂度来编码唱AST 和传统二次复杂度的Transfermer不一样
* 深入分析：复杂度，经验证据等
* 在两个数据集上显示大幅度SOTA
* 比较了多种AST的编码方式并讨论

2 Background——AST Transformer

3 elaborates 实现细节

4 不同的实现

5 分析复杂度

6 解释实验步骤 分析结果

7 TtV

8 RW

9 Conclusion



# BACKGROUND

## Transformer

最开始被提出来做机器翻译 使用了多头栈状encoder-decocer层

介绍了一下普通Transformer的层，可以并行

## AST



真的就是简单介绍 几乎没有引用



# 方法

先概述一下流程 然后分步骤介绍

## 序列化

简单介绍了三种序列化的方法，最后说用了改进的先序遍历序列法达成了SOTA 反而结构化的效果不好 路径分解效果更烂

1. POT 先序遍历序列化：简单的按照先序遍历把节点列出来 坏处在于有损 因为无法靠此重建
2. SBT 结构序列化 ：
3. PD 路径分解 ：随机展示出一条某节点到某节点的路径 因为会有超多条 所以需要随机采样

前两个都能直接放 第三个不好搞在这个任务

------

问题在于他实际使用的 改进的POT也没介绍啊





## 关系矩阵

引入了祖先矩阵和兄弟矩阵两个东西来对节点的关系建模

如果是该关系 则值为有向距离，反之为正无穷

Aij=-Aji

还介绍了一个预定义阈值P 有向距离大于P则变成正无穷 没搞懂在干什么：

十代祖先就不算了 设置了一定的视野范围

说是要用两个矩阵来动态改善树状的注意力分配



## 树状注意力

介绍了自注意力 相对位置嵌入 	注意力解耦 这三个看起来都是别人的东西 基本跟他没什么关系 有点像Background

然后介绍了树状关系注意力，

用了前面的关系矩阵来代替了相对位置嵌入的一个距离 再经过一些处理 在对新关系建模的同时还能达到更低的复杂度效果。



# 高效实现

传统的Transformer 计算复杂度会随着序列长度二次增长 但改良的ASTTrans 只需要对部分节点对计算：某些R距离大于零的 然后逐个分析实验方法：掩码 循环跳过 稀疏向量什么的



------

可以对数据集来个量化的分析

# 复杂度分析



# 实验

## 准备

数据集介绍，处理策略

预处理

超参数

评测方式

## 介绍比较的基线

根据输入的不同分类介绍

每个模型逐个介绍

输入代码：

输入AST树：有树专用的encoder 用Tree-LSTM或者GNN——想知道是怎么输入的 或者说……encoder是怎么接受这种东西的

输入PD AST：作者甚至还改进了他的模型作为基线：把模型中的LSTM换成Transformer

输入SBT AST 

输入POT AST:设计了对照组是接收同样输入的Transformer







## 主要结果

也就是分析

比较：

* 代码-树状AST-序列化AST

  发现树状AST效果最好 结果树状的效果最好 作者的分析是这个信息最完整最多 所以效果最好

  还分析了在不同数据集上的影响 觉得是长度的原因

* 三种序列化AST的比较

  SBT在JAVA效果最好 POT在python上效果最好 

  SBT 信息多 POT最短

  PD效果最烂

  

* 关系矩阵的影响

  加上之后改善了所有模型的性能

* AST-Trans vs GNN

  和其他里面最强的那个比一比

  分析了为啥会更好一点

## 	消融实验

消融对象：四种

* 仅使用一种对象矩阵：也会造成提升 但是不如都来

* 注意力头的数量 8个头几个看祖孙 几个看兄弟：更接近一种超参数实验

* 观察多远的范围来确定祖先-兄弟关系？：

  视野越大效果越好，就算一点也能改善很多，具有边际递减

* 网络层数：越深越好

## 视觉分析和定量分析

# 有效性威胁

* 选取的公开数据集可能代表性不足
* 作为基线的其他架构选择和超参数选择可能不是最优
* 自动评估和手动评估的代表性可能不强

------

全都是实验设置 做实验的问题 没提方法本身的问题

# 相关工作

## 代码总结

大多数人都把他当seq2seq任务，用Transformer

和传统翻译的唯一区别就是他的输入是无歧义的代码，有语法规则，普遍的方法都是把他当普通文本序列或者结构化序列

介绍了一下处理的方法，以罗列为主

简单比较了一下自己的方法

## 基于树的神经网络

现有的树状神经网络可以按照输入分为两种

解析法和采样法

# 总结

我们**通过**高效编码AST**实现**了代码总结：

介绍一下方法，以及带来的好处

只需要关注有关系的节点

可以不被过长的AST困惑

降低复杂度

画画饼：让他可以处理长代码，甚至一个文件

做了实验，比较



我们相信这样的基础理念可以用在别的地方

计划更多特征加入，如API序列或节点类型，来改善注意力机制
